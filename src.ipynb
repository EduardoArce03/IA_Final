{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabajo Final - Inteligencia Artificial\n",
    "## **Nombre:** Eduardo Arce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importacion de librerias necesarias para la presente implementacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported\n",
      "NLTK resources downloaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/eduardo-\n",
      "[nltk_data]     arce/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/eduardo-\n",
      "[nltk_data]     arce/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/eduardo-\n",
      "[nltk_data]     arce/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "import hdbscan\n",
    "import re\n",
    "print(\"Libraries imported\")\n",
    "\n",
    "# Adicional se descargan recursos de nlkt para tokenizar y lematizar\n",
    "# üìå Descargar recursos de NLTK si no est√°n\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "print(\"NLTK resources downloaded\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FASE 1: PLN\n",
    "## **Tecnicas Usadas:**\n",
    "\n",
    "### Limpieza de stopwords y lematizacion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seteamos diferentes stopwords para aislarlos del texto\n",
    "# üìå Stopwords personalizadas\n",
    "stop_words = set(stopwords.words(\"english\")).union({\n",
    "    \"abstract\", \"sample\", \"madrid\", \"introduction\", \"conclusion\", \"method\", \"study\", \"approach\", \n",
    "    \"paper\", \"result\", \"propose\", \"data\", \"information\", \"model\", \"analysis\",\n",
    "    \"table\", \"figure\", \"algorithm\", \"system\", \"value\", \"based\", \"case\", \"using\", \"abrahamgutierrez\", \"abrahamgutierrezupmes\"\n",
    "})\n",
    "\n",
    "## Lematizamos el texto (es decir, lo transformamos a su ra√≠z, esto mediante un diccionario que tiene la biblioteca)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de texto\n",
    "##### **- Convertir a minisculas**\n",
    "##### **- Eliminar numeros**\n",
    "##### **- Eliminar signos de puntuacion**\n",
    "##### **- Tokenizacion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Funci√≥n de limpieza mejorada\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir a min√∫sculas\n",
    "    text = re.sub(r'\\d+', '', text)  # Eliminar n√∫meros\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Eliminar signos de puntuaci√≥n\n",
    "    tokens = word_tokenize(text)  # Tokenizaci√≥n\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatizaci√≥n y stopwords\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificacion de titulos, keywords y body de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Extraer textos, t√≠tulos y keywords de PDFs\n",
    "def extract_text_titles_keywords(pdf_path):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    full_text, titles, keywords = [], [], []\n",
    "    found_keywords = False\n",
    "\n",
    "    for page_num, page in enumerate(doc):\n",
    "        raw_text = page.get_text(\"text\")\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "\n",
    "        for block in blocks:\n",
    "            if \"lines\" in block:\n",
    "                for line in block[\"lines\"]:\n",
    "                    for span in line[\"spans\"]:\n",
    "                        text = span[\"text\"].strip()\n",
    "                        if text:\n",
    "                            full_text.append(text)\n",
    "                            if span[\"size\"] > 12:  # T√≠tulos grandes\n",
    "                                titles.append(text)\n",
    "\n",
    "        # üìå Extraer Keywords\n",
    "        if page_num == 0:\n",
    "            keywords_match = re.search(r\"(?i)(?:Keywords|Palabras Clave|KEYWORDS)[:\\s]*(.*)\", raw_text)\n",
    "            if keywords_match:\n",
    "                extracted_keywords = keywords_match.group(1).strip()\n",
    "                if len(extracted_keywords) > 2:\n",
    "                    keywords.append(extracted_keywords)\n",
    "                    found_keywords = True\n",
    "\n",
    "    if not found_keywords:\n",
    "        keywords.append(\"\")  # Evitar NaN en el DataFrame\n",
    "\n",
    "    return \" \".join(full_text), \" | \".join(titles), \" | \".join(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraccion de contenido de pdfs y aplicacion de PLN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå Extraer de todos los PDFs\n",
    "def extract_text_from_pdfs_in_folder(folder_path):\n",
    "    pdf_texts = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text, titles, keywords = extract_text_titles_keywords(pdf_path)\n",
    "            pdf_texts.append({\n",
    "                \"Documento\": filename,\n",
    "                \"Titulos_Extraidos\": titles,\n",
    "                \"Keywords_Extraidas\": keywords,\n",
    "                \"Texto_Original\": text\n",
    "            })\n",
    "    return pd.DataFrame(pdf_texts)\n",
    "\n",
    "# üìÇ üìå Ruta de PDFs\n",
    "folder_path = \"Documents/Repositorio\" \n",
    "\n",
    "# üìå Extraer texto, t√≠tulos y keywords\n",
    "df_pdfs_original = extract_text_from_pdfs_in_folder(folder_path)\n",
    "\n",
    "# üìå Aplicar limpieza mejorada\n",
    "df_pdfs_original[\"Texto_Procesado\"] = df_pdfs_original[\"Texto_Original\"].apply(clean_text)\n",
    "df_pdfs_original[\"Titulos_Procesados\"] = df_pdfs_original[\"Titulos_Extraidos\"].apply(clean_text)\n",
    "df_pdfs_original[\"Keywords_Procesadas\"] = df_pdfs_original[\"Keywords_Extraidas\"].apply(clean_text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asignacion de orden de importancia a estructura de documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå üî• **DAR M√ÅS PESO A T√çTULOS Y KEYWORDS**\n",
    "df_pdfs_original[\"Texto_Final\"] = (\n",
    "    (df_pdfs_original[\"Titulos_Procesados\"] + \" \") * 3 +  # üî• T√≠tulos tienen 3X peso\n",
    "    (df_pdfs_original[\"Keywords_Procesadas\"] + \" \") * 2 +  # üî• Keywords tienen 2X peso\n",
    "    df_pdfs_original[\"Texto_Procesado\"]  # Texto normal\n",
    ")\n",
    "\n",
    "# üìå Guardar DataFrames\n",
    "df_pdfs_original.to_csv(\"textos_procesados_con_pesos.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizacion y conversion a dataframe de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Vista previa del TF-IDF DataFrame:\n",
      "       aaai   ability      able   abraham  abraham guti√©rrez  absolute  \\\n",
      "0  0.000000  0.006593  0.009520  0.010179           0.011878  0.007616   \n",
      "1  0.000000  0.000000  0.003066  0.016389           0.009563  0.012263   \n",
      "2  0.001933  0.001431  0.003719  0.000000           0.000000  0.009917   \n",
      "3  0.002557  0.005677  0.004919  0.004382           0.005114  0.008198   \n",
      "4  0.000000  0.003437  0.008933  0.003980           0.000000  0.005955   \n",
      "\n",
      "   absolute error  absolute error mae  accepted    access  ...      Ô¨Åeld  \\\n",
      "0        0.001904            0.001904  0.001904  0.006593  ...  0.000000   \n",
      "1        0.006131            0.006131  0.003066  0.007077  ...  0.000000   \n",
      "2        0.007438            0.002479  0.003719  0.011445  ...  0.052199   \n",
      "3        0.003279            0.001640  0.001640  0.011353  ...  0.000000   \n",
      "4        0.005955            0.002978  0.002978  0.013747  ...  0.051084   \n",
      "\n",
      "       Ô¨Ålms  Ô¨Åltering  Ô¨Åltering proceeding  Ô¨Åltering recommendation  \\\n",
      "0  0.000000  0.000000             0.000000                 0.000000   \n",
      "1  0.000000  0.000000             0.000000                 0.000000   \n",
      "2  0.011449  0.241661             0.007733                 0.025133   \n",
      "3  0.000000  0.000000             0.000000                 0.000000   \n",
      "4  0.005501  0.106811             0.009288                 0.000000   \n",
      "\n",
      "   Ô¨Åltering recommender       Ô¨Ånd      Ô¨Årst        ùë¢ùëñ    ùë¢ùëñ ùë¢ùëñ  \n",
      "0              0.000000  0.000000  0.000000  0.000000  0.00000  \n",
      "1              0.000000  0.000000  0.000000  0.000000  0.00000  \n",
      "2              0.011600  0.005800  0.025133  0.000000  0.00000  \n",
      "3              0.000000  0.000000  0.000000  0.048014  0.02216  \n",
      "4              0.018576  0.013932  0.032508  0.000000  0.00000  \n",
      "\n",
      "[5 rows x 1500 columns]\n"
     ]
    }
   ],
   "source": [
    "# üìå TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1500, stop_words=\"english\", ngram_range=(1, 3))\n",
    "X_tfidf = vectorizer.fit_transform(df_pdfs_original[\"Texto_Final\"]).toarray()\n",
    "\n",
    "# üìå Guardar TF-IDF en CSV\n",
    "pd.DataFrame(X_tfidf, columns=vectorizer.get_feature_names_out()).to_csv(\"tfidf_vectors_pesados.csv\", index=False)\n",
    "\n",
    "df_tfidf = pd.DataFrame(X_tfidf, columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"\\n‚úÖ Vista previa del TF-IDF DataFrame:\")\n",
    "print(df_tfidf.head())  # Imprime las primeras 5 filas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fase 2: Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizacion y normalizacion de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 1Ô∏è‚É£ Cargar el DataFrame con los textos procesados\n",
    "df = pd.read_csv(\"textos_procesados_con_pesos.csv\")\n",
    "\n",
    "\n",
    "# üìå 2Ô∏è‚É£ Dar m√°s peso a t√≠tulos y keywords\n",
    "df[\"Texto_Final\"] = df.apply(\n",
    "    lambda row: f\"{' '.join([str(row['Titulos_Procesados'])]*3)} \"\n",
    "                f\"{' '.join([str(row['Keywords_Procesadas'])]*2)} \"\n",
    "                f\"{str(row['Texto_Procesado'])}\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# üìå 3Ô∏è‚É£ Vectorizaci√≥n TF-IDF con frases clave\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=4000,  \n",
    "    stop_words=\"english\",  \n",
    "    ngram_range=(2, 4),  \n",
    "    min_df=2, \n",
    "    max_df=0.85\n",
    ")\n",
    "X_tfidf = vectorizer.fit_transform(df[\"Texto_Final\"]).toarray()\n",
    "\n",
    "# üìå 4Ô∏è‚É£ Escalar los embeddings TF-IDF\n",
    "scaler = MinMaxScaler()\n",
    "X_tfidf_scaled = scaler.fit_transform(X_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estructura de la red neuronal variacional (VAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos la dimesion latente (comprimira la entrada en solo 20 dimensiones)\n",
    "latent_dim = 20  \n",
    "# Capa de entrada\n",
    "input_layer = keras.Input(shape=(X_tfidf_scaled.shape[1],))\n",
    "#Creamos el encoder con 3 capas densas\n",
    "encoder = layers.Dense(512, activation=\"relu\")(input_layer)\n",
    "# Agregamos BatchNormalization para normalizar los valores de las capas\n",
    "encoder = layers.BatchNormalization()(encoder)\n",
    "# Agregamos Dropout para evitar overfitting\n",
    "encoder = layers.Dropout(0.2)(encoder)\n",
    "encoder = layers.Dense(256, activation=\"relu\")(encoder)\n",
    "encoder = layers.BatchNormalization()(encoder)\n",
    "encoder = layers.Dropout(0.2)(encoder)\n",
    "encoder = layers.Dense(128, activation=\"relu\")(encoder)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se definen 2 capas, que modelan la distribucion gaussiana del espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se definen 2 capas, que modelan la distribucion gaussiana del espacio latente\n",
    "z_mean = layers.Dense(latent_dim, name=\"z_mean\")(encoder)\n",
    "z_log_var = layers.Dense(latent_dim, name=\"z_log_var\")(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capa personalizada para reparametrizacion de muestreo en el VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Sampling()([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decodificador del VAE (Reconstruye los datos originales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = layers.Dense(128, activation=\"relu\")(z)\n",
    "decoder = layers.BatchNormalization()(decoder)\n",
    "decoder = layers.Dropout(0.2)(decoder)\n",
    "decoder = layers.Dense(256, activation=\"relu\")(decoder)\n",
    "decoder = layers.BatchNormalization()(decoder)\n",
    "decoder = layers.Dropout(0.2)(decoder)\n",
    "decoder = layers.Dense(512, activation=\"relu\")(decoder)\n",
    "decoder = layers.Dense(X_tfidf_scaled.shape[1], activation=\"sigmoid\")(decoder)\n",
    "\n",
    "vae = keras.Model(input_layer, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definimos la funcion de perdida (MSE) y entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Entrenando el VAE con m√°s capacidad y regularizaci√≥n...\n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 1s 961ms/step - loss: 834.2420 - val_loss: 920.2653\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 812.7067 - val_loss: 919.7148\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 783.9750 - val_loss: 920.9391\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 758.9147 - val_loss: 923.1588\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 741.6970 - val_loss: 921.7962\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 708.1305 - val_loss: 926.3725\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 676.8074 - val_loss: 924.9675\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 689.6378 - val_loss: 924.2539\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 656.8496 - val_loss: 924.0234\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 636.3590 - val_loss: 925.6577\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 606.5120 - val_loss: 925.9489\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 574.7443 - val_loss: 925.0987\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 575.7048 - val_loss: 930.6531\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 548.8599 - val_loss: 925.8239\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 549.1045 - val_loss: 949.4714\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 507.5015 - val_loss: 939.2937\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 484.5753 - val_loss: 940.0989\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 490.3242 - val_loss: 931.8252\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 445.9205 - val_loss: 925.9777\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 456.6231 - val_loss: 928.1422\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 409.0007 - val_loss: 931.8409\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 397.1669 - val_loss: 928.1873\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 385.7589 - val_loss: 948.7886\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 371.8533 - val_loss: 944.3021\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 345.3485 - val_loss: 938.7004\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 339.1775 - val_loss: 936.4198\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 273.7930 - val_loss: 936.6472\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 280.6896 - val_loss: 953.0627\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 225.8491 - val_loss: 959.9901\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 203.3425 - val_loss: 948.8159\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 211.5106 - val_loss: 948.3209\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 172.8534 - val_loss: 965.4428\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 136.0265 - val_loss: 955.6899\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 153.5872 - val_loss: 959.2372\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 113.6238 - val_loss: 974.9929\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 111.7841 - val_loss: 954.6980\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 91.6829 - val_loss: 937.3311\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 126.6708 - val_loss: 957.3293\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 69.4826 - val_loss: 1002.8433\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 57.4544 - val_loss: 920.5359\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 51.7269 - val_loss: 951.6123\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 55.3715 - val_loss: 932.4486\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 47.0436 - val_loss: 980.6987\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 43.4270 - val_loss: 984.4471\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 33.4676 - val_loss: 966.1559\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 30.4051 - val_loss: 950.9744\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 32.1604 - val_loss: 977.2151\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 20.8717 - val_loss: 953.2577\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 21.7562 - val_loss: 956.0024\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 17.3987 - val_loss: 950.1844\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x77c460d115a0>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss = tf.keras.losses.mean_squared_error(input_layer, decoder)\n",
    "reconstruction_loss *= X_tfidf_scaled.shape[1]\n",
    "kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "vae_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "vae.add_loss(vae_loss)\n",
    "vae.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0005))\n",
    "\n",
    "# üìå 7Ô∏è‚É£ Entrenar el modelo VAE\n",
    "print(\"\\nüöÄ Entrenando el VAE con m√°s capacidad y regularizaci√≥n...\")\n",
    "vae.fit(X_tfidf_scaled, X_tfidf_scaled, epochs=50, batch_size=16, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraccion de embeddings latentes del VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 56ms/step\n"
     ]
    }
   ],
   "source": [
    "# üìå 8Ô∏è‚É£ Extraer embeddings latentes\n",
    "encoder_model = keras.Model(input_layer, z_mean)\n",
    "embeddings_latentes = encoder_model.predict(X_tfidf_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicamos PCA para reducir la dimensionalidad de los embeddings obtenidos del VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 9Ô∏è‚É£ Aplicar reducci√≥n de dimensionalidad con PCA\n",
    "n_samples = embeddings_latentes.shape[0]\n",
    "n_features = embeddings_latentes.shape[1]\n",
    "n_pca_components = min(10, n_samples, n_features)\n",
    "\n",
    "pca = PCA(n_components=n_pca_components)\n",
    "embeddings_pca = pca.fit_transform(embeddings_latentes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aplicacion de HDBSCAN para descubrir topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå üîü Aplicar HDBSCAN primero\n",
    "# HDBSCAN agrupa datos bas√°ndose en densidad\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=6,  \n",
    "    min_samples=4,       \n",
    "    cluster_selection_method='eom',\n",
    "    allow_single_cluster=True  \n",
    ")\n",
    "\n",
    "df[\"T√≥pico_Descubierto\"] = clusterer.fit_predict(embeddings_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste de N-Topicos con K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Ajustando los t√≥picos con K-Means para llegar a 4...\n"
     ]
    }
   ],
   "source": [
    "# üìå 1Ô∏è‚É£1Ô∏è‚É£ Ajustar el n√∫mero de t√≥picos con K-Means si es necesario\n",
    "num_topics = int(input(\"Ingrese el n√∫mero de t√≥picos deseados: \"))\n",
    "num_detected = len(set(df[\"T√≥pico_Descubierto\"])) - (1 if -1 in df[\"T√≥pico_Descubierto\"].values else 0)\n",
    "\n",
    "if num_detected < num_topics:\n",
    "    print(f\"üîÑ Ajustando los t√≥picos con K-Means para llegar a {num_topics}...\")\n",
    "    kmeans = KMeans(n_clusters=num_topics, random_state=42, n_init=10)\n",
    "    df[\"T√≥pico_Descubierto\"] = kmeans.fit_predict(embeddings_pca)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtenemos palabras clave en cada cluster con TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 1Ô∏è‚É£2Ô∏è‚É£ Obtener frases clave representativas\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "top_phrases_per_topic = []\n",
    "\n",
    "for i in range(num_topics):\n",
    "    cluster_docs = df[df[\"T√≥pico_Descubierto\"] == i][\"Texto_Final\"]\n",
    "    \n",
    "    if cluster_docs.empty:\n",
    "        top_phrases_per_topic.append([\"Unknown Topic\"])\n",
    "        continue\n",
    "    \n",
    "    cluster_tfidf = vectorizer.transform(cluster_docs)\n",
    "    avg_tfidf = np.mean(cluster_tfidf, axis=0).flatten()\n",
    "    top_phrase_indices = np.argsort(avg_tfidf.A1)[::-1][:7]\n",
    "    top_phrases = [feature_names[idx] for idx in top_phrase_indices]\n",
    "    top_phrases_per_topic.append(top_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtramos terminos irrelevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üìå 1Ô∏è‚É£3Ô∏è‚É£ Filtrar t√©rminos irrelevantes\n",
    "stop_phrases = {\"et al\", \"pp\", \"conference\", \"journal\", \"vol\", \"dataset\", \"recommendation\", \"user\"}\n",
    "def clean_topic_name(name):\n",
    "    words = name.split()\n",
    "    return \" \".join([word for word in words if word.lower() not in stop_phrases])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generamos los nombres de topicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìå Cantidad de documentos en cada t√≥pico:\n",
      "T√≥pico_Descubierto\n",
      "2    3\n",
      "1    1\n",
      "3    1\n",
      "0    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üìå T√≥picos detectados con nombres interpretables:\n",
      "T√≥pico 0: number and synthetic datasets in random noise\n",
      "T√≥pico 1: nmf bnmf and hidden factor in number\n",
      "T√≥pico 2: latent space and group in synthetic datasets\n",
      "T√≥pico 3: et al and collaborative Ô¨Åltering in bobadilla et\n"
     ]
    }
   ],
   "source": [
    "# üìå 1Ô∏è‚É£4Ô∏è‚É£ Generar nombres de t√≥picos m√°s naturales\n",
    "def generate_topic_name(phrases):\n",
    "    phrases = [clean_topic_name(p) for p in phrases]\n",
    "    phrases = list(dict.fromkeys(phrases))\n",
    "    if len(phrases) >= 3:\n",
    "        return f\"{phrases[0]} and {phrases[1]} in {phrases[2]}\"\n",
    "    elif len(phrases) == 2:\n",
    "        return f\"{phrases[0]} and {phrases[1]}\"\n",
    "    else:\n",
    "        return phrases[0] if phrases else \"Unknown Topic\"\n",
    "\n",
    "topic_labels = [generate_topic_name(phrases) for phrases in top_phrases_per_topic]\n",
    "# üìå 1Ô∏è‚É£5Ô∏è‚É£ Asignar nombres interpretables a los t√≥picos\n",
    "df[\"Nombre_Topico\"] = df[\"T√≥pico_Descubierto\"].map(lambda x: topic_labels[x])\n",
    "\n",
    "# üìå 1Ô∏è‚É£6Ô∏è‚É£ Guardar resultados finales\n",
    "df.to_csv(\"topicos_mejorados.csv\", index=False)\n",
    "\n",
    "# üìå üî• Mostrar resumen\n",
    "print(\"\\nüìå Cantidad de documentos en cada t√≥pico:\")\n",
    "print(df[\"T√≥pico_Descubierto\"].value_counts())\n",
    "\n",
    "print(\"\\nüìå T√≥picos detectados con nombres interpretables:\")\n",
    "for i, name in enumerate(topic_labels):\n",
    "    print(f\"T√≥pico {i}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generacion de resumenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ ¬°Res√∫menes mejorados generados y guardados en 'Resumen_en_Topicos_Mejorado.pdf'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eduardo-arce/Documentos/Inteligencia Artificial/Segundo_Interciclo/Trabajo-FInal/.venv/lib/python3.10/site-packages/fpdf/ttfonts.py:670: UserWarning: cmap value too big/small: -64232\n",
      "  warnings.warn(\"cmap value too big/small: %s\" % cm)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openai\n",
    "from fpdf import FPDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# üìå 1Ô∏è‚É£ Cargar los documentos con sus t√≥picos\n",
    "df = pd.read_csv(\"topicos_mejorados.csv\")\n",
    "\n",
    "# üìå 2Ô∏è‚É£ Seleccionar los documentos m√°s representativos\n",
    "vectorizer = TfidfVectorizer(max_features=2000, stop_words=\"english\")\n",
    "X_tfidf = vectorizer.fit_transform(df[\"Texto_Final\"]).toarray()\n",
    "\n",
    "# Sumar la importancia TF-IDF por documento\n",
    "df[\"Importancia\"] = np.sum(X_tfidf, axis=1)\n",
    "\n",
    "# Seleccionar el documento m√°s representativo por t√≥pico (el de mayor importancia TF-IDF)\n",
    "df_top = df.loc[df.groupby(\"Nombre_Topico\")[\"Importancia\"].idxmax()].reset_index(drop=True)\n",
    "\n",
    "# üìå 3Ô∏è‚É£ Definir funci√≥n para generar res√∫menes con OpenAI\n",
    "def generar_resumen(texto, topico, modelo=\"gpt-4o\", max_tokens=500):\n",
    "    \"\"\"\n",
    "    Genera un resumen utilizando la API de OpenAI, enfoc√°ndose en el t√≥pico.\n",
    "    \"\"\"\n",
    "    client = openai.OpenAI(api_key=api_key)  \n",
    "    \n",
    "    prompt = (f\"\"\"Resumen del siguiente texto en aproximadamente 200 palabras. \n",
    "    Enf√≥cate en el t√≥pico: {topico}.\n",
    "    \n",
    "    Texto: {texto[:4000]}\"\"\")  # üî• Limitamos a 4000 caracteres\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=modelo,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "# üìå 4Ô∏è‚É£ Generar res√∫menes para cada t√≥pico\n",
    "df_top[\"Resumen\"] = df_top.apply(lambda row: generar_resumen(row[\"Texto_Final\"], row[\"Nombre_Topico\"]), axis=1)\n",
    "\n",
    "# üìå 5Ô∏è‚É£ Generar el PDF final con nombres de t√≥picos, documentos y res√∫menes\n",
    "pdf = FPDF()\n",
    "pdf.set_auto_page_break(auto=True, margin=15)\n",
    "pdf.add_font(\"DejaVu\", \"\", \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\", uni=True)\n",
    "pdf.add_font(\"DejaVu\", \"B\", \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", uni=True)\n",
    "\n",
    "for _, row in df_top.iterrows():\n",
    "    pdf.add_page()\n",
    "    \n",
    "    # üîπ Agregar t√≠tulo del t√≥pico\n",
    "    pdf.set_font(\"DejaVu\", \"B\", 14)\n",
    "    pdf.cell(200, 10, f\"T√≥pico: {row['Nombre_Topico']}\", ln=True, align=\"C\")\n",
    "    \n",
    "    # üîπ Agregar t√≠tulo del documento m√°s representativo\n",
    "    pdf.set_font(\"DejaVu\", \"\", 12)\n",
    "    pdf.cell(0, 10, f\"Documento: {row['Documento']}\", ln=True, align=\"C\")\n",
    "    \n",
    "    # üîπ Agregar resumen\n",
    "    pdf.set_font(\"DejaVu\", \"\", 12)\n",
    "    pdf.multi_cell(0, 10, row[\"Resumen\"])\n",
    "\n",
    "pdf.output(\"Resumen_en_Topicos_Mejorado.pdf\", \"F\")\n",
    "\n",
    "print(\"\\n‚úÖ ¬°Res√∫menes mejorados generados y guardados en 'Resumen_en_Topicos_Mejorado.pdf'!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import keyEdu\n",
    "\n",
    "importlib.reload(keyEdu)  # Fuerza a Python a recargar el m√≥dulo\n",
    "\n",
    "api_key = keyEdu.OPENAI_API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

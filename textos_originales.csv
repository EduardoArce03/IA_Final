Documento,Titulos_Extraidos,Keywords_Extraidas,Texto_Original
Wasserstein GAN based architecture to generate collaborative filtering synthetic datasets.pdf,Wasserstein GAN‚Äëbased architecture to¬†generate collaborative | filtering synthetic datasets,WGANRS¬†¬∑ Generative Adversarial Networks¬†¬∑ Recommender Systems¬†¬∑ Wasserstein distance¬†¬∑ Synthetic,"https://doi.org/10.1007/s10489-024-05313-4 Wasserstein GAN‚Äëbased architecture to¬†generate collaborative filtering synthetic datasets Jes√∫s¬†Bobadilla 1,2 ¬∑ Abraham¬†Guti√©rrez 1,2 Accepted: 1 February 2024 ¬© The Author(s) 2024 Abstract Currently, generative applications are reshaping different fields, such as art, computer vision, speech processing, and natural language. The computer science personalization area is increasingly relevant since large companies such as Spotify, Netflix, TripAdvisor, Amazon, and Google use recommender systems. Then, it is rational to expect that generative learning will increasingly be used to improve current recommender systems. In this paper, a method is proposed to generate synthetic recommender system datasets that can be used to test the recommendation performance and accuracy of a company on dif- ferent simulated scenarios, such as large increases in their dataset sizes, number of users, or number of items. Specifically, an improvement in the state-of-the-art method is proposed by applying the Wasserstein concept to the generative adversarial network for recommender systems (GANRS) seminal method to generate synthetic datasets. The results show that our pro- posed method reduces the mode collapse, increases the sizes of the synthetic datasets, improves their ratings distributions, and maintains the potential to choose the desired number of users, number of items, and starting size of the dataset. Both the baseline GANRS and the proposed Wasserstein-based WGANRS deep learning architectures generate fake profiles from dense, short, and continuous embeddings in the latent space instead of the sparse, large, and discrete raw samples that previ- ous GAN models used as a source. To enable reproducibility, the Python and Keras codes are provided in open repositories along with the synthetic datasets generated to test the proposed architecture ( https://‚Äãgithub.‚Äãcom/‚Äãjesus‚Äãbobad‚Äãilla/‚Äãganrs.‚Äãgit ). Keywords WGANRS¬†¬∑ Generative Adversarial Networks¬†¬∑ Recommender Systems¬†¬∑ Wasserstein distance¬†¬∑ Synthetic datasets¬†¬∑ Collaborative Filtering 1‚ÄÇ Introduction Recommender systems (RSs) are used to provide personali- zation facilities to users of internet services. Large compa- nies that use RSs are Spotify, TripAdvisor, Netflix, Google Music, etc. RSs are becoming increasingly important due to its capacity to provide both accurate recommendations and recommendations designed to retain people using the service. Recommendations are provided by suggesting the products or services that have a higher probability of being liked by the user. Consequently, it is necessary to filter the available items (products or services) in the RS. For this reason, RSs are usually classified according to their filtering approach. Social [ 1 , 2 ], content-based [ 3 ], demographic [ 4 , 5 ], context-aware [ 6 ], collaborative filtering (CF) [ 7 ] and their ensembles [ 8 ] are the most commonly used strategies. Social filtering recommends to the active users items that their followed, group of friends, contacts, etc., like. Content-based recommendations include items with similar content to those the active user liked. It is usual to compare descriptions or even item images. Demographic filtering selects users having demographic features such as those of the active user (similar age, same sex, same zip code or near zip code, etc.) and then extracts those item preferences. Context-aware filtering usually relies on geographic information, such as GPS coordinates. The most accurate and relevant filtering strategy is the * Jes√∫s Bobadilla jesus.bobadilla@upm.es Abraham Guti√©rrez abraham.gutierrez@upm.es 1 Universidad Polit√©cnica de Madrid, ETSISI, Ctra. de Valencia Km. 7, Madrid, Spain 2 Technical University of¬†Madrid, ETSISI, Ctra. de Valencia Km. 7, Madrid, Spain / Published online: 17 February 2024 Applied Intelligence (2024) 54:2472‚Äì2490 collaborative strategy. In this strategy, recommendations are based on the preferences of the most similar users. The machine learning method that best fits the CF concept is the K-nearest neighbours algorithm (KNN) [ 9 ]. It is simple and directly implements the CF concept, where the neigh- bours are the most similar users to the active users. The main drawbacks of the KNN are that it is a memory-based method, it runs slowly, and it is not sufficiently accurate. The model-based matrix factorization (MF) [ 10 ] solves the KNN limitations. Moreover, it contains two vectors of hidden factors. The first vector is used to code (compress) the relevant information of users, whereas the second vec- tor is used to code the relevant information of items. Both vectors belong to the same latent space, and they are com- bined using a dot product. Additionally, the hidden factors are optimized by minimizing the prediction errors. Non- negative matrix factorization (NMF) [ 11 ] ensures that the hidden factors are non-negative to enable some semantic interpretations of predictions. Deep learning [ 12 ] can currently be implemented to obtain improved MF results. The simplest deep learn- ing architecture is similar to that of MF. In this method, the hidden factors of the user are replaced by neural user embedding, and analogously, the hidden factors of items are replaced by neural item embedding. This model is called deep matrix factorization (DeepMF) [ 13 ], and it is better than MF due to the ability of its neural networks to remove complex non-linear patterns in raw data. DeepMF combines the embeddings using a dot layer. An improved DeepMF model is the variational deep matrix factorization (VDeepMF) [ 14 ], where an intermediate layer codes the parameters of a chosen distribution (usually Gaussian), and from it, a stochastic-based sampling process spreads samples in the latent space. The DeepMF (or VDeepMF) dot layer can be improved by replacing it with a multilayer perceptron (MLP) that combines the hidden factors of users and item embeddings and generates a manifold. This approach is called neural collaborative filtering (NCF) [ 15 ]. Our proposed architecture combines a DeepMF model and a Wasserstein generative adversarial network (GAN). GAN [ 16 ] networks can generate fake samples fol- lowing the distribution of a source set of real data samples. The most common use of GAN networks is to generate realistic fake faces from a dataset of real human faces. Similarly, our objective is to generate synthetic (fake) CF samples from an existing dataset of CF samples, such as MovieLens [ 17 ]. Then, by collecting many fake samples, a synthetic CF dataset can be created. Generating synthetic CF datasets makes it possible to simulate the stress situation in the RS, as it can be gener- ated ‚Äòfamilies‚Äô of datasets where gradually some of the parameters can be selected. For example, we can gener- ate a family of CF datasets where the number of users grows from several thousands to millions, and then test in advance the performance of our system in different scenarios where the number of users gradually, or sud- denly, grows (e.g. due to a marketing campaign or an influ- encer action). This simulation can avoid system failures in extreme situations. Similarly, a family of datasets can be generated with a growing number of items. It leads to more sparse scenarios where the RS accuracy could decrease. This type of simulation gives us the conveni- ence of incorporating many products or services in a short period of time. Additionally, the generation of synthetic datasets makes possible that researchers test their machine learning models in bounded scenarios, difficult to find in real datasets, such as increasingly sparse data matrices, different cold start situations, or extreme pattern variations in the user profiles. The state-of-the-art methods in CF generation include statistical methods that are not able to adequately determine the patterns of complex datasets. Therefore, adversarial approaches [ 18 ], and GAN-based approaches have been proposed. Preventing shilling attacks is a relevant objective in the RS field, and some GAN-based approaches act as a defence against them [ 19 ]. Data augmentation is an obvi- ous field where GANs can be applied. Purchase profiles are used in the collaborative filtering generative adversarial net- work model (CFGAN) [ 20 ] model to increase the number of training samples in a dataset of commercial products. The identity-preservation generative adversarial network model (IPGAN) [ 21 ] incorporates negative sampling information to improve accuracy results. It allows two separate generative models to be incorporated, with one method managing posi- tive data and the other method processing negative samples. Session information is used in the deep collaborative filter- ing generative adversarial network (DCFGAN) model [ 22 ] instead of matrices of votes combining GAN and reinforce- ment learning. To run recommendation training, the neural collaborative generative adversarial network (NCGAN) [ 23 ] incorporates a regular GAN that processes the intermediate CF results provided by a neural network stage. The recurrent generative adversarial network (RecGAN) [ 24 ] combines a recurrent neural network (RNN) and a GAN to process temporal patterns. Unbalanced datasets are managed using a Wasserstein GAN acting as a generator and the packing generative adversarial network (PacGAN) as a discriminator [ 25 ]. Finally, a conditional generative adversarial network (CGAN) performs a conditional generation of ratings [ 26 ]. The RS state-of-the-art method that generates synthetic CF datasets is divided into statistical and machine learning approaches. Solutions in the first group allow us to param- eterize the results (to change the number of items, users, etc.), but they do not adequately capture the complex non- linear relations between users and items. Consequently, the accuracy of this method is poor. The second group makes Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2473 use of deep learning generative adversarial networks to cre- ate fake profiles or fake samples. The accuracy is improved, but the GAN architectures in this group take discrete and heavily sparse CF datasets as a source, leading to results that are obtained slowly and the subject-to-mode collapse prob- lem. In addition, the number of items cannot be changed. Nevertheless, the existing generative adversarial network for recommender systems (GANRS) method makes its GAN generation start from dense and continuous latent space embeddings, obtaining more accurate results and enabling us to choose the number of users and the number of items in the synthetic datasets. The method proposed in this paper borrows the GANRS architecture, making the necessary changes to introduce the Wasserstein concept. Wasserstein generative adversarial networks (WGANs) are designed to reduce the inherent ‚Äòmode collapse‚Äô of the GAN architecture. In the model regularization Wasserstein generative adver- sarial network (MRWGAN) [ 27 ], an RS is implemented. Moreover, an autoencoder is used to implement the genera- tive model, and a model-Wasserstein regularized distance is used as the function loss. It achieves better accuracy with missing data than the state-of-the-art methods. Analogously, an L1 regularized Wasserstein loss function has been used for autoencoder-based CF [ 28 ] to learn a low-rank repre- sentation of variables in the latent space. The Wasserstein distance has also been implemented to tackle the cold-start issue in CF, minimizing it under user embedding constraints. GAN approaches also have their own drawbacks, particu- larly: a) a long training time, b) a long inference time when the GAN model is very deep, c) difficulty to set relevant CF parameters such as the number of items and the number of users in the dataset, d) possibility of suffering from the ‚Äòmode collapse‚Äô behaviour, e) difficulty to adequately learn from the sparse data sets of CF, and e) lack of fine tuning the variation of the results in successive executions. No standard machine learning quality measures are defined to compare synthetic datasets created or gener- ated using different statistical or generative models. This is because these models are designed to catch the complex nonlinear patterns of the source data, and there are no simple comparations able to discriminate the quality of the gen- erated results, such as in regression (MAE, MSD, etc.) or classification (accuracy, precision, recall, etc.). To better understand this drawback, we can analyse the face image quality assessment strategies [ 29 ], which are based on the character, fidelity, and utility features of facial biometrics. In the RS field we do not have such type of information to make a similar process, since what we are generating are user/ item vector profiles. In addition, face image quality makes a distinction between approaches that require a reference, a reduced reference, and no reference of faces, where only the two first cases have some accurate quality measure; pre- cisely those situations that RS cannot manage as they do not have the equivalent ‚Äòreference‚Äô to the face image quality field. Finally, the conceptual problem of the ‚Äòquality para- dox‚Äô inherent in these quality measures is heavily present in the RS scenario, where a generated dataset should not be too similar to the source, otherwise it would not be useful, and should not be too different from the source, otherwise it would not be representative. Therefore, research papers that generate synthetic datasets [ 23 ‚Äì 25 ] test them by run- ning several CF Machine Learning models and comparing the results obtained on different instances of the generated datasets from the same source data. This is the strategy that our paper follows in its ‚ÄòResults‚Äô section. In the seminal GANRS paper [ 30 ], relevant innovations are incorporated, and the previous RS GAN architectures are compared to generate synthetic datasets. However, a signifi- cant drawback occurs. The process to convert from the latent space generated samples (dense, small, and continuous) to the raw samples (sparse, large, and discrete) that form the synthetic dataset generates duplicated samples that must be removed. This is a common drawback in a discretization task, but if the number of duplicated samples is high, a ‚Äòcol- lapse‚Äô in the GAN generation can occur. The innovation of our proposed Wasserstein GAN approach (WGANRS) is the introduction of the Wasserstein design (function loss, weight constraints, etc.) to the existing GANRS method in the hope that the mode collapse situations are reduced. The proposed method borrows the stages defined in [ 30 ] and replaces the regular GAN generation kernel by a Wasserstein approach (WGAN). The WGAN provides four relevant improvements: 1) it incorporates a new loss function that is interpretable and has clear stopping criteria, 2) it empirically returns better results, 3) the GAN mode collapse is significantly reduced, and 4) it provides a clear theoretical backing. The WGAN loss function is based on the earth mover‚Äôs distance, and it incorporates an f w function that acts as a discrimina- tor model, called the ‚Äòcritic‚Äô. The critic estimates the earth mover‚Äôs distance, processing the highest difference between the generated distribution and the real distribution under sev- eral parameterizations of the f w function. The critic makes the generator work harder by looking at different projec- tions. Our most relevant predictor that measures the mode collapse mitigation will reduce the removed samples and consequently increase the number of samples of the syn- thetic files (their sizes). We will also test some other quality measures such as the precision, recall, and the distribution of the ratings, users, and items. Figure 1 shows the innovation of the proposed method com- pared to SOTA, particularly with the most currently published baseline (GANRS) on which our method is based. As shown in a) (top of Fig. 1 ), SOTA methods that generate CF datasets take only raw profiles and generate synthetic RS datasets. This is an analogous process to fake face creation, which can be generated by GANs from datasets of real faces. It is known J. Bobadilla, A. Guti√©rrez 2474 that a recurrent problem in these processes is mode collapse [ 30 ], which leads to a lack of balanced generation of samples. Some categories are overrepresented, whereas other categories are underrepresented. As an example, we could obtain an enor- mous quantity of fake samples of Number 7 by using the Modi- fied National Institute of Standards and Technology (MNIST) dataset. However, some other numbers are rarely generated. Notably, in Fig. 1 a (SOTA methods), the GAN module is fed with RAW data, such as image pixels or in our case, user pro- files. These RAW data are large, discrete, and sparse, leading to the mode collapse problem. The most current research in the area is the GANRS method (our baseline). Its high-level architecture is shown in Fig. 1 b, where the GAN model is not fed with RAW data. Instead, it is fed with deep learning embedded data. These embedded data are short, continuous, and dense vectors. The embedded data contain compressed information on the items and users in the RS. As a result, both the performance and the accuracy of the GANRS are improved compared to the previous SOTA models and methods. In the GANRS baseline [ 30 ], the mode collapse problem is reduced, and the RS datasets generated are less biased than those created using SOTA methods. Regardless, the mode collapse remains, and it produces a certain degree of redundant samples. To reduce mode collapse even further, our proposed WGANRS method introduces the Wasserstein concept into the GAN kernel (Fig. 1 c). The Wasserstein approach has been shown to yield better results by reducing mode collapse when applied to GANs [ 27 ]. This approach requires the introduction of a new loss function and benefits from a theoretical backing and a defined stopping criterion. The hypothesis of the paper claims that incorporating the Wasserstein concept into the generative kernel of the GANRS method will lead to a decrease in the mode collapse problem inherent to the GAN when applied to CF scenarios. Consequently, the proposed WGANRS is expected to gener- ate more accurate CF datasets. The structure of the paper is as follows: In Section 2 , the proposed WGANRS method (from the existing GANRS infor- mation) is explained and formalized. In Section 3 , the design of the experimental executions of code is introduced, and the results are shown and analysed using the MovieLens and Net- flix* datasets as a source. Moreover, the most relevant results are provided. In Section 4 , the most remarkable conclusions are presented, and some future work is proposed. Additionally, the references section includes current representative papers in the main RS area and in the specific GAN generation of CF datasets. 2‚ÄÇ Method This section is divided into two subsections. In the first subsection, the proposed method concepts, its architecture, and the sequence of processes and stages to both train the model and generate the synthetic datasets in a feedforward prediction are explained. The second subsection contains the necessary equations to formalize the method, grouped into the main stages of the architecture. The Python and Keras codes of the proposed method is available in ( https://‚Äãgithub.‚Äã com/‚Äãjesus‚Äãbobad‚Äãilla/‚Äãganrs.‚Äãgit ). 2.1‚ÄÇ Concepts and¬†architecture The proposed deep learning architecture is based on five sequential stages in which a neural CF, a WGAN model, and a clustering process are involved. Moreover, a CF dataset is used as the source, and a synthetic dataset that has the same format as the source dataset and similar data distributions is generated. The key issue involving both the GANRS [ 30 ] seminal baseline and the WGANRS proposed architecture is that the GAN or WGAN stages are fed with dense, short, and continuous embeddings in the latent space instead of sparse, large, and discrete raw data. It makes the work of both the generator and the discriminator models easier, faster, and more accurate. The obvious drawback of the proposed design is the theoretical loss of quality involved in the com- pression stage (coder) and, particularly, in the subsequent decompression (decoder). However, when converting from embedded to raw samples, a significant benefit emerges: we can choose the target number of users and items, mak- ing the GANRS and WGANRS models more flexible and useful than the state-of-the-art methods. Figure 2 shows an overview of the proposed WGANRS architecture. From an existing source dataset (most-left side in Fig. 2 ), such as MovieLens, a synthetic dataset (most-right side in Fig. 2 ) is generated with the same format (to be useful to researchers) and similar patterns and distributions of the users, items, and ratings. This dataset can be generated by choosing the desired number of users, items, and starting number of sam- ples to be useful for companies and researchers as a base for simulations, anticipating diverse future scenarios, or as ground data for new machine learning models. The proposed WGANRS first converts (compresses) the input sparse dataset to its embedding-based representation and converts (decompresses) the generated (fake) embed- ding-based dataset to its raw and sparse representation. A DeepMF model (left side in Fig. 2 ) was chosen to perform the compression stage due to its simplicity and performance, and K -means clustering (right side in Fig. 2 ) was used to run the necessary decompression. In this scenario, the K -means algorithm has the advantage of setting the K * number of users and K ** number of items we want the generated dataset to hold. Finally, our architecture kernel is based on a WGAN model (centre of Fig. 2 ) to generate fake embedding samples from real embedding samples. The DeepMF model used for the compression task has a previous learning stage (top-left draw in Fig. 3 ) where the Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2475 embedding weights are set by means of backpropagation optimization. The DeepMF model contains two separate embedding layers: one layer for users and the other layer for items. These embeddings must have the same size, which usually ranges from 5 to 15 neurons. It is expected that similar users will be coded with similar embedding maps (codes), and the same applies for items. Once the DeepMF model has been trained, we can feedforward each existing user ID to obtain its embedding representation (top-right draw in Fig. 3 ). The same process is performed with all existing item IDs. Thus, we obtain a matrix I x E containing the embedding representations of the items, where I is the number of items in the source dataset and E is the embed- ding size. Analogously, we obtain a matrix U x E containing the embedding representations of the users, where U is the number of users in the source dataset. By combining the source dataset (left side of Fig. 2 ), the compressed item matrix, and the compressed user matrix (top right draw in Fig. 3 ), we can obtain the embedding rep- resentation of the source dataset, as shown in the ‚Äúembed- ding-based CF dataset‚Äù in the bottom left graph of Fig. 3 and in Fig. 2 . Starting from the embedding-based CF dataset as a source, the proposed WGANRS architecture generates the ‚Äúfake embedding-based CF dataset‚Äù (centre of Fig. 2 ), and it is performed by means of a Wasserstein GAN. The first stage of this generative task is the WGAN training (bottom-left in Fig. 3 ), where the generator model creates synthetic samples from Gaussian stochastic vectors containing random noise. Then, the Wasserstein critic (discriminator) performs the necessary binary classification to label samples as ‚Äòreal‚Äô or ‚Äòfake‚Äô. Notably, the ‚Äòfake‚Äô samples come from the generator model, whereas the ‚Äòreal‚Äô samples are randomly taken from the previously generated ‚Äòreal embedding-based CF data- set‚Äô. Once the training process has finished, we can forget the critic model and take the generator model to create as many fake embedding samples as needed. The whole pro- cess (training and feedforward generation) is expected to be fast due to the compressed embedding representation and accurate due to the Wasserstein restrictions to avoid mode collapse. In the last stage of the proposed architecture, it is neces- sary to decompress the ‚Äòfake embedding-based CF dataset‚Äô (right side in Fig. 2 and bottom right draw in Fig. 3 ). In this stage, we have generated a very large number of fake samples, consisting of tuples‚Äâ< user_embedding,item_ embedding,rating >‚Äâ, where both the user and the item embeddings produce vectors of real numbers. We have to convert this set of tuples to a discretized version‚Äâ< user_ ID,item_ID,rating >‚Äâ, where user_IDs are integers in the range [1..number of users], and analogously item_IDs are integers in the range [1..number of items]. Once the neural network has been trained, the user embedding assigns simi- lar codes to similar users (same with the embedding layer). This inherent property of the embedding layers makes it possible to incorporate a clustering process to the proposed Fig.‚ÄØ1 Innovation of the proposed method and its expected impact in solving the GAN mode collapse. Figure 1 a shows the traditional GAN approach in the CF context, Fig. 1 b shows the improvement introduced in the baseline method to adequately process sparse data, and Fig. 1 c details the proposed introduction of the Wasserstein ker- nel to reduce the mode collapse problem J. Bobadilla, A. Guti√©rrez 2476 method, in charge of grouping similar users and items to the desired number of users and items in each synthetic dataset. This is a discretization process in which the WGANRS has been designed to set the desired number of users and items in the generated dataset. To implement it, K-Means clustering [ 31 ] was performed, since it allows us to set the K* number of users and the K** number of items. Figure 4 shows the follow- ing concept: a K-means is used to cluster K* users, whereas another K-Means process is used to cluster K** items. Since similar users should have similar embeddings, it is expected that they will be grouped in the same clusters, analogously with items. Each user number in the generated dataset corre- sponds to the cluster number in the K-Means where the fake user embedding has been grouped. For example, the left-most sample in the fake embedding-based CF dataset (left side of Fig. 4 ) was grouped with its user (green colour) in the K* group and its item (blue colour) in Group 3. Consequently, the generated fake sample in the synthetic dataset is‚Äâ<‚ÄâK*, 3, rating‚Äâ>‚Äâ. In this example, the ‚Äòrating‚Äô is the value of the first sample of the source dataset (left side of Fig. 2 ). Finally, due to the discretization process, duplicated sam- ples can be found. This happens when two different generated samples share the same user ID and the same item ID. When the chosen number of users and items is high, it is more diffi- cult to find duplicated samples since there is a wider variety of clusters, and it is expected that the users and the items will be assigned to the groups in a balanced way. Duplicated samples can be managed by simply removing the spare samples. The expected balance in the clustering groups could be ‚Äòbroken‚Äô if the mode collapse is happening in the GAN. Indeed, the Was- serstein concept is used in this paper to avoid mode collapse, and the number of removed samples will be used as a quality measure in the results section. The lower the removed samples are, the better the method is. Since two of the hyperparameters in the proposed model are the number of users and the number of items, the cluster- ing method that better fits this information is K-Means, where directly we can set K* as the number of users and K** as the number of items. In fact, this is one of the unusual situations where the K value is known before the clustering process. Thus, other relevant clustering methods such as hierarchical, distribution, density or fuzzy-based ones are not adequate in this scenario. In the following subsection, the WGANRS approach is for- malized. Moreover, equations have been provided. 2.2‚ÄÇ Formalization 2.2.1‚ÄÇ CF definitions First, we define the main sets in the RS: set of users U , items I , range of ratings V , and existing samples S . (1) We let U be the set of users who make use of a CF RS (2) We let I be the set of items available to vote on in the CF RS (3) We let V be the range of allowed votes , where V = { 1, 2, 3, 4, 5 } (4) We let S be the set of samples contained in the CF dataset , where N = | S | = the total number of cast votes (5) S = { < u , i , v > 1 , < u , i , v > 2 , ‚Ä¶ , < u , i , v > N } , where each u ‚àà{ 1, ‚Ä¶ , | U | }each i ‚àà{ 1, ‚Ä¶ , | I | } , and each v ‚àà{ 1, ‚Ä¶ , | V | } Fig.‚ÄØ2 Overview of the proposed WGANRS architecture Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2477 The formalization of the defined CF dataset consists of a set of tuples <userID, itemID, rating (number of stars)>, where the ‚Äòrating‚Äô is the vote assigned for the ‚ÄòuserID‚Äô to the ‚ÄòitemID‚Äô. 2.2.2‚ÄÇ DeepMF training (6) The Deep MF training ( top ‚àíleft graph in Figure 3 )is conducted to create a model that can embed each user ID and each item ID . These two embeddings feed the WGANRS generative stage . Each embedding is a compressed representation of the user or the item . The embeddings are unidimensional vectors of size E + 1. We define f eu ( u ) as the function that compresses the user u information and analogously f ei ( i ) as the function that compresses the item i information (7) We let E + 1 be the size of the two neural layer embeddings used to vectorize each user and each item belonging to U and I , respectively . We let f eu ( u ) = ‚Éó e u = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] , where f eu is the embedding layer output of the users and u ‚àà{ 1, ‚Ä¶ , | U | } (8) We let f ei ( i ) = ‚Éó e i = [ e i 0 , e i 1 , ‚Ä¶ , e i E ] , where f ei is the embedding layer output of the items and i ‚àà{ 1, ‚Ä¶ , | I | } Fig.‚ÄØ3 DeepMF and WGAN models involved in the WGANRS architecture Below, the most relevant equations in the back propaga- tion algorithm are defined, and we set the output error as the mean squared differences metric. These equations are not distinctive of the proposed method. 2.2.3‚ÄÇ DeepMF feedforward Once DeepMF has learned, we can collect the embedding representation of each user and each item in the CF RS. Therefore, all the existing itemID and userID in the RS (9) By combining the dense vectors of the user and item embeddings ( ‚Éó e u = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] and ‚Éó e i = [ e i 0 , e i 1 , ‚Ä¶ , e i E ]) , we can make rating predictions in the DeepMF training stage . The dot product of the user embedding and the item embedding in each < u , i , v > j ‚àà S provides its rating prediction .  y j = f eu ( u ) ‚àô f ei ( i ) = ‚Éó e iu ‚àô ‚Éó e i = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] ‚àô[ e i 0 , e i 1 , ‚Ä¶ , e i E ] (10) 1 2 ( y j ‚àí ÃÇ y j ) 2 is the output error used in the DeepMF neural network to start the back propagation algorithm , where the neural weights are iteratively improved from the Ìõø j values , Œî w ji = Ìõº y j f  ( Net i ) ‚àë k w ik Ìõø k , where k is a hidden layer , and Œî w ji = Ìõº y j f  ( Net i ) 1 2 ( y k ‚àí ÃÇ y k ) 2 if k is the output layer. i , j , and k are successive sequential layers . J. Bobadilla, A. Guti√©rrez 2478 dataset feed the trained DeepMF model, and their embed- ded representations can then be obtained. It can be done by making the feedforward prediction operation (top-right graph in Figure 3 ) on the trained DeepMF model. (11) We let E ‚àó = { < u , ‚Éó e u = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] > , ‚àÄ u ‚àà U } be the set of embeddings for all the RS users (12) We let E ‚àó ( u ) = ‚Éó e u = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] (13) Let E ‚àó‚àó = { < i , ‚Éó e u = [ e u 0 , e u 1 , ‚Ä¶ , e u E ] , ‚Éó e i = [ e i 0 , e i 1 , ‚Ä¶ , e i E ] > , ‚àÄ i ‚àà I } be the set of embeddings for all the RS items . (14) We let E ‚àó‚àó ( i ) = ‚Éó e i = [ e i 0 , e i 1 , ‚Ä¶ , e i E ] 2.2.4‚ÄÇ Setting the¬†dataset of¬†the¬†embeddings Now, we collect the above embedding representations of all the itemID and userID in the RS to translate the set ‚ÄòS‚Äô (5) to the set ‚ÄòR‚Äô (15). The set ‚ÄòR‚Äô is the embedding-based dataset version of the original RAW dataset. We let (15) R = [ < E ‚àó ( u ) , E ‚àó‚àó ( i ) , v > ] , ‚àÄ < u , i , v > j ‚àà S be the embedding ‚àíbased dataset of real samples 2.2.5‚ÄÇ WGAN training The core concept of the GAN methodology is to jointly train a generator model and a discriminator model. Once the architecture has been trained, the generator model creates new samples that follow the distribution of the training samples. The discriminator model attempts to differentiate between real samples and generated ones. This is a min-max optimization problem of the form: Min G Max D ( Ìîº x ‚àΩ p data [ log D ( x ) ] + Ìîº z ‚àΩ p latent [ log ( 1 ‚àí D ( G ( z )) ] ) , w h e r e G ‚à∂ Z ‚Üí X is the generator model, which maps from the latent space Z to the input space X ; D ‚à∂ X ‚Üí ‚Ñù is the dis- criminator model, which maps from the input space X to a clas- sification value (real/fake). ‚Ñù ‚Üí ‚Ñù is a concave function. The above formula is the optimization function, the expression that both networks (generator and discriminator) try to optimize. G aims to minimize it, whereas D wants to maximize it. The bottom-left graph in Fig. 3 shows the generative learning. The GAN architecture consists of a discriminator classifier (16) and a generator model (17). The generator creates fake samples (RS profiles, in our case), whereas the discriminator tries to detect fake samples. In generative adversarial training, the discriminator progressively learns to accurately differentiate fake profiles from real profiles. At the same time, the generator learns to make fake sam- ples difficult to distinguish from real profiles. We call f D (16) the discriminator model and f G (17) the generator Fig.‚ÄØ4 Clustering stage in the WGANRS architecture Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2479 model. Both f D and f G will iteratively learn and improve themselves by minimizing a loss function (18) f GD . f GD = Min G Max D f ( D , G ) = E R [ f w ( R )] + E z [ f w ( G ( z )] , where E R is the expected value for real samples, z is the random noise that feeds generator G , and E z is the expected value for the generated fake profiles G(z). f w is the Wasserstein function based on the earth mover‚Äôs distance . This function satisfies the 1 ‚Äì Lipschitz constraint: | || f ( x 1 ) ‚àíf ( x 2 )||| ‚â§ || x 1 ‚àí x 2 || , ‚àÄ x 1 , x 2 . R refers (16) We let f D be the discriminator D model belonging to a GAN model (17) We let f G be the generator G model belonging to a GAN model (18) We let f GD be the cost function of the GAN model to (15). Notably, the Wasserstein concept has been introduced in Eq.¬†( 18 ). Mainly, it is implemented in the loss function code of the generative model. The Wasserstein approach has been designed to reduce the mode collapse problem inherent to the GAN. We implement it to reduce the mode collapse in our WGANRS proposed method and to consequently reduce the number of excessively generated similar profiles in the RS. 2.2.6‚ÄÇ WGAN generation Once the GAN has been trained, we can generate as many samples as needed. We can introduce batches of random Gaussian noise vectors ‚Äòz‚Äô and implement the feedforward process ( model.predict ). The result are batches of fake embedded prof i les (bottom-right graph in Figure 3 ). (19) We let F = f G be the generated dataset of fake samples from different random noise vectors z 2.2.7‚ÄÇ Clustering of¬†items and¬†users Figure 4 shows the clustering process, where we set K* (20) as the number of users in the generated dataset and K** (21) as the number of items on it. The N gen- erated fake profiles will contain both fake user embed- dings and fake item embeddings (where N >> K* and N >> K** ). (20) We let K ‚àó be the number of clusters used to group the embeddings of the users (21) We let K ‚àó‚àó be the number of clusters used to group the embeddings of the items For each generated fake user (each user of N), we must select the nearest user from the K* existing users (22). The h ‚àó ( u ) function makes this group. The same process is used for items. For each generated fake item (each one of the N), we must select the nearest item from the K** existing items (23). The h ‚àó‚àó ( i ) function makes this group. (22) We let h ‚àó ( u ) = c | c ‚àà{ 1, ‚Ä¶ , K ‚àó } be the clustering operation that assigns a centroid to each user . (23) We let h ‚àó‚àó ( i ) = c | c ‚àà{ 1, ‚Ä¶ , K ‚àó‚àó } be the clustering operation that assigns a centroid to each item 2.2.8‚ÄÇ Setting the¬†dataset of¬†the¬†item IDs and¬†user IDs To create the synthetic dataset, the generated set of embed- dings F (19) is converted to its discretized version H (24). We let H be the item ID and user ID discrete dataset obtained from the embedding-based dataset F of fake samples. Sometimes, different generated samples in Set F will be discretized to the same user and item: < h ‚àó ( u ) , h ‚àó‚àó ( i ) , v > = < h ‚àó( u ) , h ‚àó‚àó( i ) , v > . (24) H = [ < h ‚àó ( u ) , h ‚àó‚àó ( i ) , v > | ‚àÄ < E ‚àó ( u ) , E ‚àó‚àó ( i ) , v > ‚àà F ] This particularly happens if the GAN suffers from mode collapse. In these cases, there are samples with identical information, and we create the set H where duplicated sam- ples are removed (25). We let S = { H } be the synthetic generated dataset version of H where duplicated samples are removed. (25) The last transformation removes samples when a fake user casts different votes ( v, v ) to the same item . (26) We let G  = { < h ‚àó ( u ) , h ‚àó‚àó ( i ) , v > ‚àà H | ‚àÑ < h ‚àó ( u  ) , h ‚àó‚àó ( i  ) , v  > ‚àà H , where h ‚àó ( u ) = h ‚àó ( u  ) ‚àß h ‚àó ( i ) = h ‚àó‚àó ( i ) ‚àß v ‚â† v  } J. Bobadilla, A. Guti√©rrez 2480 3‚ÄÇ Results The proposed method has been tested using two open-source representative CF datasets: MovieLens 1¬†M ( https://‚Äãgroup‚Äãlens.‚Äã org/‚Äãdatas‚Äãets/‚Äãmovie‚Äãlens/‚Äã1m/ ) and a subset of Netflix that we call Netflix* [ 32 ]. These two datasets were chosen because they are representative in the CF field. MovieLens is probably the most tested dataset family over the years in CF research. The results obtained in MovieLens are very informative for RS researchers. On the other hand, Netflix is not widely used due to its enor- mous size and because it is no longer available. We utilized the open version of Netflix* that is randomly shortened [ 33 ]. Net- flix* has been selected as the dataset for this research because its internal patterns are different from those of MovieLens. MovieLens has been created in a relatively short time in an academic environment, whereas Netflix is an enormous com- mercial dataset that has been growing for a long period. Since this research involves catching the internal patterns of a source dataset and parameterizing and translating those patterns to a generated dataset, it is convenient to use such different sources. Table 1 shows their main parameter values. The results of both datasets follow the same trends. Therefore, to ensure that the length of this paper is appropriate, we only explain the Mov- ieLens results and group the Netflix* results (Figs. 10 , 11 , 12 and 13 ) in Appendix A. To test the WGANRS method, two sets of synthetic datasets have been created. The first set has a varied number of users, whereas the second set has a varied number of items. Each row in Table 2 shows the values of each set. The first row indicates that five synthetic datasets have been generated. The first dataset contains 500 users and 1000 items, the second dataset holds 1000 users and 1000 items, and so on until the last dataset has 8000 users and 1000 items. All the generated datasets were created starting from 400 thousand samples. This set of data allows us to test the impact of changing the number of users. Analogously, the second row in Table 2 shows that four synthetic datasets have been created. All four datasets have 4000 users. However, the number of items varies from 500 in the first dataset to 4000 in the last set. This allows us to measure the impact of changing the number of items. Since we will use the GANRS method [ 29 ] as the baseline, all datasets have also been created using GANRS. Thirty-six datasets were generated, 5‚Äâ+‚Äâ4 using MovieLens as a source and 5‚Äâ+‚Äâ4 using Netflix*, both for GANRS and for WGANRS. To test these datasets, four different exections were conducted: ‚Ä¢ Number of generated samples : The number of samples returned by GANRS and WGANRS was compared. The WGANRS method is expected to perform better, as it focuses particularly on avoiding the typical ‚Äòmode collapse‚Äô in GANs. The better managed the mode col- lapse is, the more varied the embedding samples that the WGANRS generates, the better the performance of the clustering process to create the raw samples, and finally, the lesser the number of sample collisions, increasing the sizes of the synthetic datasets. ‚Ä¢ Rating distributions : It is important that the rating distri- bution of the generated datasets be as similar as possible to that of the source, particularly on the relevant ratings (usually 4 and 5 stars). This is an indication that the pat- terns of the fake profiles are correct, and they contain an adequate proportion of relevant and nonrelevant votes. ‚Ä¢ User and item distributions : It is interesting to test the user and item distributions as the number of users and items var- ies, comparing them to the source dataset. It is expected that the Gaussian random noise used to create the stochastic vector that feeds the WGANRS generator will force the dif- ferent Gaussian distributions of users and items. ‚Ä¢ Precision and recall : Regarding the ground distributions, balanced votes, and high number of samples, the gen- erated datasets are used to adequately analyse them on the CF task, and their recommendation quality measures return suitable values and trends. The above executions cover the potential comparatives avail- able on the generative creation of synthetic datasets in the CF area. Figure 5 shows the concept. Figure 5 a (top graph) shows the traditional CF analysis. Different state-of-the-art methods or models are applied to one or several existing CF datasets, and the recommendation results can be measured using traditional quality prediction and recommendation quality metrics, such as the precision, recall, F1, and mean absolute error (MAE). However, the field of synthetic CF dataset generation is com- pletely different. From a source dataset (Fig. 5 b), we create one or several synthetic datasets. We can set different numbers of users, items, samples, etc., and each generated dataset will hold. To compare the proposed generative method with the selected state-of-the-art baseline, we must create a synthetic dataset or group of synthetic datasets using the proposed method (orange datasets in Fig. 5 b) and a different dataset or group of datasets using the SOTA baseline (blue datasets in Fig. 5 b). Now, we need to determine which of these datasets (or groups) are bet- ter. Comparing generated datasets (Fig. 5 b) is a very different task than comparing methods or models applied to an existing dataset (Fig. 5 a). Figure 5 b shows three types of code execution that we can perform to decide whether the generated datasets using the proposed method (orange datasets) are better than the generated datasets using the baseline method (blue datasets): Table‚ÄØ1 Main parameter values of the tested datasets Dataset #users #items #ratings scores sparsity Movielens 1¬†M 6,040 3,706 911,031 1 to 5 95,94 Netflix* 23,012 1,750 535,421 1 to 5 98.68 Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2481 1)	 The mode collapse impact can be measured in the CF field by removing very similar user profiles. The GAN can collapse to a reduced number of source profiles. The higher the number of deleted profiles is, the higher the mode collapse impact. The higher the deleted profiles are, the lower the number of samples in the generated dataset. Figure 5 b1 shows a comparison where the y -axis represents the number of samples. The proposed method (orange colour) improves the SOTA baseline. 2)	 The generated datasets should have probability distribu- tions that are similar to that of the source dataset. The user, item, and rating distributions of the synthetic data- sets can be compared. Figure 5 b2 shows the distribu- tion of the ratings from the source dataset (green colour) compared to the ‚Äòbaseline‚Äô dataset (orange colour, in this graph) and the ‚Äòproposed method‚Äô dataset (blue colour). Notably, the synthetic datasets are not expected to have the same distributions as the source dataset. The genera- tive process should create similar datasets, not identical datasets. Identical datasets have no value, just as rep- licating real faces is not valuable in field of computer vision. Therefore, there is no absolute metric to measure this type of quality. Extreme distribution similarities and large distribution differences must be avoided. 3)	 The quality of the generated datasets can be indirectly meas- ured by running state-of-the-art CF methods and models on them. We expect similar behaviours to those obtained when we apply these methods to the source dataset. Very different trends or absolute values in the generated dataset graphs, compared to those in the source dataset, will tell us that the generated dataset does not contain the main patterns of the source dataset. Figure 5 b3 shows the precision and recall results on the source dataset (left graph) and the generated dataset (right graph) when measured with several SOTA CF deep learning models. Both trends and values are similar. As explained above, we do not expect identical behaviours and values since synthetic datasets should mimic the source pat- terns and not copy them. For this reason, there is no standard quality measure to compare the graphs in Fig. 5 b3. Taking into account the above considerations, several experiments have been performed on the three explained approaches, as shown in b1, b2, and b3 of Fig. 5 . Individual executions and their explained results have been structured in SubSects. 3.1 to 3.4 , followed by the Discussion Subsection. 3.1‚ÄÇ Number of¬†generated samples As explained in the previous section, the proposed method uses a WGAN to generate synthetic embedding samples. These dense and continuous samples are then converted to their sparse and discrete versions by means of the clustering process and their translation to the raw tuples in the synthetic dataset. This discretization stage causes a proportion of ‚Äòcollisions‚Äô where identical or similar generated samples must be removed. The smaller the number of samples removed, the more accu- rate the generative model, and the richer the synthetic dataset. The Wasserstein GAN is expected to improve the results, as it is designed to prevent mode collapse inherent to the GAN models. Please note that the hypothesis is that by reducing the mode collapse, the variability of the generated embedded samples will increase, and then the clustering process will be able to spread users and item IDs in a more homogeneous way. Consequently, the number of discretized samples that are repeated (and deleted) will decrease. Overall, the total size of the generated datasets will increase as the GAN mode collapse is reduced using the Wasserstein approach. The final number of samples generated is a relevant qual- ity measure since it is directly related to the impact of mode collapse in the generative process. The baseline GANRS method suffers from the mode collapse problem, leading to the generation of repeated fake profiles. The method han- dles this situation by removing spare profiles, but it does not provide the necessary diversity of samples. The proposed WGANRS is expected to improve the results due to the Wasserstein ability to reduce the mode collapse and then to improve diversity and increase the synthetic dataset size. Figure 6 shows the comparison of GANRS (gan) versus WGANRS (wgan) both for synthetic datasets where the number of users varies (left graph) and for synthetic datasets where the number of items varies (right graph). Overall, the proposed approach (wgan) significantly improves the base- line (gan). Specifically, it duplicates the number of gener- ated samples. A 213% improvement in the left graph and a 191% improvement in the right graph are achieved. This is a relevant predictor of the superiority of the proposed method. Additionally, as expected, the higher the number of users or items there are, the higher the number of generated sam- ples. This is because the clustering process can better spread the samples in the latent space as the number of centroids increases. Then, the number of duplicated samples decreases. The results show a relevant improvement when the pro- posed method is applied compared to the baseline. This con- firms that the paper hypothesis is fulfilled. Moreover, incor- porating the Wasserstein concept into the generative kernel of the GANRS method will lead to a decrease in the mode collapse problem inherent to the GAN when applied to CF scenarios. Generated datasets have less redundant profiles. Accordingly, they are more diverse, and they contain more Table‚ÄØ2 Parameter values initial #samples #users #items 400,000 {500, 1000, 2000, 4000, 8000} 1,000 400,000 4,000 {500, 1000, 2000, 4000} J. Bobadilla, A. Guti√©rrez 2482 samples. Overall, the proposed WGANRS method generates richer, unbiased, and longer synthetic datasets. 3.2‚ÄÇ Rating distributions The distribution of the ratings (one star, two stars, ‚Ä¶, five stars) is an important quality measure in the CF synthetic dataset generation process. Recommendation models are very sensitive to the relevant versus non-relevant thresh- old, which is usually set to four or five stars in CF datasets containing five possible ratings. It is not enough that the Gaussian distribution of ratings in the generated dataset has a similar mean to the Gaussian distribution in the source dataset. It is also necessary that their standard deviation be analogous. Figure 7 shows that the proposed WGANRS generates a Gaussian distribution more similar to the Mov- ieLens distribution than the baseline GANRS. Specifically, it achieves a 271.21% improvement. The improvement aver- age obtained using the synthetic datasets in the first row of Table 2 (the number of users varies) is 304% (541% in Net- flix*), whereas the second row (the number of items varies) returns a 357% improvement on average (399% in Netflix*). It is expected that these positive results will contribute to providing adequate recommendation quality results in the next subsection. Beyond the numeric improvement values shown before, we can compare the shapes of the probability distribution in Fig. 7 . The probability distribution of the source MovieLens dataset (green-colour bars) is the target. The proposed WGANRS method (blue-colour bars) is much closer to the target than the baseline GANRS method (orange-colour bars). This is the reason for the relevant numerical improvements shown in the above paragraph. Additionally, the baseline method generates a Gaussian distribution excessively centred in the average rating (three stars), whereas the proposed method adequately fits its Gaussian distribution to the correct four-star mean. Regard- ing the Gaussian standard deviation, the baseline method does not adequately catch the source dataset shape. Its deviation is smaller, and consequently, it does not generate enough profiles in the distribution edges (one star and five stars). In contrast, the proposed method performs nearly perfectly on both edges of the source distribution. Thus, the samples generated using the proposed WGANRS method are more diverse and unbiased than those obtained running the SOTA baseline. Additionally, the obtained result better follows the Gaussian distribution that describes the source shape of ratings. This result reinforces and Fig.‚ÄØ5 Traditional CF validation of the methods and models versus the validation of the synthetic datasets generated by the GAN Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2483 complements that obtained in Sect. 3.1 . Overall, the proposed method a) reduces repeated samples, b) generates more sam- ples, c) increases diversity, d) decreases the bias, and e) better mimics the probability distributions of the ratings. 3.3‚ÄÇ Precision and¬†recall In this subsection, we show the recommendation quality results obtained on synthetic datasets obtained using Mov- ieLens as a source. The WGANRS method was used for this experiment to generate the synthetic datasets. The state-of- the-art NCF (neural collaborative filtering) deep learning model has been used to make predictions and recommenda- tions. The relevancy ÌúÉ threshold was set to five. The top graphs in Fig. 8 show the results when the number of users varies, whereas the bottom graphs show the results when the number of items varies. Both the values and the trends obtained from the synthetic datasets (coloured curves) are similar and com- patible with the source datasets (black curves), which indi- cates that the proposed method generates suitable synthetic datasets to be used in the RS field. Additionally, as expected, the higher the number of users, the higher the recall is, since each user profile will contain fewer relevant ratings (recall denominator). Conversely, the higher the number of users, the lower the precision is, since the denominator is the con- stant N (number of recommendations), whereas the numerator contains the true positives of relevant ratings, where a high number of users involves less relevant ratings per user. Additionally, from the set of synthetic datasets where the number of users varies, the dataset that holds 1000 users pro- vides more precision and recall results like the MovieLens source. Since MovieLens 1¬†M contains 6000 users, it tells us that the GAN-based method generates data patterns where recommendations are easier than using the source dataset. This result is consistent with the one reported in [ 30 ]. Most importantly, the evolution of all the recommendation curves in the generated datasets (coloured curves) follow the same trends as those exhibited by the source MovieLens (black curve), indicating that the internal patterns of the source data- set have been adequately captured by the proposed WGANRS method. Regarding the results when the number of items var- ies, similar conclusions can be drawn, underlying that rec- ommendation qualities worsen in absolute values compared to the source dataset. This probably occurs because the dis- tribution of the item ratings is highly variable compared to the distribution of the user ratings, leading to more difficult pattern extraction. There is a number of items holding a very low number of ratings. 3.4‚ÄÇ User and¬†item distributions Once the rating distributions have been tested, it is also convenient to compare the user and the item distributions obtained by using both the proposed and the baseline methods. The user and item distributions of the synthetic datasets are very dependent on the Gaussian parameter values with which the noise vectors that feed the generative model have been created. In the original paper [ 28 ] that serves as a baseline, the standard deviation has been customized for each tested dataset. In contrast, by using the proposed method, we fixed it to one and then removed this hyperparameter, making it easier to fine tune the proposed approach compared to the baseline method. The top graph in Figure 9 shows the results when the number of users varies, while its bottom graph shows the result by varying the number of items. Dashed lines repre- sent the baseline results, and solid lines show the proposed approaches. In all cases, as expected, the higher the number of users there are, the lower the number of ratings assigned to Fig.‚ÄØ6 Number of samples generated using the baseline GANRS method (gan) versus the proposed WGANRS method (wgan).  Source dataset: MovieLens 1¬† M. Number of samples needed: 40,000. Left graph: generated datasets with 1000 items and a range of 500, 1000, 2000, 4000 and 8000 users. Right graph: generated datasets with 4000 users and a range of 500, 1000, 2000 and 4000 items. The higher the number of generated samples, the better the model is J. Bobadilla, A. Guti√©rrez 2484 each user, since the number of ratings in each dataset is fixed. It can also be observed that the proposed method generates Gaussian distributions with higher standard deviations than the baseline approach, which has been heuristically tailored to the dataset. Both the proposed and baseline methods generate suitable user and item distributions for the CF area. 3.5‚ÄÇ Discussion The experimental results show the superiority of the proposed WGANRS method compared to the GANRS baseline. Par- ticularly relevant is the high improvement (approximately 200%) in the number of generated samples. This indicates that the proposed Wasserstein approach effectively reduces the amount of mode collapse of the GAN, The WGANRS method also effectively mimics the rating distribution of the source dataset, obtaining high improvements compared to the baseline and making it possible that their quality precision and recall values and trends are compatible with those from the source dataset. Furthermore, even no standard quality meas- ures exist to test RS generated data, the user and item distribu- tions obtained using the proposed approach are comparable to those of the baseline method. Additionally, the proposed method has the advantage that it is not necessary to assign heuristic values to the standard deviation of the Gaussian dis- tribution used to create the noisy random vectors that feed the generator model of the WGAN. Finally, the results using the Netflix* dataset reinforce the results obtained by testing MovieLens. Appendix A shows the Netflix* results. Overall, the proposed method improves both the sta- tistical baselines and state-of-the-art generative methods. Statistical baselines are reported to reach poor accuracy. In contrast, they support adequate parameterization. Generative baselines operate quite differently. They do not support full parameterization and exceed the accuracy of statistical meth- ods [ 24 ]. Our proposed method is proven to provide both full Fig.‚ÄØ7 Comparative rating distributions among the MovieLens 1¬† M (ML) source dataset, the baseline GANRS method (gan), and the pro- posed WGANRS method (wgan). The 8000-user and 1000-item syn- thetic dataset has been chosen as a representative case from the set of generated data in the paper. The closer the distribution is to the source ML distribution, the better the model is Fig.‚ÄØ8 Quality of the recom- mendation: precision and recall obtained by varying the number N of recommendations from 2 to 10. The relevancy threshold ÌúÉ was set to 5. The upper graphs show the results on the synthetic datasets containing 500 to 8000 users. The lower graphs show the results on the synthetic datasets containing 500 to 4000 items. Precision can be seen in the left graphs, whereas recall is shown in the right graphs. The MovieLens dataset was used. The higher the values are, the better the results Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2485 parameterization and high accuracy, in addition to a strong reduction of the mode collapse problem inherent to the GAN architectures. On the other hand, our method inherits the most positive and the most negative features of its baseline [ 30 ]. However, its accuracy and performance are very high due to the short, dense, and continuous vectors that its GAN model takes as input. Its main drawback comes from the clustering stage of the method (Fig. 4 ), which requires addi- tional execution time and involves a discretization process that increases the probability of generating duplicated sam- ples. For this reason, the Wasserstein concept has been pro- posed to alleviate the explained drawback. The results show that the proposed method adequately reduces the mode col- lapse problem, maintains the baseline advantages, reduces its disadvantages, and confirms the hypothesis of this paper. 4‚ÄÇ Conclusions The most relevant conclusion in this paper is that the Wasser- stein approach reduces the mode collapse in the GAN genera- tion of the CF fake samples compared to the state-of-the-art methods. This positive effect is reflected in a relevant reduction in duplicated samples and consequently in the generation of larger synthetic datasets. Furthermore, the proposed approach returns very improved distributions of ratings, which facilitates obtaining correct values and trends in recommendation qual- ity measures. Finally, the distributions of the users and items are comparable to those of the state-of-the-art methods; these distributions act as quality measures due to the lack of stand- ard quality measures for RS generated data. Moreover, exist- ing hyperparameters are avoided in the proposed method. The standard deviation of the Gaussian distribution is used to create the noisy vectors that feed the generator model in the GAN. Overall, the results of the experiment show that by applying the Wasserstein distance and weight clipping to CF data, the generative process is improved compared to the state-of-the-art methods that use Wasserstein-based GANs. Proposed future work includes a) testing the proposed method on different RS datasets, with several sparsity ratios and different numbers of users or items, b) comparing the existing biases in the source datasets with the generated biases in the synthetic datasets, and c) checking the ability of the generated samples to serve as data augmentation when they are added to the source datasets. Fig.‚ÄØ9 Top graph: distribu- tion of the ratings when the number of users varies from 500 to 8000; comparative of the proposed WGANRS (wgan) method and the baseline GANRS (gan) method. Bottom graph: distribution of ratings when the number of items var- ies from 500 to 4000; compari- son of the proposed WGANRS (wgan) method and the baseline GANRS (gan) method. The source MovieLens (ML) dataset is used in both graphs J. Bobadilla, A. Guti√©rrez 2486 Appendix In this section, the same figures used for MovieLens are shown. However, in this case, the Netflix* dataset has been used as a source. Fig.‚ÄØ10 Number of samples generated using the baseline GANRS method (gan) versus the proposed WGANRS method (wgan).  Source dataset: Netflix*. Number of needed samples: 40,000. Left graph: generated datasets with 1000 items and a range of 500, 1000, 2000, 4000 and 8000 users; right graph: generated datasets with 4000 users and a range of 500, 1000, 2000 and 4000 items. The higher the num- ber of generated samples, the better the model is Fig.‚ÄØ11 Comparative rating distributions among the Netflix* source dataset, the baseline GANRS method (gan) and the proposed WGANRS method (wgan). The 8000-users and 1000-items synthetic dataset has been chosen as a representative case from the set of generated data in the paper. The closer the distribution is to the source ML distribution, the better the model is Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2487 Fig.‚ÄØ12 Quality of recommen- dation: precision and recall obtained by varying the number N of recommendations from 2 to 10. The relevancy threshold ÌúÉ was set to 5. The upper graphs show the results on the synthetic datasets containing 500 to 8000 users. The lower graphs show the results on the synthetic datasets containing 500 to 4000 items. Precision can be seen in the left graphs, whereas recall is shown in the right graphs. The Netflix* dataset was used. The higher the values are, the better the results Fig.‚ÄØ13 Top graph: distribu- tion of the ratings when the number of users varies from 500 to 8000. Comparison of the proposed WGANRS (wgan) method and the baseline GANRS (gan) method. Bottom graph: distribution of ratings when the number of ratings var- ies from 500 to 4000. Compari- son of the proposed WGANRS (wgan) method and the baseline GANRS (gan) method. The source Netflix* dataset is used in both graph J. Bobadilla, A. Guti√©rrez 2488 Author contributions Abraham Guti√©rrez ran most of the executions and prepared the figures and the paper format. Jes√∫s Bobadilla provided the paper concept, the model design, the experimental design, and wrote the paper. Funding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature. This work was partially supported by the Ministerio de Ciencia e Innovaci√≥n of Spain under the pro- ject PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de Madrid under Convenio Plurianual with the Universidad Polit√©cnica de Madrid in the actuation line of Programa de Excelencia para el Profesorado Universitario. Data availability The datasets generated during and/or analysed during the current study are available in the GitHub repository, https://‚Äãgithub.‚Äã com/‚Äãjesus‚Äãbobad‚Äãilla/‚Äãganrs.‚Äãgit Declarations Ethics for obtaining the data In this paper, all the conditions specified for the use of the open datasets taken as a source for the generative process are satisfied, including the reference to the paper stated in the README file. Conflicts of interest The authors have no competing interests to de- clare that are relevant to the content of this article and agree to the publishing of its content. Open Access This article is licensed under a Creative Commons Attri- bution 4.0 International License, which permits use, sharing, adapta- tion, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/ . References 1.	 Shokeen J, Rana C (2020) A study on features of social recom- mender systems. Artif Intell Rev 53(2):965‚Äì988. https://‚Äãdoi.‚Äãorg/‚Äã 10.‚Äã1007/‚Äãs10462-‚Äã019-‚Äã09684-w 2.	 Bobadilla J, Guti√©rrez A, Alonso S, Gonz√°lez-Prieto A (2022) Neural Collaborative Filtering Classification Model to Obtain Prediction Reliabilities. International Journal of Interactive Mul- timedia and Artificial Intelligence 7(4):18‚Äì26. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã 9781/‚Äãijimai.‚Äã2021.‚Äã08.‚Äã010 3.	 Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender systems leveraging multimedia content. ACM Computing Surveys (CSUR) 53(5):1‚Äì38. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1145/‚Äã34071‚Äã90 4.	 Bobadilla J, Gonz√°lez-Prieto A, Ortega F, Lara-Cabrera R (2021) Deep learning feature selection to unhide demographic recom- mender systems factors. Neural Comput Appl 33(12):7291‚Äì7308. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1007/‚Äãs00521-‚Äã020-‚Äã05494-2 5.	 Bobadilla J, Lara-Cabrera R, Gonz√°lez-Prieto √Å, Ortega F (2021) DeepFair: Deep Learning for Improving Fairness in Recom- mender Systems. International Journal of Interactive Multimedia and Artificial Intelligence 6(6):86‚Äì94. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã9781/‚Äã ijimai.‚Äã2020.‚Äã11.‚Äã001 6.	 Kulkarni S, Rodd SF (2020) Context aware recommendation systems: A review of the state of the art techniques. Computer Science Review 37:100255. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãcosrev.‚Äã2020.‚Äã 100255 7.	 Wang Z (2023) Intelligent recommendation model of tourist places based on collaborative filtering and user preferences. Appl Artif Intell 37(1):2203574. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1080/‚Äã08839‚Äã514.‚Äã 2023.‚Äã22035‚Äã74 8.	 Ray B, Garain A, Sarkar R (2021) An ensemble-based hotel rec- ommender system using sentiment analysis and aspect categoriza- tion of hotel reviews. Applied Soft Computing 98:106935. https://‚Äã doi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãasoc.‚Äã2020.‚Äã106935 9.	 Kabul MS, Setiawan EB (2022) Recommender System with User-Based and Item-Based Collaborative Filtering on Twitter using K-Nearest Neighbors Classification. Journal of Computer System and Informat- ics 3:478‚Äì484. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã47065/‚Äãjosyc.‚Äãv3i4.‚Äã2204 10.	 Eslami G, Ghaderi F (2023) Incremental trust-aware matrix factor- ization for recommender systems: towards Green AI. Appl Intell 53:12599‚Äì12612. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1007/‚Äãs10489-‚Äã022-‚Äã04150-7 11.	 Mehdi HA (2022) A novel constrained non-negative matrix fac- torization method based on users and items pairwise relationship for recommender systems. Expert Syst Appl 195:116593. https://‚Äã doi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãeswa.‚Äã2022.‚Äã116593 12.	 Gheorghe P, P√©rez-Jim√©nez M, Grzegorz R (2023) Infinite Spike Trains in Spiking Neural P Systems. Romanian Journal of Infor- mation Science and Technology 2023:251‚Äì275. https://‚Äãdoi.‚Äãorg/‚Äã 10.‚Äã59277/‚ÄãROMJI‚ÄãST.‚Äã2023.3-‚Äã4.‚Äã01 13.	 Liu H, Zheng C, Li D, Shen X, Lin K, Wang J, Zhang Z, Zhang Z, Xiong N (2021) EDMF: Efficient Deep Matrix Factorization With Review Feature Learning for Industrial Recommender System. IEEE Trans Industr Inf 18(7):4361‚Äì4371. https://‚Äãdoi.‚Äã org/‚Äã10.‚Äã1109/‚ÄãTII.‚Äã2021.‚Äã31282‚Äã40 14.	 Bobadilla J, Ortega F, Guti√©rrez A, Gonz√°lez-Prieto √Å (2022) Deep variational models for collaborative filtering-based recom- mender systems. Neural Comput Appl 35:7817‚Äì7831. https://‚Äã doi.‚Äãorg/‚Äã10.‚Äã1007/‚Äãs00521-‚Äã022-‚Äã08088-2 15.	 Hai C, Fulan Q, Jie C, Shu Z, Yanping Z (2021) Attribute-based Neural Collaborative Filtering. Expert Syst Appl 185:115539. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãeswa.‚Äã2021.‚Äã115539 16.	 Min G, Junwei Z, Junliang Y, Jundong L, Junhao W, Qingyu X (2021) Recommender systems based on generative adversarial networks: A problem-driven perspective. Inf Sci 546:1166‚Äì 1185. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãins.‚Äã2020.‚Äã09.‚Äã013 17.	 Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of a recommender system with ensemble learning and graph embed- ding: a case on MovieLens. Multimedia tools and applications 80(5):7805‚Äì7832. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1007/‚Äãs11042-‚Äã020-‚Äã09949-5 18.	 Kumar A, Aggarwal RK (2022) An exploration of semi- supervised and language-adversarial transfer learning using hybrid acoustic model for hindi speech recognition. J Reli- able Intell Environ 8:117‚Äì132. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1007/‚Äã s40860-‚Äã021-‚Äã00140-7 19.	 Deldjoo Y, Noia DT, Merra FA (2021) Survey on Adversarial Recommender Systems: From Attack/Defense Strategies to Gen- erative Adversarial Networks. ACM Comput Surv 54(2):1‚Äì38. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1145/‚Äã34397‚Äã29 20.	 Chae DK, Kang JS, Kim SW, Lee JT (2018) CFGAN: a generic collaborative filtering framework based on generative adversarial networks. In: Proceedings of the 27th, ACM International Con- ference on Information and Knowledge Management, CIKM 2018. Association for Computing Machinery, New York, NY, pp 137‚Äì146. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1145/‚Äã32692‚Äã06.‚Äã32717‚Äã43 21.	 Guo G, Zhou H, Chen B et¬†al (2022) IPGAN: Generating informa- tive item pairs by adversarial sampling. IEEE Transactions on Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets 2489 Neural Networks and Learning Systems 33(2):694‚Äì706. https://‚Äã doi.‚Äãorg/‚Äã10.‚Äã1109/‚ÄãTNNLS.‚Äã2020.‚Äã30285‚Äã72 22.	 Zhao J, Li H, Qu L, Zhang Q, Sun Q, Huo H, Gong M (2022) DCF- GAN: An adversarial deep reinforcement learning framework with improved negative sampling for session-based recommender systems. Inf Sci 596:222‚Äì235. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãins.‚Äã2022.‚Äã02.‚Äã045 23.	 Sun J, Liu B, Ren H, Huang W (2022) WNCGAN: A neural adver- sarial collaborative filtering for recommender system. Journal of intelligent & fuzzy systems 42(4):2915‚Äì2923. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã 3233/‚Äãjifs-‚Äã210123 24.	 Bharadhwaj H, Park H, Lim BY (2018) RecGAN: recurrent gen- erative adversarial networks for recommendation systems. In: Pro- ceedings of the 12th ACM Conference on Recommender Systems, RecSys september 2019. Association for Computing Machinery, New York, NY, pp 372‚Äì376. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1145/‚Äã32403‚Äã23.‚Äã 32403‚Äã83 25.	 Shafqat W, Byun YC (2022) A Hybrid GAN-Based Approach to Solve Imbalanced Data Problem in Recommendation Systems. IEEE access 10:11036‚Äì11047. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1109/‚ÄãACCESS.‚Äã 2022.‚Äã31417‚Äã76 26.	 Wen J, Zhu XR, Wang CD, Tian Z (2022) A framework for per- sonalized recommendation with conditional generative adversarial networks. Knowl Inf Syst 64(10):2637‚Äì2660. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã 1007/‚Äãs10115-‚Äã022-‚Äã01719-z 27.	 Wang Q, Huang Q, Ma K, Zhang X (2021) A Recommender System Based on Model Regularization Wasserstein Generative Adversarial Network. Inf Sci 546:1166‚Äì1185. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã 1016/j.‚Äãins.‚Äã2020.‚Äã09.‚Äã013 28.	 Zhang X, Zhong J, Liu K (2021) Wasserstein autoencoders for collaborative filtering. Neural Comput Appl 33(7):2793‚Äì2802. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1007/‚Äãs00521-‚Äã020-‚Äã05117-w 29.	 Schlett T, Rathgeb C, Henniger O, Galbally J, Fierrez J, Busch C (2022) Face Image Quality Assessment: A Literature Survey. ACM Comput Surv 54(10):1‚Äì49. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1145/‚Äã35079‚Äã01 30.	 Bobadilla J, Guti√©rrez A, Yera R, Mart√≠nez L (2023) Creating Syn- thetic Datasets for Collaborative Filtering Recommender Systems using Generative Adversarial Networks. Knowledge Based Systems 280(1):111016. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãknosys.‚Äã2023.‚Äã111016 31.	 Ioan-Daniel B, Radu-Emil P, Alexandra-Bianca B (2022) Improvement of K-means Cluster Quality by Post Processing Resulted Clusters. Procedia Computer Science 199:63‚Äì70. https://‚Äã doi.‚Äãorg/‚Äã10.‚Äã1016/j.‚Äãprocs.‚Äã2022.‚Äã01.‚Äã009 32.	 Ortega F, Mayor J, L√≥pez-Fern√°ndez D, Lara-Cabrera R (2021) CF4J 2.0: adapting collaborative filtering for java to new chal- lenges of collaborative filtering based recommender sys- tems.¬†Knowledge-Based Syst 215(4):106629. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã 1016/j.‚Äãknosys.‚Äã2020.‚Äã106629 33.	 Gong Y (2023) Distribution constraining for combating mode collapse in generative adversarial networks. J Electron Imaging 32(4):43029‚Äì43030. https://‚Äãdoi.‚Äãorg/‚Äã10.‚Äã1117/1.‚ÄãJEI.‚Äã32.4.‚Äã043029 Publisher's note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. Jes√∫s Bobadilla received the B.S. and the Ph.D. degrees in computer science from the Uni- versidad Polit√©cnica de Madrid and the Universidad Carlos III. Currently, he is a full professor with the Department of Informa- tion Systems, Universidad Poli- t√©cnica de Madrid. He is a habit- ual author of programming languages books working with McGraw- Hill, Ra-Ma and Alfa Omega publishers. His research interests include information retrieval, recommender systems and speech processing. He over- sees the FilmAffinity.com research team working on the collaborative filtering kernel of the web site. He has been a researcher into the Inter- national Computer Science Institute at Berkeley University and into the Sheffield University. Abraham Guti√©rrez received the B.S. and the Ph.D. degrees in computer science from the Uni- versidad Polit√©cnica de Madrid. Currently, he is currently an associate professor with the Department of Information Sys- tems, Universidad Polit√©cnica de Madrid. He is the author of research papers in most prestig- ious international journals. He is a habitual author of program- ming languages books working with McGraw-Hill, Ra-Ma and Alfa Omega publishers. His research interests include P-Sys- tems, machine learning, data analysis and artificial intelligence. He is in charge of this group innovation issues, including the commercial projects. J. Bobadilla, A. Guti√©rrez 2490"
Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems.pdf,Comprehensive Evaluation of Matrix Factorization Models for Collaborative | Filtering Recommender Systems | Keywords | Abstract | Comprehensive Evaluation of Matrix Factorization | Models for Collaborative Filtering Recommender | Systems | R,,"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/370797388 Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems Article in International Journal of Interactive Multimedia and Artificial Intelligence ¬∑ January 2023 DOI: 10.9781/ijimai.2023.04.008 CITATIONS 3 READS 46 4 authors: Jesus Bobadilla Universidad Polit√©cnica de Madrid 97 PUBLICATIONS 6,377 CITATIONS SEE PROFILE Jorge Due√±as-Ler√≠n Comunidad de Madrid 7 PUBLICATIONS 14 CITATIONS SEE PROFILE Fernando Ortega Universidad Polit√©cnica de Madrid 69 PUBLICATIONS 5,648 CITATIONS SEE PROFILE Abraham Gutierrez Universidad La Salle M√©xico 39 PUBLICATIONS 588 CITATIONS SEE PROFILE All content following this page was uploaded by Jorge Due√±as-Ler√≠n on 20 July 2023. The user has requested enhancement of the downloaded file. - 1 - * Corresponding author. E-mail addresses: jesus.bobadilla@upm.es (J. Bobadilla), jorgedl@alumnos.upm.es (J. Due√±as-Ler√¨n), fernando.ortega@upm.es (F. Ortega), abraham.gutierrez@upm.es (A. Gutierrez). Please cite this article in press as: J. Bobadilla, J. Due√±as-Ler√≠n, F. Ortega, A. Gutierrez. Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems, International Journal of Interactive Multimedia and Artificial Intelligence, (2023), http://dx.doi.org/10.9781/ijimai.2023.04.008 Keywords Collaborative Filtering, Matrix Factorization, Recommender Systems. Abstract Matrix factorization models are the core of current commercial collaborative filtering Recommender Systems. This paper tested six representative matrix factorization models, using four collaborative filtering datasets. Experiments have tested a variety of accuracy and beyond accuracy quality measures, including prediction, recommendation of ordered and unordered lists, novelty, and diversity. Results show each convenient matrix factorization model attending to their simplicity, the required prediction quality, the necessary recommendation quality, the desired recommendation novelty and diversity, the need to explain recommendations, the adequacy of assigning semantic interpretations to hidden factors, the advisability of recommending to groups of users, and the need to obtain reliability values. To ensure the reproducibility of the experiments, an open framework has been used, and the implementation code is provided. DOI: 10.9781/ijimai.2023.04.008 Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems Jes√∫s Bobadilla, Jorge Due√±as-Ler√≠n, Fernando Ortega, Abraham Gutierrez * Dpt. Sistemas Inform√°ticos and KNODIS Research Group, Universidad Polit√©cnica de Madrid (Spain) Received 2 July 2022 | Accepted 4 March 2023 | Early Access 28 April 2023 I.	 Introduction R ecommender System (RS) [1] is the field of artificial intelligence specialized in user personalization. Mainly, RSs provide accurate item recommendations to users: movies, trips, books, music, etc. Recommendations are made following some filtering approach. The most accurate filtering approach is the Collaborative Filtering (CF) [2] , where recommending to an active user involves a first stage to make predictions about all his or her not consumed or voted items. Then, the top predicted items are recommended to the active user. The CF approach assumes the existence of a dataset that contains explicitly voted items or implicitly consumed items from a large number of users. Remarkable commercial RSs are Amazon, Spotify, Netflix, or TripAdvisor. Regardless of the machine learning model used to implement CF, the key concept is to extract user and item patterns and then to recommend to the active user those items that he or she has not voted or consumed, and that similar users have highly valued. It fits with the K Nearest Neighbors (KNN) memory-based algorithm [3] , and it is the reason why the initial RS research was based on KNN. There are also some other filtering approaches such as demographic, social, content- based, context-aware, and their ensembles. Demographic filtering [4] makes use of user information such as gender, age, or zip code, and item information such as movie genre, country to travel, etc. Social filtering [5] , [6] has a growing importance in current RS, due to the social networks boom. The existence of trust relations and graphs [7] can improve the quality of the CF recommendations. In this decentralized and dynamic environment, trust between users provides additional information to the centralized set of ratings. Trust relationships can be local, collective, or global [8] ; local information is based on shared users‚Äô opinions, collective information uses friends‚Äô opinions, whereas global information relates to users‚Äô reputation [9] . Content- based filtering [10] recommends items with the same type (content) to consumed items (e.g. to recommend Java books to a programmer that bought some other Java book). Context-aware filtering [11] uses GPS information, biometric sensor data, etc. Finally, ensemble architectures [12] get high accuracy by merging several types of filtering. Memory-based algorithms have two main drawbacks: their accuracy is not high, and each recommendation process requires to recompute the whole dataset. Model-based approaches solve both problems: their accuracy is higher than that of memory-based methods, and they first create a model from the dataset. From the created model we can make many different recommendations, and it can be efficiently updated when the dataset changes. Matrix Factorization (MF) [13] is the most popular approach to implement current RSs: it provides accurate predictions, it is conceptually simple, it has a straightforward implementation, the model learns fast, and also updates efficiently. The MF model makes a compression of information, coding very sparse and large vectors of discrete values (ratings) to low dimensional embeddings of real numbers, called hidden factors. The hidden factors, both from the user vector and from the item vector, are combined by means of a dot product to return predictions. This is an iterative process in which the distance between training predictions and their target ratings is minimized. The Probabilistic Matrix Factorization (PMF) model based on MF [13] scales linearly with the size of the data set. It also returns accurate - 2 - International Journal of Interactive Multimedia and Artificial Intelligence results when applied to sparse, large, and imbalanced CF datasets. PMF has also been extended to include an adaptive prior on the model parameters, and it can generalize adequately, providing accurate recommendation to cold-start users. CF RSs are usually biased. A typical CF bias source comes from the fact that some users tend to highly rate items (mainly 4 and 5 stars), whereas some other users tend to be more restrictive in their ratings (mainly 3 and 4 stars). This fact leads to the extension of the MF model to handle biased data. An user-based rating centrality and an item-based rating centrality [14] have been used to improve the accuracy of the regular PMF. These centrality measures are obtained by processing the degree of deviation of each rating in the overall rating distribution of the user and the item. non-Negative Matrix Factorization (NMF) [15] can extract significant features from sparse and non-negative CF datasets (please note that CF ratings are usually a non-negative number of stars, listened songs, watched movies, etc.). When nonnegativity is imposed, prediction errors are reduced and the semantic interpretability of hidden factors is easier. The Bernoulli Matrix Factorization (BeMF) [16] has been designed to provide both prediction and reliability values; this model uses the Bernoulli distribution to implement a set of binary classification approaches. The results of the binary classification are combined by means of an aggregation process. The Bayesian non-Negative Matrix Factorization (BNMF) [17] was designed to provide useful information about user groups, in addition to the PMF prediction results. The authors factorize the rating matrix into two nonnegative matrices whose components lie within the range [0, 1] . The resulting hidden factors provide an understandable probabilistic meaning. Finally, The User Ratings Profile Model (URP) is a generative latent variable model [18] ; it produces complete rating user profiles. In the URP model, first attitudes for each item are generated, then a user attitude for the item is selected from the set of existing attitudes. URP borrows several concepts from LDA [19] and the multinomial aspect model [20] . The set of MF models mentioned above: PMF, Biased Matrix Factorization (BiasedMF), NMF, BeMF, BNMF, and URP, can be considered representative in the CF area. These models will be used in this paper to compare their behavior when applied to representative datasets. Specifically, the following quality measures will be tested: Mean Absolute Error (MAE), novelty, diversity, precision, recall, and Normalized Discounted Cumulative Gain (NDCG). Prediction accuracy will be tested using MAE [21] , whereas NDCG, Precision and Recall [22] will be used to test recommendation accuracy. Modern CF models should be tested not only regarding accuracy, but also beyond accuracy properties [23] : novelty [24] , [25] and diversity [26] . Novelty can be defined as the quality of a system to avoid redundancy; diversity is a quality that helps to cope with ambiguity or under-specification. The models have been tested using four CF datasets: MovieLens (100K and 1M versions) [27] , Filmtrust [28] and MyAnimeList [29] . These are representative open datasets and are popular in RS research. Overall, this paper provides a complete evaluation of MF methods, where the PMF, BiasedMF, NMF, BeMF, BNMF, and URP models have been tested using representative CF quality measures, both for prediction and recommendation, and also beyond accuracy ones. As far as we know this is the experimental most complete work evaluating current MF models in the CF area. The rest of the paper is structured as follows: Section II introduces the tested models, the experiment design, the selected quality measures, and the chosen datasets. Section III shows the obtained results and provides their explanations in Section IV. Section V highlights the main conclusions of the paper and the suggested future works. Finally, a references section lists current research in the area. II.	 Methods and Experiments This section abstracts the fundamentals of each baseline model (PMF, BiasedMF, NMF, BeMF, BNMF, URP), introduces the tested quality measures (MAE, precision, recall, NDCG, novelty, diversity), and shows the main parameters of the tested datasets ( Movielens , FilmTrust , MyAnimeList ). Experiments are performed by combining the previous entities. The vanilla MF [13] , [30] is used to generate rating predictions from a matrix of ratings R . This matrix contains the set of casted ratings (explicit or implicit) from a set of users U to a set of items I . Since regular users only vote or consume a very limited subset of the available items, matrix R is very sparse. The MF key concept is to compress the very sparse item and user vectors of ratings to small size and dense item and user vectors of real numbers; these small size dense vectors can be considered as embeddings, and they usually are called ‚Äòhidden factors‚Äô, since each embedding factor codes some complex non-lineal (‚Äòhidden‚Äô) relation of user or item features. The parameter K is usually chosen to set the embedding (hidden factors) size. MF makes use of two matrices: P (| U |* K ) to contain the K hidden factors of each user, and Q (| I |* K ) to contain the K hidden factors of each item. To predict how much a user u likes an item i , we compare each hidden factor of u with each corresponding hidden factor of i . Then, the dot product u ‚ãÖ i can be used as suitable CF prediction measure. MF predicts ratings by minimizing errors between the original R matrix and the predicted matrix: (1) (2) Using gradient descent, we minimize learning errors (differences between real ratings r and predicted ratings ). (3) To minimize the error, we differentiate equation (3) with respect to p uk and q ki : (4) (5) Introducing the learning rate Œ± , we can iteratively update the required hidden factors p uk and q ki : (6) (7) CF datasets have biases, since different users vote or consume items in different ways. In particular, there are users who are more demanding than others when rating products or services. Analogously, there are items more valued than others on average. Biased MF [14] is designed to consider data biases; The following equations extend the previous ones, introducing the bias concept and making the necessary regularization to maintain hidden factor values in their suitable range: (8) where Œº , b u , b i are the average bias, the user bias and the item bias. - 3 - Article in Press We minimize the regularized squared error: (9) where Œª is the regularization term. Obtaining the following updating rules: (10) (11) (12) (13) NMF [15] can be considered as a regular MF subject to the following constraints: (14) In the NMF case, predictions are made by linearly combining positive coefficients (hidden factors). NMF hidden factors are easier to semantically interpret than regular MF ones: sometimes it is not straightforward to assign semantic meanings to negative coefficient values. In the CF context, another benefit of using NMF decomposition is the emergence of a natural clustering of users and items. Intuitively, users and items can be clustered according to the dominant factor (i.e. the factor having the highest value). In the same way, the original features (gender, age, item type, item year, etc.) can be grouped according to the factor (from the k hidden factors) on which they have the greatest influence. This is possible due to the condition of positivity of the coefficients. BeMF [16] is an aggregation-based architecture that combines a set of Bernoulli factorization results to provide pairs <prediction, reliability>. BeMF uses as many Bernoulli factorization processes as possible scores in the dataset. Reliability values can be used to detect shilling attacks, to explain the recommendations, and to improve prediction and recommendation accuracy [31] . BeMF is a classification model based on the Bernoulli distribution. It adequately adapts to the expected binary results of each of the possible scores in the dataset. Using BeMF, the prediction for user u to item i is a vector of probabilities , where is the probability that i is assigned the s-th score from user u . The BeMF model can be abstracted as follows: Let S = { s 1 , ‚Ä¶, s D } be the set of D possible scores in the dataset (e.g. 1 to 5 stars: D = 5 ). From R we generate D distinct matrices ; each matrix is a sparse matrix such that . BeMF will attempt to fit the matrices by performing D parallel MFs The BeMF assumes that, given the user P matrix and the item Q matrix containing k > 0 hidden factors, the rate R ui is a Bernoulli distribution with the success probability œà ( P u . Q i ) . The mass function of this random variable is: (15) The associated likelihood is: (16) The BeMF updating equations are: (17) (18) And the aggregation to obtain the final output Œ¶ : (19) where . Let ; the prediction is: , and the reliability is . BNMF [17] provides a Bayesian-based NMF model that not only allows accurate prediction of user ratings, but also to find groups of users with the same tastes, as well as to explain recommendations. The BNMF model approximates the real posterior distribution by the distribution: (20) where: ‚Ä¢ is a random variable from a categorical distribution. ‚Ä¢ is a random variable from a Binomial distribution (which takes values from 0 to D ‚àí 1 ) ‚Ä¢ (a and b are hidden matrices). ‚Ä¢ ‚Ä¢ follows a Dirichlet distribution. ‚Ä¢ follows a Beta distribution. ‚Ä¢ follows a categorical distribution ‚Ä¢ Œª uik are parameters to be learned: BNMF iteratively approximates parameters : (21) (22) (23) (24) (25) (26) (27) where œà is the digamma function as the logarithmic derivative of the gamma function. URP is a generative latent variable model [18] . The model assigns to each user a mixture of user attitudes. Mixing is performed by a Dirichlet random variable: (28) (29) (30) - 4 - International Journal of Interactive Multimedia and Artificial Intelligence (31) (32) (33) In this paper, baseline models will be tested using a) prediction measure, b) recommendation measures, and c) beyond accuracy measures. The chosen prediction measure is the MAE, where the absolute differences of the errors are averaged. Absolute precision and relative recall measures are tested to compare the quality of an unordered list of N recommendations. The ordered lists of recommendations will be compared using the NDCG quality measure. From the beyond accuracy metrics, we have selected novelty and diversity. Novelty returns the distance from the items the user ‚Äòknows‚Äô (has voted or consumed) to his recommended set of items. Diversity tells us about the distance between the set of recommended items. Recommendations with high novelty values are valuable, since they show to the user unknown types of items. Diverse recommendations are valuable because they provide different types of items (and each type of item can be novel, or not, to the user). The GroupLens research group [27] made available several CF datasets, collected over different intervals of time. MovieLens 100K and MovieLens 1M describe 5-star rating and free-text tagging activity. These data were created from 1996 to 2018. In the Movielens 100K dataset, users were selected at random from those who had rated at least 20 movies, whereas the MovieLens 1M dataset has not this constraint. Only movies with at least one rating or tag are included in the dataset. No demographic information is included. Each user is represented by an ‚Äòid‚Äô, and no other information is provided. The dataset files are written as comma-separated values files with a single header row. Columns that contain commas (,) areescapedusing double-quotes (""). These files are encoded as UTF-8. All ratings are contained in the file named ‚Äòratings.csv‚Äô. Each line of this file after the header row represents one rating of one movie by one user, and has the following format: ‚ÄòuserId, movieId, rating, timestamp‚Äô. The lines within this file are ordered first by ‚ÄòuserId‚Äô, then, within user, by ‚ÄòmovieId‚Äô. Timestamps represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970. FilmTrust is a small dataset crawled from the entire FilmTrust website in June, 2011. As the Movielens datasets, it contains ratings voted from users to items; additionally, it provides social information structured as a graph network. Finally, MyAnimeList contains information about anime and ‚Äòotaku‚Äô consumers (anime, manga, video games and computers). Each user is able to add ‚Äòanimes‚Äô to their completed list and give them a rating; this data set is a compilation of those ratings. The MyAnimeList CF information is contained in the file ‚ÄòAnime.csv‚Äô, where their main columns are ‚Äòanime_id‚Äô: myanimelist.net‚Äôs unique ‚Äòid‚Äô identifying an anime; ‚Äòname‚Äô: full name of anime; ‚Äògenre‚Äô: comma separated list of genres for this anime; ‚Äòtype‚Äô: movie, TV, OVA, etc; ‚Äòepisodes‚Äô: how many episodes in this show; ‚Äòrating‚Äô: average rating out of 10 for this anime. These datasets are available in the Kaggle and GitHub repositories, as well as in the KNODIS research group CF4J [32] repository https://github.com/ferortega/cf4j. Table I contains the values of the main parameters of the selected CF data sets: Movielens 100K, Movielens 1M, FilmTrust and MyAnimeList. We have run the explained MF models on each of the four Table I datasets, testing the chosen quality measures. Please note that the MyAnimeList dataset ratings range from 1 to 10 , whereas MovieLens datasets range from 1 to 5 and FilmTrust ranges from 0 to 5 with 0.5 increments. It is also remarkable the sparsity difference between FilmTrust and the rest of the tested datasets. TABLE I. Main Parameter Values of the Tested Datasets Dataset #users #items #ratings Scores Sparsity MovieLens100k 943 1682 99,831 1 to 5 93.71 MovieLens1M 6,040 3,706 911,031 1 to 5 95.94 MyAnimeList 19,179 2,692 548,967 1 to 10 98.94 FilmTrust 1,508 2,071 35,497 0 to 5 87.98 Experiments have been performed using random search and applying four-fold cross-validation. To ensure reproducibility, we used a seed in the random process. Results shown in the paper are the average of the partial results obtained by setting the number k of latent factors to {4, 8, 12} , and the number of MF iterations to {20, 50, 75, 100} . Additionally, to run the PMF, BiasedMF, and BeMF models, both the learning rate and the regularization parameters have been set to {0.001, 0.01, 0.1, 1.0} . The BNMF model requires two specific parameters: Œ± and Œ≤ ; the chosen values por these parameters are: Œ± = {0.2, 0.4, 0.6, 0.8} , and Œ≤ = {5, 15, 25} . The tested number of recommendations N ranges from 1 to 10. We have used 4 stars as recommendation threshold Œ∏ for datasets whose ratings range from 1 to 5 , while the testing threshold has been 8 when MyAnimeList was chosen. The experiments have been implemented using the open framework [33] and the code has been made available at https://github.com/KNODIS-Research-Group/choice-of-mf-models. III.	Results The prediction quality obtained by testing each baseline model is shown in table II. The bold numbers correspond to the best results, and, of them, those highlighted gray are the top ones. As can be seen, BiasedMF and BNMF models provide the best CF prediction results. PMF, NMF, BeMF and URP seem to be more sensitive to the type of CF input data. TABLE II. Prediction Quality Results Using the Mean Absolute Error (MAE). The Lower the Error Value, the Better the Result PMF BiasedMF NMF BeMF BNMF URP MovieLens 100K 0.770 0.754 0.804 0.805 0.748 0.837 MovieLens 1M 0.729 0.712 0.744 0.748 0.693 0.795 FilmTrust 0.863 0.652 0.876 0.712 0.666 0.831 MyAnimeList 1.110 0.926 1.147 1.034 0.943 1.159 Fig. 1 shows the quality of recommendation obtained using the Precision measure. The most remarkable in Fig. 1 is the superiority of the models PMF and BiasedMF. For the remaining models, URP and BeMF provide the worst results, whereas the nonnegative NMF and BNMF return an intermediate quality. It is important to highlight the good performance of the BiasedMF model for both the prediction and the recommendation tasks. To test the quality of CF recommendations of unordered recommendations, precision and recall measures are usually processed, and they are provided separately, or joined in the F1 score. We have done these experiments and we have not found appreciable differences in Recall values for the tested models in the selected datasets. In order to maintain the paper as short as possible, Fig. 2 only shows the Recall results obtained by processing the Movielens 1M dataset. Results from the rest of datasets are very similar; consequently, the Recall quality measure does not help, in this context, to find out the best MF models in the CF area. - 5 - Article in Press Fig. 2. Recall Recommendation quality results obtained in the MovieLens 1M dataset. The results of the other three considered datasets are very similar to this one; to maintain the paper as short as possible, the results of other datasets are not shown. PMF 0,6 0,5 0,4 0,3 0,2 0,1 0 1 3 5 Number of recommendations Recall 7 9 BiasedMF BeMF NMF BNMF URP Fig. 2. Recall Recommendation quality results obtained in the MovieLens 1M dataset. The results of the other three considered datasets are very similar to this one; to maintain the paper as short as possible, the results of other datasets are not shown. In the RSs field, recommendations are usually provided in an ordered list. Users‚Äô trust in RSs quickly decays when the first recommendations in the list do not meet their expectations; for that reason, the NDCG quality measure particularly penalizes errors in the first recommendations of the list. Fig. 3 (NDCG results) shows a similar behavior to Fig. 1, where the BiasedMF and PMF models provide the best recommendation quality. So, these two models perform fine both in recommending ordered and unordered lists. Traditionally, RSs have been evaluated attending to their prediction and recommendation accuracy; nevertheless, there are some other valuable beyond accuracy aims and their corresponding quality measures. The Diversity measure tests the variety of recommendations, penalizing recommendations focused on the same ‚Äòarea‚Äô (Star Wars III, Star Wars I, Star Wars V, Han Solo). Fig. 4 shows the Diversity results obtained by testing the selected models; the most diverse recommendations are usually returned when the BiasedMF model is used, followed by both PMF and NMF. This fact is particularly interesting, since it is not intuitive that the same model (BiasedMF) can, simultaneously, provide accurate and diverse recommendations. Novelty is an important beyond accuracy objective in RSs. Users appreciate accurate recommendations, but they also want to discover unexpected (and accurate enough) recommendations. Please note that a set of recommendations can be diverse and not novel, as they can be novel and not diverse. It would be great to receive, simultaneously, accurate, novel, and diverse recommendations, but usually improving some of the objectives leads to worsening others. Fig. 5 shows the results of the novelty quality measure: NMF returns novel recommendations, compared to other models; NMF provides a balance between accuracy and novelty. BiasedMF and PMF also provide novel recommendations compared to BeMF and URP. PMF 0,85 0,8 0,75 0,7 0,65 0,9 0,95 0,85 0,8 0,75 0,7 1 3 5 Number of recommendations Precision 7 9 1 3 5 7 9 BiasedMF BeMF NMF BNMF URP PMF BiasedMF BeMF NMF BNMF URP PMF 0,85 0,81 0,83 0,79 0,75 0,77 0,70 1 3 5 Number of recommendations Precision 7 9 BiasedMF BeMF NMF BNMF URP PMF 0,85 0,87 0,89 0,81 0,83 0,79 0,75 0,77 0,73 1 3 5 Number of recommendations Number of recommendations Precision Precision 7 9 BiasedMF BeMF NMF BNMF URP (a) (b) (c) (d) Fig. 1. Precision recommendation quality results; a) MovieLens100K , b) MovieLens 1M , c) FilmTrust , d) MyAnimeList . The higher the values, the better the results. - 6 - International Journal of Interactive Multimedia and Artificial Intelligence PMF 0,85 0,8 0,75 0,7 0,65 0,6 0,65 0,67 0,69 0,71 0,73 0,75 0,77 0,79 0,81 0,83 1 3 5 Number of recommendations NDCG NDCG 7 9 1 3 5 7 9 BiasedMF BeMF NMF BNMF URP PMF BiasedMF BeMF NMF BNMF URP PMF 0,95 0,9 0,85 0,75 0,8 0,7 1 3 5 Number of recommendations NDCG 7 9 BiasedMF BeMF NMF BNMF URP PMF 0,79 0,82 0,73 0,76 0,7 0,64 0,67 0,61 1 3 5 Number of recommendations Number of recommendations NDCG 7 9 BiasedMF BeMF NMF BNMF URP (a) (b) (c) (d) Fig. 3. Normalized Discounted Cumulative Gain recommendation quality results; a) MovieLens100K , b) MovieLens 1M , c) FilmTrust , d) MyAnimeList . The higher the values, the better the results. PMF 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 0.001 Diversity Diversity 2 4 6 8 10 BiasedMF BeMF NMF BNMF URP PMF BiasedMF BeMF NMF BNMF URP PMF Diversity BiasedMF BeMF NMF BNMF URP PMF Number of recommendations 2 4 6 8 10 Number of recommendations 2 4 6 8 10 Number of recommendations 2 4 6 8 10 Number of recommendations Diversity BiasedMF BeMF NMF BNMF URP (a) (b) (c) (d) Fig. 4. Diversity beyond accuracy results; a) MovieLens100K , b) MovieLens 1M , c) FilmTrust , d) MyAnimeList . The higher the values, the better the results. - 7 - Article in Press IV.	Discussion In this section, we provide a comparative discussion of the most adequate MF models when applied to a set of different CF databases. To judge each MF model, we simultaneously measure a set of conflicting goals: prediction accuracy, recommendation accuracy (unordered and ordered lists) and beyond accuracy aims. We will promote some MF models as ‚Äòwinners‚Äô, attending to their high performance (overall quality results) when applied to the tested datasets. We also provide a summary table to better identify those MF models that perform particularly fine on any individual quality objective: novelty, diversity, precision, etc., as well as any combination of those quality measures. TABLE III. MF Models Comparative PMF BiasedMF NMF BeMF BNMF URP MAE ++ +++ + + +++ + Precision +++ +++ ++ + ++ + NDCG +++ +++ + + + + Diversity ++ +++ ++ + + + Novelty ++ ++ +++ + + + Total 12 14 9 5 8 5 Table III summarizes the results of this section. BiasedMF is the most appropriate model when novelty of recommendations is not a particularly relevant issue. PMF can be used instead BiasedMF when simplicity is required (e.g. educational environments). BeMF should only be used when reliability information is required or when reliability values are used to improve accuracy [31] . NMF and BNMF are adequate when semantic interpretation of hidden factors is needed. NMF is the best choice when we want to be recommended with novel items. BNMF provides good accuracy and it is designed to recommend to group of users. V.	 Conclusions This paper makes a comparative of relevant MF models applied to collaborative filtering recommender systems. Prediction, recommendation, and beyond accuracy quality measures have been tested on four representative datasets. The results show the superiority of the BiasedMF model, followed by the PMF one. BiasedMF arises as the most convenient model when novelty is not a particularly important feature. PMF combines simplicity with accuracy; it can be the best choice for educational or not commercial implementations. NMF and BNMF are adequate when we want to do a semantic interpretation of their non-negative hidden factors. NMF is preferable to BNMF when beyond accuracy (novelty and diversity) results are required, whereas it is better to make use of BNMF when prediction accuracy is required or when recommending to group of users, or when explaining recommendations is needed. NMF and BiasedMF are the best choices when beyond accuracy aims are selected, whereas PMF or BiasedMF performs particularly well in recommendation task, both for unordered and ordered options. BeMF can only be selected when reliability values are required or when they are used to improve accuracy. Finally, URP does not seem to be an adequate choice in any of the combinations tested. As future work, it is proposed to add new MF models, quality measures, and datasets to the experiments, as well as the possibility of including neural network models such as DeepMF or Neural Collaborative Filtering (NCF). PMF 9,4 9,6 9,8 10 10,2 10,4 10 10 10 10 10 11 11 8,6 8,8 9,2 9 9,4 9,6 9,8 06 06 07 07 07 07 07 Novelty Novelty BiasedMF BeMF NMF BNMF URP PMF BiasedMF BeMF NMF BNMF URP PMF Novelty BiasedMF BeMF NMF BNMF URP PMF Number of recommendations Number of recommendations 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 1 3 5 7 9 Number of recommendations Number of recommendations Novelty BiasedMF BeMF NMF BNMF URP (a) (b) (c) (d) Fig. 5. Novelty beyond accuracy quality results; a) MovieLens100K , b) MovieLens 1M , c) FilmTrust , d) MyAnimeList . The higher the values, the better the results. - 8 - International Journal of Interactive Multimedia and Artificial Intelligence Acknowledgments This work has been co-funded by the Ministerio de Ciencia e Innovaci√≥n of Spain and European Regional Development Fund (FEDER) under grants PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de Madrid under Convenio Plurianual with the Universidad Polit√©cnica de Madrid in the actuation line of Programa de Excelencia para el Profesorado Universitario . References [1] Z. Batmaz, A. Yurekli, A. Bilge, C. Kaleli, ‚ÄúA review on deep learning for recommender systems: challenges and remedies,‚Äù Artificial Intelligence Review , vol. 52, no. 1, pp. 1‚Äì37, 2019. [2] J. Bobadilla, S. Alonso, A. Hernando, ‚ÄúDeep learning architecture for collaborative filtering recommender systems,‚Äù Applied Sciences, vol. 10, no. 7, p. 2441, 2020. [3] B. Zhu, R. Hurtado, J. Bobadilla, F. Ortega, ‚ÄúAn efficient recommender system method based on the numerical relevances and the non-numerical structures of the ratings,‚Äù IEEE Access , vol. 6, pp. 49935‚Äì49954, 2018. [4] J. Bobadilla, R. Lara-Cabrera, √Å. Gonz√°lez-Prieto, F. Ortega, ‚ÄúDeepfair: Deep learning for improving fairness in recommender systems,‚Äù International Journal of Interactive Multimedia and Artificial Intelligence , vol. 6, no. 6, pp. 86‚Äì94, 2021, doi: 10.9781/ijimai.2020.11.001. [5] J. Carb√≥, J. M. Molina, J. D√°vila, ‚ÄúFuzzy referral based cooperation in social networks of agents,‚Äù AI Communications , vol. 18, pp. 1‚Äì13, 2005. 1. [6] D. Medel, C. Gonz√°lez-Gonz√°lez, S. V. Aciar, ‚ÄúSocial relations and methods in recommender systems: A systematic review,‚Äù International Journal of Interactive Multimedia and Artificial Intelligence , vol. 7, no. 4, p. 7, 2022, doi: 10.9781/ijimai.2021.12.004. [7] M. Caro-Mart√≠nez, G. Jim√©nez-D√≠az, J. A. Recio- Garc√≠a, ‚ÄúLocal model- agnostic explanations for black-box recommender systems using interaction graphs and link prediction techniques,‚Äù International Journal of Interactive Multimedia and Artificial Intelligence , vol. InPress, no. InPress, p. 1, 2021, doi: 10.9781/ijimai.2021.12.001. [8] S. Afef, Z. Brahmi, M. Gammoudi, ‚ÄúTrust-based recommender systems: An overview,‚Äù in 27th IBIMA Conference , 05 2016. [9] I. Pinyol, J. Sabater-Mir, ‚ÄúComputational trust and reputation models for open multi-agent systems: a review,‚Äù Artificial Intelligence Review , vol. 40, pp. 1‚Äì25, Jun 2013, doi: 10.1007/s10462-011-9277-z. [10] Y. Deldjoo, M. Schedl, P. Cremonesi, G. Pasi, ‚ÄúRecommender systems leveraging multimedia content,‚Äù ACM Computing Surveys (CSUR) , vol. 53, no. 5, pp. 1‚Äì38, 2020. [11] S. Kulkarni, S. F. Rodd, ‚ÄúContext aware recommendation systems: A review of the state of the art techniques,‚Äù Computer Science Review , vol. 37, p. 100255, 2020. [12] S. Forouzandeh, K. Berahmand, M. Rostami, ‚ÄúPresentation of a recommender system with ensemble learning and graph embedding: a case on movielens,‚Äù Multimedia Tools and Applications , vol. 80, no. 5, pp. 7805‚Äì7832, 2021. [13] R. Salakhutdinov, A. Mnih, ‚ÄúProbabilistic matrix factorization,‚Äù in Proceedings of the 20th International Conference on Neural Information Processing Systems , NIPS‚Äô07, Red Hook, NY, USA, 2007, p. 1257‚Äì1264, Curran Associates Inc. [14] Z. Wu, H. Tian, X. Zhu, S. Wang, ‚ÄúOptimization matrix factorization recommendation algorithm based on rating centrality,‚Äù in International Conference on Data Mining and Big Data , 2018, pp. 114‚Äì125, Springer. [15] C. F√©votte, J. Idier, ‚ÄúAlgorithms for nonnegative matrix factorization with the Œ≤-divergence,‚Äù Neural computation , vol. 23, no. 9, pp. 2421‚Äì2456, 2011. [16] F. Ortega, R. Lara-Cabrera, √Å. Gonz√°lez-Prieto, J. Bobadilla, ‚ÄúProviding reliability in recommender systems through bernoulli matrix factorization,‚Äù Information Sciences , vol. 553, pp. 110‚Äì128, 2021. [17] A. Hernando, J. Bobadilla, F. Ortega, ‚ÄúA non negative matrix factorization for collaborative filtering recommender systems based on a bayesian probabilistic model,‚Äù Knowledge-Based Systems , vol. 97, pp. 188‚Äì202, 2016. [18] B. M. Marlin, ‚ÄúModeling user rating profiles for collaborative filtering,‚Äù Advances in neural information processing systems , vol. 16, 2003. [19] D. M. Blei, A. Y. Ng, M. I. Jordan, ‚ÄúLatent dirichlet allocation,‚Äù Journal of machine Learning research , vol. 3, no. Jan, pp. 993‚Äì1022, 2003. [20] T. Hofmann, ‚ÄúLearning what people (don‚Äôt) want,‚Äù in European Conference on Machine Learning , 2001, pp. 214‚Äì 225, Springer. [21] A. Gunawardana, G. Shani, ‚ÄúEvaluating recommender systems,‚Äù in Recommender systems handbook , Springer, 2015, pp. 265‚Äì308. [22] C. C. Aggarwal, ‚ÄúEvaluating recommender systems,‚Äù in Recommender systems , Springer, 2016, pp. 225‚Äì254. [23] J. Bobadilla, A. Guti√©rrez, S. Alonso, √Å. Gonz√°lez- Prieto, ‚ÄúNeural collaborative filtering classification model to obtain prediction reliabilities,‚Äù International Journal of Interactive Multimedia and Artificial Intelligence , vol. 7, no. 4, pp. 18‚Äì26, 2022, doi: 10.9781/ijimai.2021.08.010. [24] S. Vargas, P. Castells, ‚ÄúRank and relevance in novelty and diversity metrics for recommender systems,‚Äù in Proceedings of the fifth ACM conference on Recommender systems , 2011, pp. 109‚Äì116. [25] P. Castells, S. Vargas, J. Wang, ‚ÄúNovelty and diversity metrics for recommender systems: choice, discovery and relevance,‚Äù in Proceedings of the 33rd European Conference on Information Retrieval (ECIR‚Äô11) , 2011. [26] S. Vargas, P. Castells, D. Vallet, ‚ÄúIntent-oriented diversity in recommender systems,‚Äù in Proceedings of the 34th international ACM SIGIR conference on Research and development in Information Retrieval , 2011, pp. 1211‚Äì 1212. [27] F. M. Harper, J. A. Konstan, ‚ÄúThe movielens datasets: History and context,‚Äù Acm transactions on interactive intelligent systems (tiis) , vol. 5, no. 4, pp. 1‚Äì19, 2015, doi: https://doi.org/10.1145/2827872. [28] J. Golbeck, J. A. Hendler, ‚ÄúFilmtrust: movie recommendations using trust in web-based social networks,‚Äù CCNC 2006. 2006 3rd IEEE Consumer Communications and Networking Conference, 2006. , vol. 1, pp. 282‚Äì286, 2006, doi: 10.1109/CCNC.2006.1593032. [29] J. Miller, G. Southern, ‚ÄúRecommender system for animated video,‚Äù Issues in Information Systems , vol. 15, no. 2, pp. 321‚Äì7, 2014. [30] Y. Koren, R. Bell, C. Volinsky, ‚ÄúMatrix factorization techniques for recommender systems,‚Äù Computer , vol. 42, no. 8, pp. 30‚Äì37, 2009. [31] J. Bobadilla, A. Guti√©rrez, S. Alonso, √Å. Gonz√°lez- Prieto, ‚ÄúNeural collaborative filtering classification model to obtain prediction reliabilities,‚Äù International Journal of Interactive Multimedia and Artificial Intelligence , vol. 7, no. 4, pp. 18‚Äì26, 2022, doi: 10.9781/ijimai.2021.08.010. [32] F. Ortega, B. Zhu, J. Bobadilla, A. Hernando, ‚ÄúCf4j: Collaborative filtering for java,‚Äù Knowledge- Based Systems , vol. 152, pp. 94‚Äì99, 2018, doi: https:// doi.org/10.1016/j.knosys.2018.04.008. [33] F. Ortega, J. Mayor, D. L√≥pez-Fern√°ndez, R. Lara- Cabrera, ‚ÄúCf4j 2.0: Adapting collaborative filtering for java to new challenges of collaborative filtering based recommender systems,‚Äù Knowledge-Based Systems , vol. 215, p. 106629, 2021. Jorge Due√±as-Ler√≠n Jorge Due√±as-Ler√≠n received the B.S. in computer science from the Universidad Polit√©cnica de Madrid. He received the M.S. degree in highschool, vocational training and languages teacher from the Universidad Nacional de Educaci√≥n a Distancia. He is currently a Ph.D. student as part of the KNOledge Discovery and Information Systems - KNODIS research group. Jes√∫s Bobadilla Jes√∫s Bobadilla received the B.S. and the Ph.D. degrees in computer science from the Universidad Polit√©cnica de Madrid and the Universidad Carlos III. Currently, he is a full professor with the Department of Applied Intelligent Systems, Universidad Polit√©cnica de Madrid. He is a habitual author of programming languages books working with McGraw-Hill, Ra-Ma and Alfa Omega publishers. His research interests include information retrieval, recommender systems and speech processing. He oversees the FilmAffinity.com research teamworking on the collaborative filtering kernel of the web site. He has been a researcher into the International Computer Science Institute at Berkeley University and into the Sheffield University. - 9 - Article in Press Fernando Ortega Fernando Ortega was born in Madrid, Spain, in 1988. He received the B.S. degree in software engineering, the M.S. degree in artificial intelligence, and the Ph.D. degree in computer sciences from theUniversidad Polit√©cnica de Madrid, in 2010, 2011, and 2015, respectively. He is currently Associate Professor in the Universidad Polit√©cnica de Madrid. He is author of more than 50 research papers in most prestigious international journals. He leads several national projects to include machine learning algorithms into the society. His research interests include machine learning, data analysis, and artificial intelligence. He is the head researcher of the KNOledge Discovery and Information Systems - KNODIS research group. Abraham Guti√©rrez Abraham Guti√©rrez received the B.S. and the Ph.D. degrees in computer science from the Universidad Polit√©cnica de Madrid. Currently, he is currently an associate professor with the Department of Information Systems, Universidad Polit√©cnica de Madrid. He is the author of search papers in most prestigious international journals. He is a habitual author of programming languages books working with McGraw-Hill, Ra-Ma and Alfa Omega publishers. His research interests include P-Systems, machine learning, data analysis and artificial intelligence. He is in charge of this group innovation issues, including the commercial projects. View publication stats"
Recommender systems survey.pdf,Recommender systems survey | Knowledge-Based Systems,Recommender systems,"Recommender systems survey J. Bobadilla ‚áë , F. Ortega, A. Hernando, A. Guti√©rrez Universidad Polit√©cnica de Madrid, Ctra. De Valencia, Km. 7, 28031 Madrid, Spain a r t i c l e i n f o Article history: Received 7 October 2012 Received in revised form 4 March 2013 Accepted 19 March 2013 Available online 6 April 2013 Keywords: Recommender systems Collaborative Ô¨Åltering Similarity measures Evaluation metrics Prediction Recommendation Hybrid Social Internet of things Cold-start a b s t r a c t Recommender systems have developed in parallel with the web. They were initially based on demo- graphic, content-based and collaborative Ô¨Åltering. Currently, these systems are incorporating social infor- mation. In the future, they will use implicit, local and personal information from the Internet of things. This article provides an overview of recommender systems as well as collaborative Ô¨Åltering methods and algorithms; it also explains their evolution, provides an original classiÔ¨Åcation for these systems, iden- tiÔ¨Åes areas of future implementation and develops certain areas selected for past, present or future importance.  2013 Elsevier B.V. All rights reserved. 1. Introduction Recommender Systems (RSs) collect information on the prefer- ences of its users for a set of items (e.g., movies, songs, books, jokes, gadgets, applications, websites, travel destinations and e-learning material). The information can be acquired explicitly (typically by collecting users‚Äô ratings) or implicitly [134,60,164] (typically by monitoring users‚Äô behavior, such as songs heard, applications downloaded, web sites visited and books read). RS may use demo- graphic features of users (like age, nationality, gender). Social information, like followers, followed, twits, and posts, is commonly used in Web 2.0. There is a growing tend towards the use of infor- mation from Internet of things (e.g., GPS locations, RFID, real-time health signals). RS make use of different sources of information for providing users with predictions and recommendations of items. They try to balance factors like accuracy, novelty, dispersity and stability in the recommendations. Collaborative Filtering (CF) methods play an important role in the recommendation, although they are often used along with other Ô¨Ålterning techniques like content-based, knowledge-based or social ones. CF is based on the way in which humans have made decisions throughout history: besides on our own experiences, we also base our decisions on the experiences and knowledge that reach each of us from a relatively large group of acquaintances. Recently, RS implementation in the Internet has increased, which has facilitated its use in diverse areas [171] . The most com- mon research papers are focused on movie recommendation stud- ies [53,230] ; however, a great volume of literature for RS is centered on different topics, such as music [134,162,216] , televi- sion [238,18] , books [164,88] , documents [206,184,183,185] , e- learning [241,30] , e-commerce [104,54] , applications in markets [67] and web search [154] , among others. The kinds of Ô¨Åltering most used at the beginning of the RS (col- laborative, content-based and demographic) were described in [177] . Breese et al. [43] evaluated the predictive accuracy of differ- ent algorithms for CF; later, the classical paper [94] describes the base for evaluating the Collaborative Filtering RS. The evolution of RS has shown the importance of hybrid tech- niques of RS, which merge different techniques in order to get the advantages of each of them. A survey focused on the hybrid RS has been presented in [47] . However, it does not deal with the role of social-Ô¨Åltering, a technique which has become more popular in the recent years through social networks. The neighborhood-based CF has been the recommendation method most popular at the beginning of the RS; Herlocker et al. [93] provides a set of guidelines for designing neighborhood-based prediction systems. Adomavicius and Tuzhilin [3] present an over- view on the RS Ô¨Åeld standing out the most complex areas on which 0950-7051/$ - see front matter  2013 Elsevier B.V. All rights reserved. http://dx.doi.org/10.1016/j.knosys.2013.03.012 ‚áë Corresponding author. Tel.: +34 913365133; fax: +34 913367527. E-mail address: jesus.bobadilla@upm.es (J. Bobadilla). Knowledge-Based Systems 46 (2013) 109‚Äì132 Contents lists available at SciVerse ScienceDirect Knowledge-Based Systems journal homepage: www.elsevier.com/locate/knosys researchers in RS should focus in the ‚Äò‚Äònext generation of RS‚Äô‚Äô: lim- ited content analysis and overspecialization in content-based methods, cold-start and sparsity in CF methods, model-based tech- niques, nonintrusiveness, Ô¨Çexibility (real-time customization), etc. While researchers have been developing RS, different survey papers have been published summarizing the most important is- sues in this Ô¨Åeld. In view of the impossibility of showing every de- tail of all these techniques in just a paper, this publication selects those issues the authors have felt most suitable to understand the evolution of RS. While the existing surveys focus on the most relevant methods and algorithms of the RS Ô¨Åeld, our survey instead tries to enhance the evolution of the RS: from a Ô¨Årst phase based on the tradi- tional Web to the present second phase based on social Web, which is presently progressing to a third phase (Internet of things). With the purpose of being useful to the new readers of RS Ô¨Åeld, we have included in this survey some traditional topics: RS foundations, k -Nearest Neighbors algorithm, cold-start issues, similarity measures, and evaluation of RS. The rest of the paper deals with novel topics that existing surveys do not consider. Through this survey, advanced readers in RS will study in depth concepts, classiÔ¨Åcations and approaches related to social informa- tion (social Ô¨Åltering: followers, followed, trust, reputation, credi- bility, content-based Ô¨Åltering of social data; social tagging and taxonomies), recommending to groups of users and explaining recommendations. Readers interested in brand new and future applications will Ô¨Ånd this survey useful since it informs about the most recent works in location-aware RS trends and bio-in- spired approaches. They will also discover some important issues, such as privacy, security, P2P information and Internet of things use (RFID data, health parameters, surveillance data, teleopera- tion, telepresence, etc.). According to the idea that RS tend to make use of different sources of information (collaborative, social, demographic, content, knowledge-based, geographic, sensors, tags, implicit and explicit data acquisition, etc.), this survey emphasizes hybrid architectures, based on making recommendations through different known tech- nologies (each one designed on behalf of a speciÔ¨Åc source of information). Much of the quality of a survey can be measured by an appro- priate choice of its references. This survey contains 249 references systematically obtained, which have been selected taking into ac- count factors like the number of recent citations and the impor- tance of the journal in which the paper has been published. The remainder of this article is structured as follows: In Sec- tion 2 , we explain concisely the methodology used to select the most signiÔ¨Åcative papers on the RS Ô¨Åeld. Section 3 describes the RS foundations: methods, algorithms and models used for provid- ing recommendations based from the information of the tradi- tional web: ratings, demographic data and item data (CF, demographic Ô¨Åltering, content-based Ô¨Åltering and hybrid Ô¨Åltering). Section 4 describes measures for evaluating the quality of the RS predictions and recommendations. Section 5 shows the use of so- cial information from Web 2.0 for making recomendations through concepts like trust, reputation and credibility. We will also de- scribe techniques based on content-based for social information (e.g. tags and posts). Section 6 focusses on two important areas (although not very well studied yet): recommendation to group of users and explanation of recommendations. Section 7 focusses on recommender system trends, covering bio-inspired approaches and Web 3.0 information Ô¨Åltering such as location-aware RS. Sec- tion 8 explains related works and the original contributions of this survey. The concluding section summarizes the RS history and focuses on the type of data used as well as the development of algorithms and evaluation measures. The conclusions section also indicates seven new areas that we consider likely to be the focus of RS re- search in the scientiÔ¨Åc community in the near future. 2. Methodology An initial study was performed to determine the most represen- tative topics and terms in the RS Ô¨Åeld. First, 300 RS papers were se- lected from journals, with a higher priority for current and for often-cited articles. Next, we extracted from these 300 papers the most signiÔ¨Åcant terms. We gave the most emphasis to keywords, less emphasis to titles and, Ô¨Ånally, the least emphasis to abstracts. We have overlooked common words, like articles, prepositions and general-use words from the remaining pool, we selected 300 terms represented in the RS Ô¨Åeld. From a matrix of arti- cles  words, wherein we stored the importance of each word from each article, we generated a tree of relationships between the words. Fig. 1 depicts the most signiÔ¨Åcant section of the graph (due to space constraints, the entire tree is not shown, but it is pro- vided as additional material in Fig. 1 AdditionalData.png ). The short distances between words indicate the highest similarities; warm colors indicate a greater reliability for the relationships. The size of the nodes indicates the importance of the words as a function of the parameters N k , N t , N a (number of signiÔ¨Åcative words in the keywords, title and abstract) and N k w ; N t w ; N a w (number of times that the word w appears in the keywords, title and abstract). The equa- tion used to determine the importance of each word w is as follows: f w ¬º 1 3 N k w N k √æ N t w N t log N a N t √æ N a w N a N a N t ! Example : we will consider a paper where N k = 5 keywords, N t = 11 words in the title, and N a = 52 words of abstract length. We will get the values of f factorization and f matrix , where the word ‚Äòfactorization‚Äô appears once as a keyword, once in the title and three times in the abstract; the word ‚Äòmatrix‚Äô does not appear as a keyword, but it is contained once in the title and twice in the abstract. The importance of these words will be: f factorization ¬º 1 3 1 5 √æ 1 11 log 52 11 √æ 3 52 52 11 ! ¬º 0 : 09 f matrix ¬º 1 3 0 5 √æ 1 11 log 52 11 √æ 2 52 52 11 ! ¬º 0 : 02 The information depicted in Fig. 1 is used to identify the most relevant aspects of RS. They are represented by the most signiÔ¨Åcant words in the graph and the related terms. The articles referenced herein were chosen based on the following criteria: (a) the tran- scendence of the subject according to the importance of the words in Fig. 1 ; (b) its historical contribution (a signiÔ¨Åcant fraction of the classic reference articles are included); (c) the number of times the article is cited; (d) articles published in journals with an impact factor were preferred over conferences and workshops; and (e) re- cent articles were preferred over articles published many years ago. Fig. 2 shows a temporal distribution for the referenced papers. We use the clusters of words in Fig. 1 to structure the explica- tions of the survey. For each concept explained: (1) we have ob- tained their keywords and all the words related to them according to Fig. 1 ; (2) we have identiÔ¨Åed, among the set of 300 pa- pers, those which are more related to the set of words associated to the concept; (3) we have selected the subset of papers which deal with the concept, giving priority to those with high values in crite- ria like importance of the paper and the number of cites; and (4) we have tried to balance the number of times a paper is referenced in our survey, aiming to reference most of the 300 papers selected. 110 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 3. Recommender systems foundations This section presents the most relevant concepts on which the traditional RS are based. Here, we provide general descriptions on the classical taxonomies, algorithms, methods, Ô¨Åltering ap- proaches, databases, etc. Besides, we show a graphic depicting the traditional models of recommendations and their relations. Next, we will describe the cold-start problem, which will illustrate the difÔ¨Åculty of making collaborative recommendation when the RS contains a small amount of data. Next, we will describe the k NN algorithm; the most used algorithm for implementing RS based on CF. Finally, we will describe different proposed similarity measures for comparing users or items. We will show graphics for measuring the quality of these similarity measures. 3.1. Fundamentals The process for generating an RS recommendation is based on a combination of the following considerations:  The type of data available in its database (e.g., ratings, user reg- istration information, features and content for items that can be ranked, social relationships among users and location-aware information).  The Ô¨Åltering algorithm used (e.g., demographic, content-based, collaborative, social-based, context-aware and hybrid).  The model chosen (e.g., based on direct use of data: ‚Äò‚Äòmemory- based,‚Äô‚Äô or a model generated using such data: ‚Äò‚Äòmodel-based‚Äô‚Äô).  The employed techniques are also considered: probabilistic approaches, Bayesian networks, nearest neighbors algorithm; bio-inspired algorithms such as neural networks and genetic algorithms; fuzzy models, singular value decomposition tech- niques to reduce sparsity levels, etc.  Sparsity level of the database and the desired scalability.  Performance of the system (time and memory consuming).  The objective sought is considered (e.g., predictions and top N recommendations) as well as  The desired quality of the results (e.g., novelty, coverage and precision). Research in RS requires using a representative set of public dat- abases to facilitate investigations on the techniques, methods and algorithms developed by researchers in the Ô¨Åeld. Through these databases, the scientiÔ¨Åc community can replicate experiments to validate and improve their techniques. Table 1 lists the current public databases referenced most often in the literature. Last.Fm and Delicious incorporate implicit ratings and social information; their data were generated from the versions released in the HetRec, 2011 data sets, hosted by the GroupLens research Group. The internal functions for RS are characterized by the Ô¨Åltering algorithm . The most widely used classiÔ¨Åcation divides the Ô¨Åltering algorithms into [3,51,203] : (a) collaborative Ô¨Åltering, (b) demo- graphic Ô¨Åltering, (c) content-based Ô¨Åltering and (d) hybrid Ô¨Åltering. Fig. 1. Words represented in the recommender systems research Ô¨Åeld. Short distances indicate higher similarities, and a warm color indicates greater reliability. The size of the nodes is proportional to the importance of the words. Fig. 2. Temporal distribution for the referenced papers. J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 111 Content-based Ô¨Åltering [131,11,158] makes recommendations based on user choices made in the past (e.g. in a web-based e-com- merce RS, if the user purchased some Ô¨Åction Ô¨Ålms in the past, the RS will probably recommend a recent Ô¨Åction Ô¨Ålm that he has not yet purchased on this website). Content-based Ô¨Åltering also gener- ates recommendations using the content from objects intended for recommendation; therefore, certain content can be analyzed, like text, images and sound. From this analysis, a similarity can be established between objects as the basis for recommending items similar to items that a user has bought, visited, heard, viewed and ranked positively. Demographic Ô¨Åltering [177,126,185] is justiÔ¨Åed on the principle that individuals with certain common personal attributes (sex, age, country, etc.) will also have common preferences. Collaborative Filtering [3,94,92,51,212] allows users to give rat- ings about a set of elements (e.g. videos, songs, Ô¨Ålms, etc. in a CF based website) in such a way that when enough information is stored on the system, we can make recommendations to each user based on information provided by those users we consider to have the most in common with them. CF is an interesting open research Ô¨Åeld [232,34,32] . As noted earlier, user ratings can also be Table 1 Most often used memory-based recommender systems public databases. Without social information With social information (hosted by the GroupLens) MovieLens 1M MovieLens 10M NetÔ¨Çix Jester EachMovie Book-crossing ML Last.Fm Delicious Ratings 1 million 10 million 100 million 4.1 million 2.8 million 1.1 million 855,598 92,834 104,833 Users 6040 71,567 480,189 73,421 72,916 278,858 2113 1892 1867 Items 3592 10,681 17,770 100 1628 271,379 10,153 17,632 69,226 Range {1, . . . ,5} {1, . . . ,5} {1, . . . ,5}  10, 10 [0,1] {1, . . . ,10} {1, . . . ,5} Implicit Implicit Tags N/A N/A N/A N/A N/A N/A 13222 11946 53388 Tags assignment N/A N/A N/A N/A N/A N/A 47957 186479 437593 Friends relations N/A N/A N/A N/A N/A N/A N/A 25434 15328 Items Movies Movies Movies Jokes Movies Books Movies Music URL‚Äôs Fig. 3. Traditional models of recommendations and their relationships. 112 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 implicitly acquired (e.g., number of times a song is heard, informa- tion consulted and access to a resource). The most widely used algorithm for collaborative Ô¨Åltering is the k Nearest Neighbors (kNN) [3,203,32] . In the user to user version, k NN executes the following three tasks to generate recommenda- tions for an active user: (1) determine k users neighbors (neighbor- hood) for the active user a ; (2) implement an aggregation approach with the ratings for the neighborhood in items not rated by a ; and (3) extract the predictions from in step 2 then select the top N recommendations . Hybrid Ô¨Åltering [47,185] . Commonly uses a combination of CF with demographic Ô¨Åltering [224] or CF with content-based Ô¨Åltering [18,60] to exploit merits of each one of these techniques. Hybrid Ô¨Åltering is usually based on bioinspired or probabilistic methods such as genetic algorithms [76,99] , fuzzy genetic [7] , neural net- works [133,62,192] , Bayesian networks [50] , clustering [209] and latent features [199] . A widely accepted taxonomy divides recommendation methods into memory-based and model-based method categories: Memory-based methods [3,51,123,214] . Memory-based methods can be deÔ¨Åned as methods that (a) act only on the matrix of user ratings for items and (b) use any rating generated before the refer- ral process (i.e., its results are always updated). Memory-based methods usually use similarity metrics to obtain the distance be- tween two users, or two items, based on each of their ratios. Model-based methods [3,212] . Use RS information to create a model that generates the recommendations. Herein, we consider a method model-based if new information from any user outdates the model. Among the most widely used models we have Bayesian classiÔ¨Åers [59] , neural networks [107] , fuzzy systems [234] , genetic algorithms [76,99] , latent features [251] and matrix factorization [142] , among others. To reduce the problems from high levels of sparsity in RS dat- abases, certain studies have used dimensionality reduction tech- niques [202] . The reduction methods are based on Matrix Factorization [124,142,143] . Matrix factorization is especially ade- quate for processing large RS databases and providing scalable ap- proaches [215] . The model-based technique Latent Semantic Index (LSI) and the reduction method Singular Value Decomposition (SVD) are typically combined [224,244,48] . SVD methods provide good prediction results but are computationally very expensive; they can only be deployed in static off-line settings where the known preference information does not change with time. RS can use clustering techniques to improve the prediction qual- ity and reduce the cold-start problem when applied to hybrid Ô¨Ål- tering. It is typical to form clusters of items in hybrid RS [209,237] . A different common approach uses clustering both for items and users ( bi-clustering ) [252,85] . RS comprising social infor- mation have been clustered to improve the following areas: tagging [208] , explicit social links [179] and explicit trust information [181,70] . The graph in Fig. 3 shows the most signiÔ¨Åcant traditional meth- ods, techniques and algorithms for the recommendation process as well as their relationships and groupings. Different sections of this paper provide more detail on the most important aspects involved in the recommendation process. As may be seen in Fig. 3 , we can use some of the traditional Ô¨Ål- tering methods (content-based, demographic and collaborative) applied to databases. Model-based technologies (genetic algo- rithms, neural networks, etc.) make use of this kind of information. Typical memory-based approaches are: item to item; user to user; and hybrids of the two previous. The main purpose of both mem- ory-based and model-based approaches is to get the most accurate predictions in the tastes of users. The accuracy of these predictions may be evaluated through the classical information retrieval mea- sures, like MAE, precision, and recall. Researchers make use of these measures in order to improve the RS methods and technologies. 3.2. Cold-start The cold-start problem [203,3] occurs when it is not possible to make reliable recommendations due to an initial lack of ratings. We can distinguish three kinds of cold-start problems: new com- munity , new item and new user . The last kind is the most important in RS that are already in operation. The new community problem [204,129] refers to the difÔ¨Åculty, when starting up a RS, in obtaining, a sufÔ¨Åcient amount of data (ratings) for making reliable recommendations. Two common ways are used for tackling this problem: to encourage users to make ratings through different means; to take CF-based recom- mendations when there are enough users and ratings. The new item problem [174,172] arises because the new items entered in RS do not usually have initial ratings, and therefore, they are not likely to be recommended. In turn, an item that is not rec- ommended goes unnoticed by a large part of the community of users, and as they are unaware of it they do not rate it; this way, we can enter a vicious circle in which a set of items of the RS are left out of the ratings/recommendations process. The new item problem has less of an impact on RS in which the items can be dis- covered via other means (e.g. movies) than in RS where this is not the case (i.e. e-commerce, blogs, photos, videos, etc.). A common solution to this problem is to have a set of motivated users who are responsible for rating each new item in the system. The new user problem [190,197] represents one of the great dif- Ô¨Åculties faced by the RS in operation. Since new users in the RS have not yet provided any rating in the RS, they cannot receive any personalized recommendations based on memory-based CF; when the users enter their Ô¨Årsts ratings they expect the RS to offer them personalized recommendations, but the number of ratings introduced in the RS is usually not yet sufÔ¨Åcient to be able to make reliable CF-based recommendations, and, therefore, new users may feel that the RS does not offer the service they expected and they may stop using it. The common strategy to tackle the new user problem consists of turning to additional information to the set of ratings in order to be able to make recommendations based on the data available for each user. The cold-start problem is often faced using hybrid approaches (usually CF-content based RS, CF-demographic based RS, CF-social based RS) [118,140] . Leung et al. [135] propose a no- vel content-based hybrid approach that makes use of cross-level association rules to integrate content information about domains items. Kim et al. [118] use collaborative tagging employed as an approach in order to grasp and Ô¨Ålter users‚Äô preferences for items and they explore the advantages of the collaborative tagging for data sparseness and cold-start users (they collected the dataset by crawling the collaborative tagging delicious site). Weng et al. [228] combine the implicit relations between users‚Äô items prefer- ences and the additional taxonomic preferences to make better quality recommendations as well as alleviate the cold-start prob- lem. Loh et al. [140] represent user‚Äôs proÔ¨Åles with information ex- tracted from their scientiÔ¨Åc publications. Martinez et al. [148] present a hybrid RS which combines a CF algorithm with a knowl- edge-based one. Chen and He [56] propose a number of common terms/ term frequency (NCT/TF) CF algorithm based on demo- graphic vector. Saranya and Atsuhiro [199] propose a hybrid RS that utilizes latent features extracted from items represented by a multi-attributed record using a probabilistic model. Park et al. [173] propose a new approach: they use Ô¨Ålterbots, and surrogate users that rate items based only on user or item attributes. J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 113 3.3. The k nearest neighbors recommendation algorithm The k Nearest Neighbors ( k NN) recommendation algorithm is the reference algorithm for the collaborative Ô¨Åltering recommendation process. Its primary virtues are simplicity and reasonably accurate results; its major pitfalls are low scalability and vulnerability to sparsity in the RS databases. This section provides a general expla- nation of this algorithm function. CF based on the k NN algorithm is conceptually simple, with a straightforward implementation; it also generally produces good- quality predictions and recommendations. However, due to the high level of sparsity [142,29] in RS databases, similarity measures often encounter processing problems (typically from insufÔ¨Åcient mutual ratings for a comparison of users and items) and cold start situations (users and items with low number of rankings) [204,98,36,135] . Another major problem for the k NN algorithm is its low scalabil- ity [142] . As the databases (such as NetÔ¨Çix) increase in size (hun- dreds of thousands of users, tens of thousands of items, and hundreds of millions of rankings), the process for generating a neighborhood for an active user becomes too slow; The similarity measure must be processed as often as new users are registered in the database. The item to item version of the k NN algorithm sig- niÔ¨Åcantly reduces the scalability problem [200] . To this end, neigh- bors are calculated for each item; their top n similarity values are stored, and for a period of time, predictions and recommendations are generated using the stored information. Although the stored information does not include the ratings from previous process- ing/storage, outdated information for items is less sensitive than for the users. A recurrent theme in CF research is generating metrics to calcu- late with accuracy and precision the existing similarity for the users (or items). Traditionally, a series of statistical metrics have been used [3,51] , such as the Pearson correlation , cosine , constraint Pearson correlation and mean squared differences . Recently, metrics have been designed to Ô¨Åt the constraints and peculiarities of RS [31,35] . The relevance ( signiÔ¨Åcance ) concept was introduced to af- ford more importance to more relevant users and items [34,227] . Additionally, a group of metrics was speciÔ¨Åcally designed to ade- quately function in cold-start situations [6,36] . The k NN algorithm is based on similarity measures. Next sub- section provides further details on the current RS similarity mea- sures. The similarity approaches typically compute the similarity between two users x and y (user to user) based on both users‚Äô item ratings. The item to item k NN version computes the similarity be- tween two items i and j . A formal approach of the k NN algorithm may be found in [32] . In this section, we will provide an illustrative example of this algo- rithm. The method for making recommendations is based on the following three steps: (a) Using the selected similarity measure, we produce the set of k neighbors for the active user a . The k neighbors for a are the nearest k (similar) users to u . (b) Once the set of k users (neighbors) similar to active a has been calculated, in order to obtain the prediction of item i on user a , one of the following aggregation approaches is often used: the average, the weighted sum and the adjusted weighted aggregation (deviation-from-mean). (c) To obtain the top- n recommendations, we choose the n items, which provide most satisfaction to the active user according to our predictions. Fig. 4 shows a case study using the user to user k NN algorithm mechanism. In the item to item version [200,77] of the k NN algorithm, the following three tasks are executed: (1) determine q items neigh- bors for each item in the database; (2) for each item i not ranked by the active user a , calculate its prediction based on the ratings of a from the q neighbors of i ; and (3) select the top n recommen- dations for the active user (typically the n major predictions from a ). Step (1) can be executed periodically, which facilitates an accel- erated recommendation with regard to the user to user version. The item to item and user to user versions of the k NN algorithm can be combined [188] to take advantage of the positive aspects from each approach. These approaches are typically fused by pro- cessing the similarity between objects. 3.4. Similarity measures A metric or a Similarity Measure (SM) determines the similarity between pairs of users (user to user CF) or the similarity between pairs of items (item to item CF). For this purpose, we compare the ratings of all the items rated by two users (user to user) or the rat- ings of all users who have rated two items (item to item). The k NN algorithm is based essentially on the use of traditional similarity metrics of statistical origin. These metrics require, as the only source of information, the set of votes made by the users on the items (memory-based CF). Among the most commonly used traditional metrics we have: Pearson correlation (CORR), cosine (COS), adjusted cosine (ACOS), constrained correlation (CCORR), Mean Squared Differences (MSD) and Euclidean (EUC) [51,3] . We will describe and compare a representative group of SM used in the k NN algorithm. The SM discussed include the following variations: (a) cold-start and general cases, (b) based or not based on models, and (c) using trust information or only ratings. Table 2 shows a classiÔ¨Åcation of the memory-based CF SM which will be tested in this section. A new metric (JMSD) has recently been published, which be- sides using the numerical information from the ratings (via mean squared differences) also uses the non-numerical information pro- vided by the arrangement of these (via Jaccard) [31] . Ortega et al. [169] use Pareto dominance to perform a pre-Ô¨Åltering process eliminating less representative users from the k -neighbur selection process while retaining the most promising ones. A specialization of the memory-based CF SM, which appeared recently [35] , uses the information contained in the votes of all users, instead of restricting it to the ratings of the two users com- pared (user to user) or the two items compared (item to item). We will call this SM SING (singularities). The possibility exists to create a model (model-based CF) from the full set of users‚Äô ratings in order to later determine the similar- ity between pairs of users or pairs of items based on the model cre- ated. The potential advantages of this focus are an increase in the accuracy obtained, in the performance (time consuming) achieved or in both. The drawback is that the model must be regularly up- dated in order to consider the most recently entered set of ratings. Fig. 4. User to user k NN algorithm example, k = 3. Similarity measure: 1 ‚Äì (mean squared differences). Aggregation approach: average. 114 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 Bobadilla et al. [33] provides a metric based on a model generated using genetic algorithms. We will call this SM GEN (genetic-based). As a result of the increase in web 2.0 websites on the Internet, a set of metrics has appeared which use the new social information available (friends, followers, followeds, etc.). Most of these SM are grouped in papers related to trust, reputation and credibility [71,239,138] , although this situation is also produced in other Ô¨Åelds [30] . These metrics could not be considered strictly mem- ory-based CF, as they use additional information which not all RS have. In this sense, each SM proposed is tailored to a speciÔ¨Åc RS or at most to a very small set of RS which share the same structure in their social information. There are SM [112,127] which aim to extract information re- lated to trust and reputation by only using the users‚Äô set of ratings (memory-based CF). The advantage is that their use can be general- ized to all CF RS; the drawback is that the social information ex- tracted is really poor. We will call TRUST the SM proponed in Jeong et al. [112] . Currently, two new interesting SM get more cov- erage [38] and accuracy [61] . Fig. 5 shows the results from several evaluation measures gen- erated by applying the SM discussed in this section. The results show that the RS-tailored SM are superior compared with the tra- ditional SM from statistics. Processing for the memory-based infor- mation and results from Fig. 5 follow the framework schematic published previously [32] . There are so far research papers dealing with the cold-start prob- lem through the users‚Äô ratings information. Ahn [6] presents a heu- ristic SM named PIP, that outperforms the traditional statistical SM Table 2 Tested collaborative Ô¨Åltering similarity measures. Not based on models Model-based No trust extraction Trust extraction Traditional (only the ratings of both users or both items) Not tailored to cold-start users JMSD, CORR, CCORR, COS, ACOS, MSD, EUC GEN Tailored to cold-start users PIP UERROR NCS Extended to all the ratings SING TRUST Fig. 5. Evaluation measures results obtained from current similarities measures; MovieLens database. (A) Prediction results, (B) recommendation results, (C) novelty results, and (D) trust results. J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 115 (Pearson correlation, cosine, etc.). Heung-Nam et al. [98] proposes a method (UERROR) that predicts Ô¨Årst actual ratings and subsequently identiÔ¨Åes prediction errors for each user. Taking into account this er- ror information, some speciÔ¨Åc ‚Äò‚Äòerror-reÔ¨Çected‚Äô‚Äô models, are de- signed. Bobadilla et al. [36] presents a metric based on neural learning (model-based CF) and adapted for new user cold-start situ- ations, called NCS. Fig. 6 shows results from several evaluation measures gener- ated by applying the cold-start SM presented in this section; These results show that the RS-tailored SM are superior compared with the traditional SM from statistics. Since the database Movielens does not take into account cold-start users, we have removed rat- ings of this database in order to achieve cold-start users. Indeed, we have removed randomly between 5 and 20 ratings of those users who have rated between 20 and 30 items. In this way, we will regard those users who now result to rate between 2 and 20 items as cold-start users. 4. Evaluation of recommender systems results Since RS research began, evaluation of predictions and recom- mendations has become important [94,201] . Research in the RS Ô¨Åeld requires quality measures and evaluation metrics [90] to know the quality of the techniques, methods, and algorithms for predic- tions and recommendations. Evaluation metrics [94,95] and evalua- tion frameworks [92,32] facilitate comparisons of several solutions for the same problem and selection from different promising lines of research that generate better results. Because of evaluation measures, RS recommendations have gradually been tested and improved [48] . A representative set of existing evaluation measures has standard formulations, and a group of open RS public databases has been generated. These two advances have facilitated quality comparisons for new proposed recommendation methods and previously published methods; thus, RS methods and algorithms research has progressed continuously. The most commonly used quality measures are the following [90,95] : (1) prediction evaluations, (2) evaluations for recommen- dation as sets, and (3) evaluations for recommendations as ranked lists. Fig. 5 shows results from applying several evaluation mea- sures to a set of representative similarity measures. Evaluation metrics [12] can be classiÔ¨Åed as [94,95] (a) predic- tion metrics: such as the accuracy ones: Mean Absolute Error ( MAE ), Root of Mean Square Error ( RMSE ), Normalized Mean Average Error ( NMAE ); and the coverage (b) set recommendation metrics: such as Precision , Recall and Receiver Operating Characteristic ( ROC ) [204] (c) rank recommendation metrics: such as the half-life [43] and the discounted cumulative gain [17] and (d) diversity met- rics: such as the diversity and the novelty of the recommended items [105] . The validation process is performed by employing the most common cross validation techniques ( random sub-sam- pling and k-fold cross validation ) [21] ; for cold-start situations, due to the limited number of users (or items) votes involved, the usual method chosen to carry out the experiments is leave-one- out cross validation [36] . Hern√°ndez and Gaudioso [95] propose an evaluation process based on the distinction between interactive and non-interactive Fig. 6. Evaluation results obtained from current cold-start similarities measures. (A) Prediction results, (B) recommendation results, (C) novelty results, and (D) trust results. 116 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 subsystems. General publications and reviews also exist which in- clude the most commonly accepted evaluation measures: mean absolute error , coverage , precision , recall and derivatives of these: mean squared error , normalized mean absolute error , ROC and fallout ; Goldberg et al. [87] focuses on the aspects not related to the eval- uation, Breese et al. [43] compare the predictive accuracy of vari- ous methods in a set of representative problem domains. The majority of articles discuss attempted improvements to the accuracy of RS results ( RMSE , MAE , etc.). It is also common to at- tempt an improvement in recommendations (precision, recall, ROC, etc.). However, additional objectives should be considered for generating greater user satisfaction [253] , such as topic diversi- Ô¨Åcation and coverage serendipity . Currently, the Ô¨Åeld has a growing interest in generating algo- rithms with diverse and innovative recommendations, even at the expense of accuracy and precision. To evaluate these aspects, various metrics have been proposed to measure recommendation novelty and diversity [105,220] . The frameworks aid in deÔ¨Åning and standardizing the methods and algorithms employed by RS as well as the mechanisms to eval- uate the quality of the results. Among the most signiÔ¨Åcant papers that propose CF frameworks are Herlocker et al. [92] which evaluates the following: similarity weight, signiÔ¨Åcance weighting, variance weighting, selecting neighborhood and rating normaliza- tion; Hern√°ndez and Gaudioso [95] proposes a framework in which any RS is formed by two different subsystems, one of them to guide the user and the other to provide useful/interesting items. Koutrika et al. [125] is a framework which introduces levels of abstraction in CF process, making the modiÔ¨Åcations in the RS more Ô¨Çexible. Antunes et al. [12] presents an evaluation framework assuming that evaluation is an evolving process during the system lifecicle. The majority of RS evaluation frameworks proposed until now present two deÔ¨Åciencies: the Ô¨Årst of these is the lack of formal- ization. Although the evaluation metrics are well deÔ¨Åned, there are a variety of details in the implementation of the methods which, in the event they are not speciÔ¨Åed, can lead to the generation of different results in similar experiments. The second deÔ¨Åciency is the absence of standardization of the evalu- ation measures in aspects such as novelty and trust of the recommendations. Bobadilla et al. [32] provides a complete series of mathematical formalizations based on sets theory. Authors provide a set of eval- uation measures, which include the quality analysis of the follow- ing aspects: predictions, recommendations, novelty and trust. Presented next is a representative selection of the RS evaluation quality measures most often used in the bibliography. 4.1. Quality of the predictions: mean absolute error, accuracy and coverage In order to measure the accuracy of the results of an RS, it is usual to use the calculation of some of the most common predic- tion error metrics, amongst which the Mean Absolute Error (MAE) and its related metrics: mean squared error, root mean squared error, and normalized mean absolute error stand out. We deÔ¨Åne U as the set of the RS users, I as the set of the RS items, r u , i the rating of user u on item i ,  the lack of rating ( r u , i =  means user u has not rated item i ), p u , i the prediction of item i on user u . Let O u = { i 2 I j p u , i ‚Äì  ^ r u , i ‚Äì  }, set of items rated by user u hav- ing prediction values. We deÔ¨Åne the MAE and RMSE of the system as the average of the user‚Äôs MAE. We remark that the absolute dif- ference between prediction and real value, j p u , i  r u , i j , informs about the error in the prediction. MAE ¬º 1 # U X u 2 U 1 # O u X i 2 O u j p u ; i  r u ; i j ! √∞ 1 √û RMSE ¬º 1 # U X u 2 U Ô¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É 1 # O u X i 2 O u √∞ p u ; i  r u ; i √û 2 s √∞ 2 √û The coverage could be deÔ¨Åned as the capacity of predicting from a metric applied to a speciÔ¨Åc RS. In short, it calculates the percent- age of situations in which at least one k -neighbor of each active user can rate an item that has not been rated yet by that active user. We deÔ¨Åned K u , i as the set of neighbors of u which have rated the item i . We deÔ¨Åne the coverage of the system as the average of the user‚Äôs coverage: Let C u ¬º f i 2 I j r u ; i ¬º  ^ K u ; i ‚Äì ¬£ g ; D u ¬º f i 2 I j r u ; i ¬º g co v erage ¬º 1 # U X u 2 U 100  # C u # D u   √∞ 3 √û 4.2. Quality of the set of recommendations: precision, recall and F1 The conÔ¨Ådence of users for a certain RS does not depend directly on the accuracy for the set of possible predictions. A user gains conÔ¨Ådence on the RS when this user agrees with a reduced set of recommendations made by the RS. In this section, we deÔ¨Åne the following three most widely used recommendation quality measures: (1) precision, which indicates the proportion of relevant recommended items from the total number of recommended items, (2) recall, which indicates the pro- portion of relevant recommended items from the number of rele- vant items, and (3) F1, which is a combination of precision and recall. Let X u as the set of recommendations to user u , and Z u as the set of n recommendations to user u . We will represent the evaluation precision, recall and F 1 measures for recommendations obtained by making n test recommendations to the user u , taking a h rele- vancy threshold. Assuming that all users accept n test recommendations: precision ¬º 1 # U X u 2 U # f i 2 Z u j r u ; i P h g n √∞ 4 √û recall ¬º 1 # U X u 2 U # f i 2 Z u j r u ; i P h g # f i 2 Z u j r u ; i P h g √æ # i 2 Z c u  r u ; i P h   √∞ 5 √û F 1 ¬º 2  precision  recall precision √æ recall √∞ 6 √û 4.3. Quality of the list of recommendations: rank measures When the number n of recommended items is not small, users give greater importance to the Ô¨Årst items on the list of recommen- dations. The mistakes incurred in these items are more serious er- rors than those in the last items on the list. The ranking measures consider this situation. Among the ranking measures most often used are the following standard information retrieval measures: (a) half-life (7) [43] , which assumes an exponential decrease in the interest of users as they move away from the recommenda- tions at the top and (b) discounted cumulative gain (8) [17] , wherein decay is logarithmic. HL ¬º 1 # U X u 2 U X N i ¬º 1 max √∞ r u ; p i  d ; 0 √û 2 √∞ i  1 √û = √∞ a  1 √û √∞ 7 √û DCG k ¬º 1 # U X u 2 U r u ; p 1 √æ X k i ¬º 2 r u ; p i log 2 √∞ i √û ! √∞ 8 √û J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 117 p 1 , . . . , p n represents the recommendation list, r u , pi represents the true rating of the user u for the item p i , k is the rank of the eval- uated item, d is the default rating, a is the number of the item on the list such that there is a 50% chance the user will review that item. 4.4. Novelty and diversity The novelty evaluation measure indicates the degree of differ- ence between the items recommended to and known by the user. The diversity quality measure indicates the degree of differentia- tion among recommended items. Currently, novelty and diversity measures do not have a stan- dard; therefore, different authors propose different metrics [163,220] . Certain authors have [105] used the following: di v ersity Z u ¬º 1 # Z u √∞ # Z u  1 √û X i 2 Z u X j 2 Z u ; j ‚Äì i ¬Ω 1  sim √∞ i ; j √û √∞ 9 √û no v elty i ¬º 1 # Z u  1 X j 2 Z u ¬Ω 1  sim √∞ i ; j √û ; i 2 Z u √∞ 10 √û Here, sim ( i , j ) indicates item to item memory-based CF similar- ity measures. Z u indicates the set of n recommendations to user u . 4.5. Stability The stability in the predictions and recommendations inÔ¨Çu- ences on the users‚Äô trust towards the RS. A RS is stable if the pre- dicitions it provides do not change strongly over a short period of time. Adomavicius and Zhang [4] propose a quality measure of stability, MAS (Mean Absolute Shift). This measure is deÔ¨Åned through a set of known ratings R 1 and a set of predictions of all un- known ratings, P 1 . For an interval of time, users of the RS will have rated a subset S of these unknown ratings and the RS can now make new predictions, P 2 . MAS is deÔ¨Åned as follows: stability ¬º MAS ¬º 1 j P 2 j X √∞ u ; i √û2 P 2 j P 2 √∞ u ; i √û  P 1 √∞ u ; i √ûj √∞ 11 √û 4.6. Reliability The reliability of a prediction or a recommendation informs about how seriously we may consider this prediction. When RS recommends an item to a user with prediction 4.5 in a scale {1, . . . ,5}, this user hopes to be satisÔ¨Åed by this item. However, this value of prediction (4.5 over 5) does not reÔ¨Çect with which certain degree the RS has concluded that the user will like this item (with value 4.5 over 5). Indeed, this prediction of 4.5 is much more reli- able if it has obtained by means of 200 similar users than if it has obtained by only two similar users. In Hernando et al. [96] , a realibility measure is proposed accord- ing the usual notion that the more reliable a prediction, the less lia- ble to be wrong. Although this reliability measure is not a quality measure used for comparing different techniques of RS through cross validation, this can be regarded as a quality measure associ- ated to a prediction and a recommendation. In this way, the RS pro- vides a pair of values (prediction value, reliability value), through which users may balance its preference: for example users would probably prefer the option (4,0.9) to the option (4.5,0.1). Conse- quently, the reliability measure proposed in Hernando et al. [96] provides a new understandable factor, which users may consider for taking its decisions. Nevertheless, the use of this reliability measure is just constrained to those RS based on the k NN algorithm. The deÔ¨Ånition of reliability on the prediction, p u , i , is based on two numeric factors: s u , i and v u , i . s u , i measures the similar- ity of the neighbors used for making the prediction p u , i ; v u , i measures the degree of disagreement between these neighbors rating the item i . Finally, the reliablity measure is deÔ¨Åned as follows: f S √∞ s u ; i √û ¬º 1   s  s √æ s u ; i ; s u ; i ¬º X v 2 K u ; i sim √∞ u ; v √û √∞ 12 √û where f S √∞ s u ; i √û ¬º 1   s  s √æ s u ; i ; s u ; i ¬º X v 2 K u ; i sim √∞ u ; v √û √∞ 13 √û Fig. 7. Recommender systems evaluation process. 118 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 f v √∞ v u ; i √û ¬º max  min  v u ; i max  min   ln 0 : 5 ln max  min   v max  min ; v u ; i ¬º P v 2 K u ; i sim √∞ u ; v √û√∞ r v ; i   r v  p u ; i √æ  r u √û 2 P v 2 K u ; i sim √∞ u ; v √û √∞ 14 √û where  s and  v are respectively the median of the values of s u , i and v u , i in the speciÔ¨Åc RS. K u , i is the set of neighbors of u which have rated the item i . {min, . . . ,max} is the discrete range of rating values. Fig. 7 shows the general mechanism for cross validation used to generate quality results form the evaluation measures. The data- base is divided in training and test areas for both users and items. In the Ô¨Årst phase (top on the left side), k -neighbors are calculated for the active user (while the active user is selected from the set of test users, the k -neighbors are selected from the set of training users). In the aggregation phase (top on the right side), predictions are calculated for the active user (from the set of test items). Final- ly, evaluation metrics are used to compare the predictions and rec- ommendations obtained with the real ratings of the user; the more accurate the predictions and recommendations, better quality of the proposed recommendation algorithm. 5. Social information As the web 2.0 has developed, RS have increasingly incorpo- rated social information (e.g., trusted and untrusted users, fol- lowed and followers, friends lists, posts, blogs, and tags). This new contextual information [145,216] improves the RS. Social information improves the sparsity problem inherent in memory- based RS because social information reinforces traditional mem- ory-based information (users ratings): users connected by a net- work of trust exhibit signiÔ¨Åcantly higher similarity on items and meta-data that non-connected users [132] . Social information is used by researchers with three primary objectives: (a) to improve the quality of predictions and recom- mendations [53,13] , (b) propose or generate new RS [139,210] , and (c) elucidate the most signiÔ¨Åcant relationships between social information and collaborative processes [100,178] . Trust and reputation is an important area of research in RS [166] ; this area is closely related to the social information currently in- cluded in RS [114] . The most common approachs to generating trust and reputation measurements are the following: (a) user trust: to calculate the credibility of users through explicit informa- tion of the rest of users [239,138] or to calculate the credibility of users through implicit information obtained in a social network [59,150] and (b) item trust: to calculate the reputation of items through a feedback of users [114] or to calculate the reputation of items studying how users work with these items [58,122] . In the social RS Ô¨Åeld, users can introduce labels associated with items. The set of triples h user, item, tag i form information spaces referred to as folksonomies . Fundamentally, folksonomies are used in the following two ways: (1) to create tag recommendation sys- tems (RS based only on tags) [147] and (2) to enrich the recom- mendation processes using tags [81] . Content-based Ô¨Åltering has recently become more important due to the surge in social networks. RS show a clear trend to allow users to introduce content [13,178] , such as comments, critiques, ratings, opinions and labels as well as to establish social relation- ship links (e.g., followed, followers, like user and dislike user). This additional information increases the accuracy of predictions and recommendations, which has generated a variety of research arti- cles: Kim et al. [117] , Zheng and Li [248] and Carrer-Neto et al. [53] . The rest of this section deal is dealt with the concepts and re- search in the two lines considered previously: Filtering of social information and content Ô¨Åltering. 5.1. Social Filtering Social information can be gathered explicitly or implicitly through identiÔ¨Åcation of a community network or afÔ¨Ånity network [196] using the individual information that users generate (e.g., communications and web logs) [178] . Even using only the ratings from the users, it is possible to improve the RS results creating an implicit social networking [180] . Both implicit and explicit information sources can be combined to generate recommenda- tions [144] . The explicit social information can be used via a trust-based CF in order to improve the quality of recommendations. Trust infor- mation can be generated or used through different approaches, such as trust propagation mechanisms [42] , a ‚Äòfollow the leader‚Äô approach [8,186] , personality-based similarity measures [101] , trust networks [239,221] , distrust analysis [223,20] , and dynamic trust based on the ant colonies metaphor [20] . Most of the research work that uses social information applied to RS aims to obtain improvements in the recommendations made by referring to the extra information provided by the social infor- mation used. Among the most relevant current work which uses this approach we have: Woerndl and Groh [231] use social net- works to enhance collaborative Ô¨Åltering; Their evaluation shows that the social recommender outperforms traditional collaborative Ô¨Åltering algorithms in the used scenario. Arazy et al. [13] improve accuracy by using data from online social networks and electronic communication tools. Xin et al. [233] propose an approach for improving RS through exploiting the learners note taking activity. They maintain that notes‚Äô features can be exploited by collabora- tive learning systems in order to enrich and extend the user proÔ¨Åle and improve personalized learning. The Bonhard and Sasse [41] re- search has shown that the relationship between advice-seeker and recommender is extremely important, so ways of indicating social closeness and taste overlap are required. They thus suggest that drawing on similarity and familiarity between the user and the persons who have rated the items can aid judgment and decision making. Fengkun and Hong [75] developed a way to increase rec- ommendation effectiveness by incorporating social network infor- mation into CF. They collected data about users‚Äô preference ratings and their social network relationships from a social networking web site; then, they evaluated CF performance with diverse neigh- bor groups combining groups of friends and nearest neighbors. Carmagnola et al. [52] state that joining in a network with other people exposes individuals to social dynamics which can inÔ¨Çuence their attitudes, behaviors and preferences: They present SoNARS , an algorithm for recommending content in social RS. SoNARS tar- gets users as members of social networks, suggesting items that re- Ô¨Çect the trend of the network itself, based on its structure and on the inÔ¨Çuence relationships among users. In Ramaswamy et al. [189] the design of the social network based RS incorporates three features that complement each other to derive highly targeted ads. First, they analyze information such as customer‚Äôs address books to estimate the level of social afÔ¨Ånity among various users. This social afÔ¨Ånity information is used to identify the recommendations to be sent to an individual user. Another group of research work uses social information to cre- ate or enable RS. That is, the aim is not to improve the results of a particular RS in operation, the aim is to propose or make possible RS which still do not exist, or if they do exist they are not based on social information: The Siersdorfer and Sergei [210] objective is to construct social recommender systems that predict the utility of items, users, or groups based on the multi-dimensional social environment of a given user; they do a mining of the rich set of structures and social relationships that provides the folksonomies. In the Li and Chen [137] study they propose a blog recommenda- tion mechanism that combines trust model, social relation and J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 119 semantic analysis and illustrates how it can be applied to a presti- gious online blogging system. In the Jason [111] research project, they have applied a system to discover the social networks be- tween mobile users by collecting a dataset from about two millions of users. They argue that social network is applicable to generate context-based recommendation services. Jyun and Chui [115] pa- per uses trading relationships to calculate level of recommendation for trusted online auction sellers. They demonstrate that network structures formed by transactional histories can be used to expose such underlying opportunistic collusive seller behaviors. In Dell‚Äôamico and Capra [69] users‚Äô trustworthiness has been mea- sured according to one of the following two criteria: taste similar- ity (i.e., ‚Äò‚ÄòI trust those who agree with me‚Äô‚Äô), or social ties (i.e., ‚Äò‚ÄòI trust my friends, and the people that my friends trust‚Äô‚Äô). They argue that, in order to be trusted, users must be both well intentioned and competent. Based on this observation, they propose a novel approach that they call social Ô¨Åltering. A third group of work provides the foundation of the research to discover the most signiÔ¨Åcant relationships between social informa- tion and collaborative processes, without creating, proposing or improving any particular RS. This research moves at a higher level of abstraction, with the aim of establishing bases and general prin- ciples. Bonhard [40] paper explains that qualitative research con- ducted to date has shown that the relationship between recommender and recommendee has a signiÔ¨Åcant impact on deci- sion-making. Hossain and Fazio [100] present a study exploring the connection between social networks and collaborative process. They focus on exploring academics‚Äô network position and its effect on their collaborative networks. By deÔ¨Åning network position in this way, they develop a social network that uses the academics as nodes within the network instead of each published paper. The Esslimani et al. [72] paper presents a new CF approach based on a behavioral network that uses navigational patterns to model relationships between users and exploits social networks tech- niques. Golbeck and Kuter [86] present an experimental study of several types of trust inference algorithms to answer the following questions on trust and change: How far does a single change prop- agate through the network? How large is the impact of that change? How does this relate to the type of inference algorithm? The experimental results provide insights into which algorithms are most suitable for certain applications. Research in the Ô¨Åeld of trust and reputation could provide a suitable starting point to create social interaction among users of the RS, however, the most relevant work on the subject is limited to the use of trust relationships to improve the quality of the rec- ommendation services. O‚Äôdonovan [165] book chapter examines the diversity of sources from which trust information can be har- nessed within social web applications and discusses a high level classiÔ¨Åcation of those sources. It is shown that harnessing an in- creased amount of information upon which to make trust decisions greatly enhances the user experience with the social web applica- tion. Massa and Avesani [151] explain that RS making use of trust information are the most effective in term of accuracy while pre- serving a good coverage. This is especially evident on users who provided few ratings. Yuan et al. [239] choose the trust aware RS as an example to demonstrate the advantages by making use of the veriÔ¨Åed small-world nature of the trust network. Li and Kao [138] present a RS based on the trust of social networks; Through the trust computing, the quality and the veracity of peer produc- tion services can be appropriately assessed. The experimental re- sults show that the proposed RS can signiÔ¨Åcantly enhance the quality of peer production services. Table 3 classiÔ¨Åes the current approaches to address user credi- bility and item reputation in social-based RS. In the CF Ô¨Åeld, the trust of users is used to make predictions, weighting trust values. That is to say, the more trust a user has, the more important its ratings are for making predictions [58,112,239] . In Ma et al. [145] , they propose a probabilistic factor analysis framework, combining ratings and trusted friends; this framework can be applied to pure user-item rating matrix. 5.2. Content-based Ô¨Åltering Content-based Ô¨Åltering (CBF) tries to recommend items to the active user similar to those rated positively in the past. It is based on the concept that items with similar attributes will be rated sim- ilarly [16,177,203] . For example, if a user likes a web page with the words ‚Äò‚Äòcar‚Äô‚Äô, ‚Äò‚Äòengine‚Äô‚Äô and ‚Äò‚Äògasoline‚Äô‚Äô, the CBF will recommend pages related to the automotive world. CBF is becoming especially important as RS incorporate infor- mation on items from users working in web 2.0 environments, such as tags, posts, opinions and multimedia material. Two challenging problems for content-based Ô¨Åltering are lim- ited content analysis and overspecialization [3] . The Ô¨Årst problem arises from the difÔ¨Åculty in extracting reliable automated informa- tion from various content (e.g., images, video, audio and text), which can greatly reduce the quality of recommendations. The sec- ond problem (overspecialization) refers to the phenomenon in which users only receive recommendations for items that are very similar to items they liked or preferred; therefore, the users are not receiving recommendations for items that they might like but are unknown (e.g., when a user only receives recommendations about Ô¨Åction Ô¨Ålms). Recommendations can be evaluated for novelty [32,105] . For CBF to operate, attributes of the items you wish to recom- mend must be extracted [176] . Typically, a set of attributes is man- ually deÔ¨Åned for each item depending on its domain. In certain instances, such as when it is desired to recommend textual infor- mation, classic information retrieval techniques must be used to automatically deÔ¨Åne such attributes (e.g., term frequency, inverse document frequency and normalization to page length). Fig. 8 shows the CBF mechanism, which includes the following steps: (1) extract the attributes of items for recommendation, (2) compare the attributes of items with the preferences of the active Table 3 State of the art on trust and reputation. User trust Item trust Explicit trust systems The ‚Äòcredibility‚Äô of users is calculated through explicit information of the rest of users. [71,239,240] . Services P2P usually implement this technique [138] The ‚Äòreputation‚Äô of items is calculated by means of a feedback of users who are asked about their opinions [114] . E-commerce services often use this technique Implicit trust systems The ‚Äòcredibility‚Äô of users is calculated through implicit information obtained in a social network [59,150,200] The ‚Äòreputation‚Äô of items is calculated studying how users work with these items (for example, the number of times a song is played) [58,122] Memory based trust The ‚Äòcredibility‚Äô measure is calculated taking into account the users‚Äô ratings [112,127,145] 120 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 user, and (3) recommend items with characteristics that Ô¨Åt the user‚Äôs interests. When the attributes of the items and the user proÔ¨Åles are known, the key purpose for CBF [158] is to determine whether a user will like a speciÔ¨Åc item. This task is resolved traditionally by using heuristic methods [198,15,79] or classiÔ¨Åcation algorithms , such us: rule induction [65,119] , nearest neighbors methods [236,27] , Rocchio‚Äôs algorithm [131,16] , linear classiÔ¨Åers [113] , and probabilistic methods [175,160,84] . The pure CBF has several shortcomings [16,176,212] : (a) In certain domains (e.g., music, blogs, and videos), it is a complicated task to generate the attributes for items. (b) CBF suffers from an overspecialization problem because by nature it tends to recommend the same types of items. (c) It is more difÔ¨Åcult to acquire feedback from users because with CBF, users do not typically rate the items (as in CF), and, therefore, it is not possible to determine whether the recommendation is correct. Because of these shortcomings, it is rare to Ô¨Ånd a pure CBF implementation. It is more common to use the hybrid CBF/CF Burke 2002. CF solves CBF‚Äôs problems because it can function in any domain; it is less affected by overspecialization; and it ac- quires feedback from users. CBF adds the following qualities to CF: improvement to the quality of the predictions, because they are calculated with more information, and reduced impact from the cold-start and sparsity problems. CBF and CF can be combined in different ways [3] . Fig. 9 shows the different alternatives. Fig. 9 a shows the methods that calculate CBF and CF recom- mendations separately and subsequently combine them. Claypool et al. [64] propose to use a weighted average for combining CBF and CF predictions depending on the type of prediction. In another study, Pazzani [177] proposes combining the CBF and CF recom- mendation lists by assigning the items scores according to their position on the lists. Additionally, Billsus and Pazzani [26] and Tran and Cohen [218] propose to select the CBF or CF prediction in accordance with the quality. Fig. 9 b depicts the methods that incorporate CBF characteristics into the CF approach. Balabanovic and Shoham [16] maintain user proÔ¨Åles based on content analysis and directly compare the pro- Ô¨Åles to determine similar users for CF recommendations. Good et al. [89] construct specialized Ô¨Ålterbots using CBF techniques, which later act as neighbors in the CF stage. Melville et al. [157] propose to add predictions from the CBF into the ratting matrix employed by the CF. Li [136] modiÔ¨Åes the ratting matrix, which is input for the CF, by combining it with another matrix generated from clustering the items according to their attributes. In Hu and Pu [101] , authors incorporate personality characteristics in the CF similarity measure to minimize the new-user problem. Fig. 9 c illustrates the methods to construct a uniÔ¨Åed model with both CBF and CF characteristics. Basu et al. [19] propose using CBF and CF characteristics in a single rule-based classiÔ¨Åer. Popescul et al. [182] and Schein et al. [204] propose using prob- ability models to combine CBF and CF recommendations. In an- other studies [66,10,50] , the authors employ Bayesian networks to combine CBF and CF characteristics and generate more accu- rate recommendations. Burke [45] and Middleton et al. [159] propose using knowledge-based techniques to solve the cold- start problem. Fig. 9 d shows the methods that incorporate CF characteristics into a CBF approach. In Soboroff and Nicholas [211] , the authors use LSI to create the user proÔ¨Åles used in CBF recommendations beginning with the CF ratting matrix. Mooney and Roy [160] use CF system predictions as input for CBF. The current trend in CBF is to add social information to the items attributes, such as tags, comments, opinion, and social net- work sharing. Social tagging systems are the most popular because they allow users to annotate online resources with arbitrary labels, which produces rich information spaces ( folksonomies ). These new components have opened novel lines of RS research that can be di- vided into two categories: (1) tag recommendation systems and (2) use of tags in the recommendation process: (1) RS tags attempt to provide personalized item recommenda- tions to users through the most representative tags. In J√§chke et al. [110] , the authors compare different mecha- nisms for tags recommendations. Marinho and Schmidt-Thi- eme [147] improve tags recommendations by applying classic recommendation methods. Additionally, Landia and Anand [130] propose a method that combines clustering- based CBF with CF to suggest new tags to users. (2) The methods using tags in the recommendation process increase the capacity of traditional RS. Tso-Sutter et al. [219] propose a generic method that allows tags to be incor- porated to standard CF algorithms. Bogers and Van Den Bosh [39] examine how to incorporate the tags and other metada- ta into a hybrid CBF/CF algorithm by replacing the tradi- tional user-based and item-based similarity measures by tag overlap. Gemmell et al. [83] propose a weighted hybrid recommender, wherein they combine the graph-based tag recommendations with user-based CF and item-based CF. Gedikli and Jannach [81] propose to use tags as a means to express which features of an item users particularly like or dislike. In Gemmell et al. [82] , the authors offer a hybrid RS, wherein they predict the user preferences for items by only consulting the user‚Äôs tagging history. 6. Additional recommender systems objectives Commercial RS compete in the market by offering the best con- tent and quality in recommendations as well as greatest variety of services. Recommendations to user groups [108] facilitate joint recommendations to user groups (e.g., a group of four friends who wish to choose a movie). For CF, four design approaches offer an opportunity for action: (1) acting into the similarity measures stage [168] , (2) acquiring neighbors [37] , (3) acquiring predictions [63] , and (4) generating recommendations [17] . Research results [168] indicate that the quality of the recommendations does not vary greatly between the different approaches, but the execution time is dramatically reduced as we advance when it is used (when the design of a similarity measure for groups is the most efÔ¨Åcient solution). For the RS generated recommendations to be valuable for users, they must be explained well in a simple, compelling and accurate manner. The recommendation explanation Ô¨Åeld has been investi- gated with new developments in RS [91] until now [170] . Tradi- tionally, the explanation type is divided into the following categories: (a) human style (user to user approach), (b) item style (item to item approach), (c) feature style (items features), and (d) hybrid . It also employs the use of conversational techniques [155] and incorporates geo-social information [235] . 6.1. Recommending to groups of users RS that consider groups of users [108] are starting to expand and to be used in different areas: tourism [14] , music [55] , TV [238] , web [176] . Given the speciÔ¨Åc characteristics of the recommendation to groups, it is appropriate to establish a consensus for different J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 121 group semantics that formalize the agreements and disagreements among users [195] . With the aim of presenting the work carried out to date in a structured way, we provide a classiÔ¨Åcation of the recommendation to groups in CF RS. Fig. 10 graphically illustrates the four basic lev- els on which we can act in order to unify the group‚Äôs users‚Äô data with the objective of obtaining the data of the group of users: sim- ilarity metric, establishing the neighborhood, prediction phase, determination of recommended items. In Fig. 10 , the individual members of a group are represented on the left, in grey; each graticule represents the matrix of ratings by the users (horizontal) on the items (vertical). The graph shows the four representative cases of tackling the solution to recommenda- tion by groups (one case for each matrix on the left of the Ô¨Ågure). The circles show key information: they indicate the CF process phase where the uniÔ¨Åcation is performed: ‚Äò‚Äò n users ? 1 group‚Äô‚Äô. In the Ô¨Årst case, at the top of the graph, the data uniÔ¨Åcation is performed in the prediction phase of the CF process: n individual predictions of n users of the group are combined in one prediction of the group (predictions aggregation). This approach has been used by Berkovsky and Freyne [22] , Garc√≠a et al. [78] and Christen- sen and SchiafÔ¨Åno [63] . The second case acts on the sets of neighbors of the group‚Äôs users, by unifying them in one neighborhood for the whole group. This approach has been studied by Bobadilla et al. [37] , proposing the intersection of a large number ( k ) of neighbors of each user of the group. In the third case, the recommendations obtained for each indi- vidual user of the group are merged into one recommendation for the group. Baltrunas et al. [17] use rank aggregation of individual lists of recommendations. The fourth case [168] uses a similarity metric that acts directly on the set of ratings of the group of users. This solution is the only one that directly provides a set of neighbors for the group of users. A study exists [9] which, prior to any of the previous cases, pro- poses, as a front-end, the incorporation of a process of estimation of missing information when dealing with incomplete fuzzy lin- guistic preference relations. 6.2. Explaining recommendations An important research subject in the RS Ô¨Åeld focuses on provid- ing explanations that justify the recommendations the user has re- ceived. This is an important aspect of an RS because it aids in maintaining a higher degree of user conÔ¨Ådence in the results gen- erated by the system. The type of explanations used thus far can be classiÔ¨Åed as fol- lows [170] . Human style explanations (user to user approach). For example, we recommend movie i because it was liked by the users who rated movies j , k , m , . . . very positively ( j , k , m , . . . are movies rated well by the active user). Item style explanations (item to item approach). For example, we recommend the vacation destination i because you liked the vacation destinations g , c , r , . . . ( g , c , r , . . . are vacation destina- tions similar to i and rated well by the active user). Feature style explanations (it is recommended based on items‚Äô features). For example, we recommend movie i because it was directed by director d , it features actors a , b , and it belongs to genre g ( d , a , b , g are features the active user is interested in). Hybrid methods . This category primarily includes the following: human/item, human/feature, feature/item, and human/feature/ item. Additionally, in geo-social RS (Foursquare, Google latitude, etc.), location information exists that must be used in the recommenda- tion explanation mechanism [235] . Geo-social RS typically adopt a hybrid human/item explanation method based on social, location and memory-based information. A reference publication that is a helpful introduction to the RS explanations research Ô¨Åeld has been published previously [91] . They explore the utility of explanations in CF RS, and they stated three key research questions: (1) What models and techniques are effective in supporting explanations? (2) Can explanation facil- ities increase the acceptance of CF RS? (3) Can explanation facilities increase the Ô¨Åltering performance of the CF RS users? To answer to the Ô¨Årst question, they propose using rating histograms, indica- tions of past performance, comparisons to similar rated items, and use of domain speciÔ¨Åc content features. The results from the experiments conducted with RS users support an afÔ¨Årmative re- sponse to the second question. The third question is unanswered Fig. 8. Content-based Ô¨Åltering mechanism. Fig. 9. Different alternatives for combining CF and CBF. 122 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 because users perform Ô¨Åltering based on many different channels of input. A dynamic approach that favors the mechanisms for RS expla- nations includes using conversational techniques, such as the CCBR (conversational case-base reasoning), explained into McSherry [155] . As CCBR they use an incremental nearest neighbor process based on the Pareto case dominance approach. In a different study [153] , a dynamic approach is also adopted, but it employs a differ- ent perspective. Instead of attempting to justify a particular recom- mendation they focus on how explanations can help users to understand the recommendation opportunities that remain if the current recommendation should not meet their requirements. They generate compound critiques as explanations: Users have the opportunity to accept or critique recommendations. If they cri- tique a recommendation, the critique acts as a Ô¨Ålter over the remaining recommendations. In a separate study [24] , authors differentiate between the con- cepts promotion (increasing of the acceptance of the recommended item) and satisfaction (user satisfaction with the recommended item). They also produced better results by using the keyword style explanation (based on content data) compared with the neighbor style explanation (human style explanation). Authors propose a new classiÔ¨Åcation of the recommendation justiÔ¨Åcations: Keyword Style Explanation (for content-based RS), Neighbor Style Explana- tion (for collaborative Ô¨Åltering RS) and InÔ¨Çuence Style Explanation (tells the user how their interactions with the RS inÔ¨Çuences the recommendation). Tintarev and Masthoff [217] describe the advantages of making justiÔ¨Åcations in recommendations: trans- parency, scutability, trustworthiness, effectiveness, persuasive- ness, efÔ¨Åciency and satisfaction. Billus and Pazzani [25] propose a recommendation system on news, which provides keyword style justiÔ¨Åcations of the recom- mendations through the weights used for obtaining these recom- mendations. Wang et al. [226] describe a system of justiÔ¨Åcations based on the features of users‚Äô preference. Tintarnev and Masthoff [217] design a recommedation system on Ô¨Ålms whose recommen- dations are justiÔ¨Åed through the features. Vig et al. [222] propose a mechanism for justifying recommendations called tagsplanations, which is based on community tags. Trangsplanations have two key components: tag relevance, the degree to which a tag describes an item; and tag preference, the user‚Äôs sentiment toward a tag. Fahri [73] provides a framework for organizing justiÔ¨Åcations, used to categorize explanations; they propose the categorization of the discourse: explicative, theoretical, pragmatic, ethical, moral, legal, aesthetic, and personal. Although this theoretical framework has not been used into the research literature, it can be used to de- sign new types of explanations. Hernando et al. [97] present a no- vel explanation technique based on the visualization of trees of items; these trees provide valuable information about the reliabil- ity of recommendations and the importance of the ratings the user has made. The most relevant investigations that produce justiÔ¨Åcations in recommender systems include a study [187] wherein the authors design a new organization interface where results are grouped according to their tradeoff properties. They have developed a trust model for recommender agents based on the Pareto algorithm (excluding dominated categories). Symeonidis et al. [213] Ô¨Årst con- struct a feature proÔ¨Åle for the users to reveal their favorite features, later they group users into biclusters to exploit partial matching between de preferences of groups of users over groups of items. Additionally they propose a metric to measure the quality of justi- Ô¨Åcations: the explain coverage ratio. In Symeonidis et al. [214] they use a prototype ‚Äò‚ÄòMoviExplain‚Äô‚Äô to put into the test the research showed into Symeonidis et al. [213] . In Hu et al. [102] they use im- plicit feedback to derive an estimate of the user preference (like or dislike an item) and user conÔ¨Ådence for each user-item pair. 7. Recommender systems trends From the evolution of existing RS and research papers in the Ô¨Åeld, there is a clear tendency to collect and integrate more and different types of data. This trend is parallel to the evolution of the web, which we can deÔ¨Åne through the following three primary stages: (1) at the genesis of the web, RS used only the explicit rat- ings from users as well as their demographic information and con- tent-based information included by the RS owners. (2) For the web 2.0, in addition to the above information, RS collect and use social Fig. 10. ClassiÔ¨Åcation of the recommendations to groups in CF RS. The Ô¨Ågure represents the four representative cases for approaching the solution to group recommendations. J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 123 information, such as friends, followers, followed, both trusted and untrusted. Simultaneously, users aid in the collaborative inclusion of such information: blogs, tags, comments, photos and videos. (3) For the web 3.0 and the Internet of things, context-aware informa- tion from a variety of devices and sensors will be incorporated with the above information. Currently, geographic information is included, and the expected trend is gradual incorporation of information, such as radio frequency identiÔ¨Åcation (RFID) data, sur- veillance data, on-line health parameters and food and shopping habits, as well as teleoperation and telepresence. Context-aware recommender systems [5,1] , focus on additional contextual information, such as time, location, and wireless sensor networks [80] . The contextual information can be obtained explic- itly, implicitly, using data mining or with a mixture of these meth- ods (hybrid). Currently, mobile applications increasingly use geographic information; this information enables geographic RS that can be considered as location-aware RS. For geographic RS [167,152] , recommendations are typically generated by consider- ing the geographical position of the user that receives the recommendation. This section provides an introduction of concepts, which are gaining popularity in the RS research Ô¨Åeld: Internet of things, pri- vacy preservation, shilling attacks, new frameworks, etc. In this introduction, we provide a novel classiÔ¨Åcation for analyzing these RS concepts. Next, we will deal with the research on the loca- tion-aware RS, which may be regarded as the Ô¨Årst steps for future RS based on Web 3.0. Finally, we will describe the most signiÔ¨Åca- tive results on a promising research Ô¨Åeld: the RS based on bio-in- spired models. 7.1. Introduction There is a clear trend towards collection of implicit information instead of a traditional explicit evaluation of items by ratings. Last.Fm is a good example of this situation; the user ratings are in- ferred by the number of times they have heard each song. The same can be applied in a number of everyday situations, such as for access to web addresses, use of various public transport sys- tems, food purchased, access to sports facilities and access to learn- ing resources. Incorporation of implicit information on the daily habits of users allows RS to use a variety of data; these data will be used in future CF processes, which are increasingly useful and accurate. Privacy and security considerations will be increasingly important with the widespread trend in using, with consent, devices and sen- sors for the Internet of things. Privacy is an important issue for RS [23] because the systems contain information on large numbers of registered users. For pri- vacy preservation in RS, a certain level of uncertainty must be intro- duced into the predictions [156] , primarily through tradeoffs between accuracy and privacy [146] . Furthermore, privacy can be preserved when different RS companies share information (com- bining their data) [116,242] . Privacy becomes more important as RS increasingly incorporate social information. Because RS are often used in electronic commerce , unscrupulous producers may Ô¨Ånd proÔ¨Åtable to shill RS by lying to the systems in order to have their products recommended more often than those of their competitors. RS can experience shilling attacks [128,57] , which generate many positive ratings for a product, while products from competitors receive negative ratings. RS are still highly vul- nerable to such attacks [191] . Knowledge-based Ô¨Åltering is emerging as an important Ô¨Åeld of RS. Knowledge RS [46] ‚Äò‚Äòuse knowledge about users and products to pursue a knowledge-based approach to generating recommenda- tions, reasoning about what products meet the user‚Äôs requeri- ments‚Äô‚Äô. Recommendations are based on inferences about users needs and preferences. User models are based on knowledge struc- tures such as querys (preferred features por products) [109] , cases (case-based reasoning) [44] , constraints (constraint-based reason- ing) [74] , ontologies [159] , matching metrics and knowledge vec- tors [194] , and social knowledge [53] . WorkÔ¨Çow is a current knowledge Ô¨Åeld where the user model is based on ‚Äò‚Äòusers-roles-tasks reference information that describes which member plays which roles or fulÔ¨Ålls which tasks‚Äô‚Äô [245,246] . Peer-to-peer (P2P) networks are other current knowl- edge Ô¨Åeld, where user information is based on the distributed information existing from each peer and the set of peers who may need her [247] . Gradual incorporation of different types of information (e.g., ex- plicit ratings, social relations, user contents, locations, use trends, knowledge-based information) has forced RS to use hybrid ap- proaches. Once the memory-based, social and location-aware methods and algorithms are consolidated, the evolution of RS dem- onstrates a clear trend toward combining existing collaborative methods. The latest research in the CF Ô¨Åeld has generated only modest improvements for predictions and recommendations from a single type of information (e.g., when the only information used is user ratings, information from social relations, or item content). The re- sults improve further when several algorithms are combined with their respective data types. A growing number of publications ad- dress hybrid approaches that use current databases to simulta- neously incorporate memory-based, social and content-based information. To unify the above concepts, Fig. 11 provides an original taxon- omy for RS. The taxonomy is classiÔ¨Åed depending on the nature of the data rather than according to the methods and algorithms used. The core of the taxonomy focuses on data classiÔ¨Åcation by three factors: (1) the target of the data: user or item; (2) mode of acquisition: explicit (i.e., ratings to items made by users) or impli- cit (e.g., number of times a user has heard a song); and (3) informa- tion level: memory, content or social context. Fig. 11 shows the recommender methods and algorithms (la- beled as ‚Äò‚Äòcollaborative Ô¨Åltering algorithms‚Äô‚Äô). Depending on the information type in each RS database, it adopts a hybrid Ô¨Åltering approach. Each hybrid approach will use an appropriate subset of algorithms to consider processing of existing information in a coor- dinated manner. Future developments will include different rec- ommendation frameworks that address the most common situations. These frameworks allow RS to incorporate the CF kernel with the most appropriate recommendations methods based on the available information in a simple and straightforward manner. At higher levels (prediction and recommendation), Fig. 11 incorporates current evaluation quality measures, such as those for diversity and novelty. The importance of such measures, and measures developed in the future will grow as users demand novel, stable and less predictable recommendations. 7.2. Location-aware recommender systems Due to the increasing use of mobile devices, location-aware sys- tems are becoming more widespread. These systems show a ten- dency towards their consolidation as web 3.0 services and this naturally leads to location-aware CF and location-aware RS, which can be called geographic CF and geographic RS. We introduce a classiÔ¨Åcation for geographic CF RS and focus on the most relevant section of the classiÔ¨Åcation obtained. Table 4 establishes the different possibilities of tackling a geographic RS according to the nature of the ratings made (‚Äò‚Äòrating stage‚Äô‚Äô) and the recommendation process followed (‚Äò‚Äòrecommendation stage‚Äô‚Äô). ‚Äò‚ÄòUser‚Äô‚Äô indicates that the rating and/or recommendation are made without having or using the user‚Äôs Geographic Information (GI). 124 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 Similarly, ‚Äò‚ÄòItem‚Äô‚Äô indicates that the rating and/or recommendation are made without having or using the item‚Äôs GI. In the cases la- beled as ‚Äò‚ÄòUser g ‚Äô‚Äô and ‚Äò‚ÄòItem g ‚Äô‚Äô the GI is used. The cases identiÔ¨Åed are:  RS: Traditional RS, in which ratings and recommendations are made without using geographical information.  RS + G: Traditional RS, which also contributes the item‚Äôs geo- graphical position. These RS cannot be regarded as geographic RS, as the GI does not play a part in the recommendation process.  GRS: This group of Geographic RS is most likely to become pop- ular in the near future. In these, ratings are made in a traditional way, whilst recommendations are made by considering the geo- graphical position of the user to whom the recommendation is to be made. A representative example is that of a RS for restau- rants; the users rate a restaurant using very diverse concepts, which do not include the distance at the time of voting between the user and the restaurant. However, users of a Geographic RS expects a restaurant to be recommended to them not only because of good ratings from similar users ( k -neighbors), but also according to the distance between their current position and that of the restaurant. Other possible examples are RS for cinemas, pubs, supermarkets, cultural activities in a city, lan- guage learning centers, gyms and sports clubs, etc.  GRS + : In this case, users establish ratings on items by weighting the distance between them and the items rated. In this type of geographic RS two possibilities can be established: 1. Hybrid CF/Demographic Ô¨Åltering: Each item accepts a max- imum of one vote per user, to which the geographical posi- tion from which it has been issued is associated. 2. Geographic RS where each item accepts more than one rat- ing for each user, depending on the geographical position from which each rating is made. 3. The hybrid RS in case 1 respond to regional or national geo- graphical approaches, in which recommendations can be established according to weighting between the similarity of the votes (CF) and their origin. This type of GRS may be regarded as an extended case of hybrid CF/demographic Ô¨Åltering, in which the GI is given for each vote instead of for each user. From a theoretical point of view, Type 2 GRS + are the most com- plete; however, from a practical point of view, they involve a semantic difÔ¨Åculty in the item rating process, which makes their use very difÔ¨Åcult. Rating items in this GRS + involves that each user can rate items according to the relative distances between the user and the items. In this way, a user can rate a restaurant from their home differently to how they would rate it from their workplace; and when the distances are very different, the ratings are also likely to be so. The mental process would be something like this: I am 1 km from the restaurant and I rate very positively travelling 1 km to go to that restaurant which I think is good; but after some time, the same user, who is at work, 24 km away from the restau- rant, could cast a vote indicating they do not consider it to be po- sitive to travel 24 km to go to the restaurant even if they think it is good. In summary, GRS + have the advantage that they accept a wider variety of ratings and that these also contain the relative impor- tance that each user gives to the items according to the distance re- quired to access them. The disadvantage is that it is difÔ¨Åcult to involve users in a particularly complex and demanding ratings process. This subsection focuses on the GRS-type geographic CF RS. At present, there are few publications regarding GI-based RS; This is due, to a great extent, to the lack of public databases that include ratings and geographic positions capable of being combined in an RS. Some of the publications that focus more closely on the Ô¨Åeld are as follows: Martinez et al. [149] and Biuk-Aghai et al. [28] are examples of the RS + G group. In Schlieder [205] , they propose a novel approach for modeling the collaborative semantics of geographic folksono- mies. This approach is based on multi-object tagging, that is, the analysis of tags that users assign to composite objects. This paper is based on the concept of groups of people who share a common geospatial feature data dictionary (including deÔ¨Ånitions of feature relationships) and a common metadata schema. Wan-Shiou et al. [225] can be considered as a hybrid content based/geographic RS. The core of the system is a hybrid content based/geographic recommendation mechanism that analyzes a customer‚Äôs history and position so that vendor information can be ranked according to the match with the preferences of a customer. Matyas and Schlieder [152] show a collaborative system that we could situate between a RS and a GRS. In this case, the users‚Äô ratings are taken based on the photos they have downloaded from a Web 2.0 and the photos they have uploaded to the same Web (the photos have a GPS address associated to them). After this, a search of k -neighborhoods based on this data is carried out. The recommendation process does not take into account the user‚Äôs position. It is possible to collect travel GPS traces from users and use the database to generate recommendations [249] . The travel GPS traces can be reinforced with social information based on friends [250] . Both papers can be classiÔ¨Åed as GRS + . 7.3. Bio-inspired approaches Much of the proposed model-based RS are based on bio-in- spired approaches, which primarily use Genetic Algorithms (GAs) and Neural Networks (NNs). Models have also been proposed based on ArtiÔ¨Åcial Immune Networks (AINs). GA are heuristic approaches based on evolutionary principles such as natural selection and survival of the Ô¨Åtest. GA have mainly been used in two aspects of RS: clustering [120,243] and hybrid user models [76,99,7] . A common technique to improve the fea- tures of RS consists of initially carrying out a clustering on all of the users, in such a way that a group of classes of similar users is obtained, after this, the desired CF techniques can be applied to each of the clusters, obtaining similar results but in much shorter calculation times; It is usual to use common genetic clustering algorithms such as GA-based K -means [121] . The RS hybrid user models commonly use a combination of CF with demographic Ô¨Åltering or CF with content based Ô¨Åltering, to exploit merits of each one of these techniques. In these cases, the chromosome structure can easily contain the demographic charac- teristics and/or those related to content-based Ô¨Åltering. In order to tackle location-based advertisement, Dao et al. [68] propose a model-based CF using GA. They combine both user‚Äôs preferences and interaction context. Bobadilla et al. [33] use GA to create a similarity metric, weighting a set of very simple similar- ity measures. Hwang et al. [106] employ a GA to learn personal preferences of customers. NN is a model based on the observed behavior of biological neu- rons. This model, intended to simulate the way the brain processes information, enables the computer to ‚Äò‚Äòlearn‚Äô‚Äô to a certain degree. A NN typically consists of a number of interconnected nodes. Each handles a designated sphere of knowledge, and has several inputs from the network. Based on the inputs it gets, a node can ‚Äò‚Äòlearn‚Äô‚Äô about the relationships between sets of data, pattern, and, based upon operational feedback, are molded into the pattern required to generate the required results. J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 125 The RS most relevant research available in which NN usually fo- cuses is hybrid RS, in which NN are used for learn users proÔ¨Åles; NN have also been used in the clustering processes of some RS. The hybrid approaches enable NN to act on the additional infor- mation to the ratings. In Ren et al. [192] they propose a hybrid rec- ommender approach that employs Widrow-Hoff [229] algorithm to learn each user‚Äôs proÔ¨Åle from the contents of rated items. This improves the granularity of the user proÔ¨Åling. In Christakou and Stafylopatis [62] they use a combination of content-based and CF in order to construct a system that provides more precise recom- mendations concerning movies. In Lee and Woo [133] Ô¨Årst, all users are segmented by demographic characteristics and users in Fig. 11. Recommender systems taxonomy. 126 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 each segment are clustered according to the preference of items using the Self-Organizing Map (SOM) NN. Kohonon‚Äôs SOMs are a type of unsupervised learning; their goal is to discover some underlying structure of the data. Two alternative NN uses are presented in Huang et al. [103] and Roh et al. [193] . In the Ô¨Årst case paper, authors use a training back- propagation NN for generating association rules that are mined from a transactional database; in the second paper, authors pro- pose a model that combines a CF algorithm with two machine learning processes: SOM and Case Based Reasoning (CBR) by changing an unsupervised clustering problem into a supervised user preference reasoning problem. Neuro-fuzzy inference has been used in Sevarac et al. [207] to create pedagogical rules in e-learning. A new cold-start similarity measure has been perfected in Bobadilla et al. [36] using optimiza- tion based on neural learning. ArtiÔ¨Åcial immune systems are distributed and adaptive systems using the models and principles derived form the human immune system. They model the defence system which can protect our body against infections. In order to tackle the RS sparsity problem and to make algorithms more scalable, Acilar and Arslan [2] pres- ent a new CF model based on the AIN Algorithm (aiNet). AIN were previously proposed to general recommendations [49] and to rec- ommend web sites [161] . 8. Related works and original contributions of the paper As CF has become more complex, different survey papers have been published in this area. Schafer et al. [203] introduces the core concepts of CF: the theory and practice, the rating systems and their acquisition, evaluation, interaction interfaces and privacy is- sues. Candillier et al. [51] review the main CF Ô¨Åltering methods and compare their results. Su and Khoshgoftaar [212] presents a survey of CF techniques. Authors introduce the theory on CF and concisely deal with the main challenges: sparsity, scalability, synonymy, gray sheep, shil- ling attacks, privacy, etc. They also expose an overview table of CF techniques. Park et al. [171] review 210 papers on RS and classiÔ¨Åes them by the year and journal of the publication, their application Ô¨Åelds, and their data mining techniques. Additionaly, they categorized the pa- pers into eight application Ô¨Åelds (Ô¨Ålms, music, etc.). A review in RS algorithms is presented in [141] . This paper fo- cuses on explaining carefully how the most used algorithms in RS work. The paper presents also the basic concepts of CF and their evaluation metrics, dimensionality reduction techniques, diffu- sion-based methods, social Ô¨Åltering and meta approaches. Our survey tries to include the most novel issues that have not been dealt carefully in the previous papers. Next, we will stand out the most outstanding features of this survey:  Uses a methodology for selecting the most suitable papers in the RS, standing out the latest and most cited papers in the area of RS.  Provides an updated overview table of the most used RS public databases, including tags and friend relations information.  Studies the cold-start problem inherent to all the RS.  Presents a novel overview table informing both the classical similarity measure and those which have recently been pro- posed. It includes both the tailored metrics for cold-start users and the general-purpose metrics. Besides, we show the quality measures obtained when evaluating such metrics.  Includes the recent quality measurements, beyond accuracy, to evaluate RS: novelty, diversity and stability. Additionaly, we include a reliability measure associated to predictions and recommendations.  Provides a comprehensive survey on social Ô¨Åltering, presenting a novel overview table on trust, reputation and credibility.  Introduces the content-based Ô¨Åltering from a modern perspec- tive standing out its application for dealing with social informa- tion, such as social tagging.  Presents a summary of the most relevant contributions in the RS for group of users. We will show a novel classiÔ¨Åcation for the existing methods.  Deals with a fast growing RS Ô¨Åeld: the location-aware RS, based on geographic information. This section is estructured with the help of a novel geographic RS classiÔ¨Åcation table.  Summarizes the most relevant contributions on the use of bio- inspired approaches.  Describes the RS trends to implicitally collect data (specially those derived from the use of Internet of things).  Provides an RS taxonomy for classifying the RS through three factors: source of data (traditional web, social web 2.0, Internet of things/web 3.0); target of data (users, items); method for extracting data (explicit, implicit). 9. Conclusions Recommender systems are proving to be a useful tool for addressing a portion of the information overload phenomenon from the Internet. Its evolution has accompanied the evolution of the web. The Ô¨Årst generation of recommender systems used tradi- tional websites to collect information from the following three sources: (a) content-based data from purchased or used products, (b) demographic data collected in users‚Äô records, and (c) mem- ory-based data collected from users‚Äô item preferences. The second generation of recommender systems, extensively use the web 2.0 by gathering social information (e.g., friends, followers, followed, trusted users, untrusted users). The third generation of recom- mender systems will use the web 3.0 through information pro- vided by the integrated devices on the Internet. The use of location information already incorporated in many recommender systems will be followed by data from devices and sensors, which will be widely used (e.g., real-time health signals, RFID, food habits, online local weather parameters such as temperature and pressure). The Ô¨Årsts recommender systems were focused on improving recommendation accuracy through Ô¨Åltering. Most memory-based methods and algorithms were developed and optimized in this context (e.g., k NN metrics, aggregation approaches, singular value decomposition, diffusion-based methods, etc.). At this stage, hybrid approaches (primarily collaborative‚Äìdemographic and collabora- tive‚Äìcontent Ô¨Åltering) improved the quality of the recommenda- tions. In the second stage, algorithms that included social information with previous hybrid approaches were adapted and developed (e.g., trust-aware algorithms, social adaptive ap- proaches, social networks analysis, etc.). Currently, the hybrid ensemble algorithms incorporate location information into exist- ing recommendation algorithms. Evaluation of the predictions and recommendations has evolved since the origins of recommender systems, which weighted prediction errors (accuracy) heavily. They also recognized the Table 4 Geographic collaborative Ô¨Åltering recommender systems classiÔ¨Åcation. Rating stage Recommendation stage User GI Item Item g Item Item g User RS/GRS ‚Äì RS RS + G Not User g ‚Äì GRS + ‚Äì GRS/GRS + Yes Item GI Not Yes Not Yes J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 127 convenience of evaluating the quality of the top n recommenda- tions as a set; evaluation of the top n recommendations as a ranked list was then incorporated. Currently, there is a tendency to assess new evaluation measures, such as diversity and novelty. Future research will concentrate on advancing the existing methods and algorithms to improve the quality of recommender systems predictions and recommendations. Simultaneously, new lines of research will be developed for Ô¨Åelds and aims, such as on: (1) proper combination of existing recommendation methods that use different types of available information, (2) to get the maximum use of the individual potential of various sensors and devices on the Internet of things, (3) acquisition and integration of trends related to the habits, consumption and tastes of individ- ual users in the recommendation process, (4) data mining from RS databases for non-recommendation uses (e.g., market research, general trends, visualization of differential characteristics of demo- graphic groups), (5) enabling security and privacy for recom- mender systems processes, (6) new evaluation measures and developing a standard for non-standardized evaluation measures, and (7) designing Ô¨Çexible frameworks for automated analysis of heterogeneous data. References [1] S. Abbar, M. Bouzeghoub, S. Lopez, Context-aware recommender systems: a service oriented approach, in: Proceedings of the 3rd International Workshop on Personalized Access, ProÔ¨Åle Management and Context Awareness in Databases, 2009. [2] A.M. Acilar, A. Arslan, A collaborative Ô¨Åltering method based on artiÔ¨Åcial immune network, Expert Systems with Applications 36 (4) (2008) 8324‚Äì 8332 . [3] G. Adomavicius, A. Tuzhilin, Toward the next generation of recommender systems: a survey of the state-of-the-art and possible extensions, IEEE Transactions on Knowledge and Data Engineering 17 (6) (2005) 734‚Äì749 . [4] G. Adomavicius, J. Zhang, On the stability of recommendations algorithms, in: ACM Conference on Recommender Systems, 2010, pp. 47‚Äì54. [5] G. Adomavicius, A. Tuzhilin, Context-Aware recommender Systems, in: F. Ricci, et al. (Ed.), Recommender Systems Handbook, 2011, pp. 217‚Äì253. [6] H.J. Ahn, A new similarity measure for collaborative Ô¨Åltering to alleviate the new user cold-starting problem, Information Sciences 178 (2008) 37‚Äì51 . [7] M.Y.H. Al-Shamri, K.K. Bharadwaj, Fuzzy-genetic approach to recommender systems based on a novel hybrid user model, Expert Systems with Applications 35 (3) (2008) 1386‚Äì1399 . [8] J. Al-Sharawneh, M.A. Williams, Credibility-aware Web-based social network recommender: follow the leader, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 1‚Äì8. [9] S. Alonso, F.J. Cabrerizo, F. Chiclana, F. Herrera, E. Herrera-Viedma, Group decision making with incomplete fuzzy linguistic preference relations, International Journal of Intelligent Systems 24 (2009) 201‚Äì222 . [10] A. Ansari, S. Essegaier, R. Kohli, Internet recommendation systems, Journal of Marketing Research 37 (3) (2000) 363‚Äì375 . [11] N. Antonopoulus, J. Salter, Cinema screen recommender agent: combining collaborative and content-based Ô¨Åltering, IEEE Intelligent Systems (2006) 35‚Äì 41 . [12] P. Antunes, V. Herskovic, S.F. Ochoa, J.A. Pino, Structuring dimensions for collaborative systems evaluation, ACM Computing Surveys 44 (2) (2012). Article 8 . [13] O. Arazy, N. Kumar, B. Shapira, Improving Social Recommender Systems, Journal IT Professional 11 (4) (2009) 31‚Äì37 . [14] L. Ardissono, A. Goy, G. Petrone, M. Segnan, P. Torasso, INTRIGUE: Personalized recommendation of tourist attractions for desktop and handset devices, Applied ArtiÔ¨Åcial Intelligence 17 (8-9) (2003) 687‚Äì714 . [15] R. Baeza-Yates, B. Ribeiro-Neto, Modern Information Retrieval, Addison- Wesley, 1999 . [16] M. Balabanovic, Y. Shoham, Content-based, collaborative recommendation, Communications of the ACM 40 (3) (1997) 66‚Äì72 . [17] L. Baltrunas, T. Makcinskas, F. Ricci, Group recommendation with rank aggregation and collaborative Ô¨Åltering, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 119‚Äì126. [18] A.B. Barrag√°ns-Martƒ±¬¥nez, E. Costa-Montenegro, J.C. Burguillo, M. Rey-L√≥pez, F.A. Mikic-Fonte, A. Peleteiro, A hybrid content-based and item-based collaborative Ô¨Åltering approach to recommend TV programs enhanced with singular value decomposition, Information Sciences 180 (22) (2010) 4290‚Äì 4311 . [19] C. Basu, H. Hirsh, W. Cohen, Recommendation as classiÔ¨Åcation: using social and content-based information in recommendation, in: Proceedings of the Fifteenth National Conference on ArtiÔ¨Åcial Intelligence, 1998, pp. 714‚Äì720. [20] P. Bedi, R. Sharma, Trust based recommender system using ant colony for trust computation, Expert Systems with Applications 39 (1) (2012) 1183‚Äì 1190 . [21] Y. Bengio, Y. Grandvalet, No umbiased estimator of the variance of k-fold cross-validation, Journal of Machine Learning Research 5 (2004) 1089‚Äì1105 . [22] S. Berkovsky, J. Freyne, Group-based recipe recommendations: analysis of data aggregation strategies, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 111‚Äì118. [23] A. Bilge, H. Polat, An improved privacy-preserving DWT-based collaborative Ô¨Åltering scheme, Experts Systems with Applications 39 (3) (2012) 3654‚Äì 3841 . [24] M. Bilgic, R. Mooney, Explanation for recommender systems: satisfaction vs. promotion, in: Next Stage of Recommender Systems Research Workshop (IUI conference), 2005, pp. 13‚Äì18. [25] D. Billsus, M. Pazzani, A personal news agent that talks, learns and explains, in: Proc. Auton. Agents Conf., 1999, pp. 268‚Äì275. [26] D. Billsus, M. Pazzani, User modeling for adaptive news access, User Modeling and User-Adapted Interaction 10 (2‚Äì3) (2000) 147‚Äì180 . [27] D. Billsus, M. Pazzani, J. Chen, A learning agent for wireless news access, in: Proceedings of the International Conference on Intelligent User Interfaces, 2002, pp. 33‚Äì36. [28] R.P. Biuk-Aghai, S. Fong, S. Yain-Whar, Design of a recommender system for mobile tourism multimedia selection, in: 2nd International Conference on Internet Multimedia Services Architecture and Applications (IMSAA), 2008, pp. 1‚Äì6. [29] J. Bobadilla, F. Serradilla, The effect of sparsity on collaborative Ô¨Åltering metrics, in: Australian Database Conference, 2009, pp. 9‚Äì17. [30] J. Bobadilla, F. Serradilla, A. Hernando, Collaborative Ô¨Åltering adapted to recommender systems of e-learning, Knowledge Based Systems 22 (2009) 261‚Äì265 . [31] J. Bobadilla, F. Serradilla, J. Bernal, A new collaborative Ô¨Åltering metric that improves the behavior of recommender systems, Knowledge Based Systems 23 (2010) 520‚Äì528 . [32] J. Bobadilla, A. Hernando, F. Ortega, J. Bernal, A framework for collaborative Ô¨Åltering recommender systems, Expert Systems with Applications 38 (12) (2011) 14609‚Äì14623 . [33] J. Bobadilla, F. Ortega, A. Hernando, J. Alcal√°, Improving collaborative Ô¨Åltering recommender systems results and performance using genetic algorithms, Knowledge Based Systems 24 (8) (2011) 1310‚Äì1316 . [34] J. Bobadilla, A. Hernando, F. Ortega, A. Guti√©rrez, Collaborative Ô¨Åltering based on signiÔ¨Åcances, Information Sciences 185 (1) (2012) 1‚Äì17 . [35] J. Bobadilla, F. Ortega, A. Hernando, A collaborative Ô¨Åltering similarity measure based on singularities, Information Processing and Management 48 (2) (2012) 204‚Äì217 . [36] J. Bobadilla, F. Ortega, A. Hernando, J. Bernal, A collaborative Ô¨Åltering approach to mitigate the new user cold start problem, Knowledge Based Systems 26 (2012) 225‚Äì238 . [37] J. Bobadilla, F. Ortega, A. Hernando, J. Bernal, Generalization of recommender systems: collaborative Ô¨Åltering extended to groups of users and restricted to groups of items, Expert Systems with Applications 39 (2012) 172‚Äì186 . [38] J. Bobadilla, F. Ortega, A. Hernando, A. Arroyo, A balanced memory-based collaborative Ô¨Åltering similarity measure, International Journal of Intelligent Systems 27 (10) (2013) 939‚Äì946 . [39] T. Bogers, A. Van Den Bosch, Collaborative and content-based Ô¨Åltering for item recommendation on social bookmarking websites, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 9‚Äì16. [40] P. Bonhard, Who do trust? Combining recommender systems and social networking for better advice, in: International Conference on Intelligent User Interfaces, 2005. [41] P. Bonhard, M.A. Sasse, ‚ÄòKnowing me, knowing you‚Äô‚ÄîUsing proÔ¨Åles and social networking to improve recommender Systems, BT Technology Journal 24 (3) (2006) 84‚Äì98 . [42] P. Borzymek, M. Sydow, A. Wierbicki, Enriching trust prediction model in social network with user rating similarity, in: Proceedings of the 2009 International Conference on Computational Aspects of Social Network, 2009, pp. 40‚Äì47. [43] J.S. Breese, D. Heckerman, C. Kadie, Empirical analysis of predictive algorithms for collaborative Ô¨Åltering, in: 14th Conference on Uncertainty in ArtiÔ¨Åcial Intelligence, 1998, pp. 43‚Äì52. [44] D. Bridge, M.H. Goker, L. McGinty, B. Smyth, Case-based recommender systems, The Knowledge Engineering Review 20 (3) (2005) 315‚Äì320 . [45] R. Burke, Encyclopedia of library and information systems, in: A. Kent (Ed.), vol. 69(Suppl. 32), Marcel Dekker, 2000 (Chapter: Knowledge-Based Recommender Systems). [46] R. Burke, A case-based reasoning approach to collaborative Ô¨Åltering, in: EWCBR 2000, 2000, pp. 370‚Äì379. [47] R. Burke, Hybrid recommender systems: survey and experiments, User Modeling and User-Adapted Interaction 12 (4) (2002) 331‚Äì370 . [48] F. Cacheda, V. Carneiro, D. Fern√°ndez, V. Formoso, Comparison of collaborative Ô¨Åltering algorithms: limitations of current techniques and proposals for scalable, high-performance recommender Systems, ACM Transactions on the Web 5 (1) (2011). Article 2 . [49] S. Caizer, U. Aickelin, A recommender system based on idiotypic artiÔ¨Åcial immune networks, Journal of Mathematics, Models and Algorithms 4 (2) (2005) 181‚Äì198 . 128 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 [50] L.M. Campos, J.M. Fern√°ndez-Luna, J.F. Huete, M.A. Rueda-Morales, Combining content-based and collaborative recommendations: a hybrid approach based on Bayesian Networks, International Journal of Approximate Reasoning 51 (7) (2010) 785‚Äì799 . [51] L. Candillier, F. Meyer, M. Boull√©, Comparing state-of-the-art collaborative Ô¨Åltering systems, Lecture Notes in Computer Sciece 4571 (2007) 548‚Äì562 . [52] F. Carmagnola, F. Vernero, P. Grillo, SoNARS: a social networks-based algorithm for social recommender systems, in: Proceedings of the 17th International Conference on User Modeling, Adaptation, and Personalization: Formerly UM and AH, 2009, pp. 223‚Äì234. [53] W. Carrer-Neto, M.L. Hern√°ndez-Alcaraz, R. Valencia-Garcƒ±¬¥a, F. Garcƒ±¬¥a- S√°nchez, Social knowledge-based recommender system, Application to the movies domain. Expert Systems with Applications 39 (12) (2012) 10990‚Äì 11000 . [54] J.J. Castro-Sanchez, R. Miguel, D. Vallejo, L.M. L√≥pez-L√≥pez, A highly adaptive recommender system based on fuzzy logic for B2C e-commerce portals, Expert Systems with Applications 38 (3) (2011) 2441‚Äì2454 . [55] D. Chao, J. Balthrop, S. Forrest, Adaptive radio: achieving consensus using negative preferences, in: International ACM SIGGROUP Conference on Supporting Group Work, 2005, pp. 120‚Äì123. [56] T. Chen, L. He, Collaborative Ô¨Åltering based on demographic attribute vector, in: Proceedings of the International Conference on Future Computer and Communication, 2009, pp. 225‚Äì229. [57] P.A. Chirita, W. Nejdl, C. ZamÔ¨År, Preventing shilling attacks in online recommender systems, in: Workshop on Web Information and Data Management, 2005, pp. 67‚Äì74. [58] J. Cho, K. Kwon, Y. Park, Q-rater: a collaborative reputation system based on source credibility theory, Expert Systems with Applications 36 (2009) 3751‚Äì 3760 . [59] S.B. Cho, J.H. Hong, M.H. Park, Location-based recommendation system using Bayesian user‚Äôs preference model in mobile devices, Lecture Notes in Computer Science 4611 (2007) 1130‚Äì1139 . [60] K. Choi, D. Yoo, G. Kim, Y. Suh, A hybrid online-product recommendation system: combining implicit rating-based collaborative Ô¨Åltering and sequential pattern analysis. Electronic Commerce Research and Applications, in press, doi: 10.1016/j.elerap.2012.02.004. [61] K. Choi, Y. Suh, A new similarity fuction for selecting neighbors for each target item in collaborative Ô¨Åltering, Knowledge Based Systems 37 (2013) 146‚Äì153 . [62] C. Christakou, A. Stafylopatis, A hybrid movie recommender system based on neural networks, in: International Conference on Intelligent Systems Design and Applications, 2005, pp. 500‚Äì505. [63] I.A. Christensen, S. SchiafÔ¨Åno, Entertainment recommender systems for group of users, Expert Systems with Applications 38 (2011) 14127‚Äì14135 . [64] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, D. Netes, M. Sartin, Combining content-based and collaborative Ô¨Ålters in an online newspaper, in: Proceedings of ACM SIGIR Workshop on Recommender Systems, 1999, pp. 40‚Äì48. [65] W. Cohen, Fast effective rule induction, in: Proceedings of the Twelfth International Conference on Machine Learning, 1995, pp. 115‚Äì123. [66] M. Condliff, D. Lewis, D. Madigan, C. Posse, Bayesian mixed-effects models for recommender systems, in: ACM SIGIR ‚Äô99 Workshop on Recommender Systems: Algorithms and Evaluation, 1999, pp. 23‚Äì30. [67] E. Costa-Montenegro, A.B. Barrag√°ns-Martƒ± ¬¥ nez, M. Rey-L√≥pez, Which App? A recommender system of applications in markets: implementation of the service for monitoring users‚Äô interaction, Expert Systems with Applications 39 (10) (2012) 9367‚Äì9375 . [68] T.H. Dao, S.R. Jeong, H. Ahn, A novel recommendation model of location-based advertising: context-aware collaborative Ô¨Åltering using GA approach, Expert Systems with Applications 39 (3) (2012) 3731‚Äì3739 . [69] M. Dell‚Äôamico, L. Capra, SOFIA: social Ô¨Åltering for robust recommendation, IFIP Advances in Information and Communication Technology 263 (2008) 135‚Äì150 . [70] T. Dubois, J. Golbeck, J. Kleint, A. Srinivasan, Improving recommendation accuracy by clustering social networks with trust, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 1‚Äì8. [71] M. Ekstr√∂m, H. Bj√∂rnsson, C. Nass, A reputation mechanism for business-to- business electronic commerce that accounts for rater credibility, Journal of Organizational Computing and Electronic Commerce 15 (1) (2005) 1‚Äì18 . [72] I. Esslimani, A. Brun, A. Boyer, From social networks to behavioral networks in recommender systems, in: Proceedings of the 2009 International Conference on Advances in Social Network Analysis and Mining, 2009, pp. 143‚Äì148. [73] Y. Fahri, A Framework for Organizing JustiÔ¨Åcations for Strategic use in Adaptive Iteraction Contexts, ECIS, 2008. Article 250 . [74] A. Felfernig, R. Burke, Constraint-based recommender systems: technologies and research issues, in: 10th International Conference on Electronic Commerce, 2008 (Article No. 3). [75] L. Fengkun, J.L. Hong, Use of social network information to enhance collaborative Ô¨Åltering performance, Expert Systems with Applications 37 (7) (2010) 4772‚Äì4778 . [76] L.Q. Gao, C. Li, Hybrid personalizad recommended model based on genetic algorithm, in: International Conference on Wireless Communication, Networks and Mobile Computing, 2008, pp. 9215‚Äì9218. [77] M. Gao, Z. Wu, F. Jiang, Userrank for item-based collaborative Ô¨Åltering recommendation, Information Processing Letters 111 (9) (2011) 440‚Äì446 . [78] I. Garcƒ±¬¥ a, L. Sebastia, E. Onaindia, On the design of individual and group recommender systems for tourism, Expert Systems with Applications 38 (2011) 7683‚Äì7692 . [79] R. Garcƒ±¬¥a, X. Amatriain, Weighted content based methods for recommending connections in online social networks, in: Proceedings of the 2010 ACM conference on Recommender Systems, 2010, pp. 68‚Äì71. [80] D. Gavalas, M. Kenteris, A web-based pervasive recommendation system for mobile tourist guides, Personal and Ubiquitous Computing 15 (7) (2011) 759‚Äì770 . [81] F. Gedikli, D. Jannach, Rating items by rating tags, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 25‚Äì32. [82] J. Gemmell, T. Schimoler, B. Mobasher, R. Burke, Resource recommendation for social tagging: a multi-channel hybrid approach, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 60‚Äì67. [83] J. Gemmell, T. Schimoler, M. Ramezani, L. Christiansen, B. Mobasher, Improving FolkRank with item-based collaborative Ô¨Åltering, in: Proceedings of the 2009 ACM conference on Recommender Systems, 2009, pp. 17‚Äì24. [84] M. Gemmis, P. Lops, G. Semeraro, P. Basile, Integrating tags in a semantic content-based recommender, in: Proceedings of the 2008 ACM conference on Recommender Systems, 2008, pp. 163‚Äì170. [85] T. George, S. Meregu, A scalable collaborative Ô¨Åltering Framework base don co-clustering, in: IEEE International Conference on Data Mining (ICDM), 2005, pp. 625‚Äì628. [86] J. Golbeck, U. Kuter, The ripple effect: change in trust and its impact over a social network, in: Computing with Social Trust, Human‚ÄìComputer Interaction Series, Part II, 2009, pp. 169‚Äì181 (Chapter 7). [87] K. Goldberg, T. Roeder, D. Gupta, C. Perkins, Eigentaste: a constant time collaborative Ô¨Åltering algorithm, Information Retrieval 4 (2) (2001) 133‚Äì151 . [88] R. Gonz√°lez-Crespo, O. Sanju√°n-Martƒ±¬¥nez, J. Manuel-Cueva, B. Cristina-Pelayo, J.E. Labra-Gayo, P. Ordo√±ez, Recommendation system based on user interaction data applied to intelligent electronic books, Computers in Human Behavior 27 (4) (2011) 1445‚Äì1449 . [89] N. Good, J.B. Schafer, J.A. Konstan, A. Borchers, B. Sarwar, J.L. Herlocker, J. Riedl, in: Proceedings of the Sixteenth National Conference on ArtiÔ¨Åcial Intelligence and the Eleventh Innovative Applications of ArtiÔ¨Åcial Intelligence Conference Innovative Applications of ArtiÔ¨Åcial Intelligence, 1999, pp. 439‚Äì 446. [90] A. Gunawardana, G. Shani, A survey of accuracy evaluation metrics of recommender tasks, Journal of Machine Learning Reearch 10 (2009) 2935‚Äì 2962 . [91] J.L. Herlocker, J.A. Konstan, J. Riedl, Explaining collaborative Ô¨Åltering recommendations, in: ACM Conference on Computer Supported Cooperative Work (CSCW), 2000, pp. 241‚Äì250. [92] J.L. Herlocker, J.A. Konstan, A.L. Borchers, J.T. Riedl, An algorithmic framework for performing collaborative Ô¨Åltering, in: Proceedings of the 22nd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 1999, pp. 230‚Äì237. [93] J.L. Herlocker, J.A. Konstan, J.T. Riedl, An empirical analysis of design choices in neighborhood-based collaborative Ô¨Åltering algorithms, Information Retrieval 5 (2002) 287‚Äì310 . [94] J.L. Herlocker, J.A. Konstan, J.T. Riedl, L.G. Terveen, Evaluating collaborative Ô¨Åltering recommender systems, ACM Transactions on Information Systems 22 (1) (2004) 5‚Äì53 . [95] F. Hern√°ndez, E. Gaudioso, Evaluation of recommender systems: a new approach, Expert Systems with Applications 35 (2008) 790‚Äì804 . [96] A. Hernando, J. Bobadilla, F. Ortega, J. Tejedor, Incorporating reliability measurements into the predictions of a recommender systems. Information Sciences, in press, doi: 10.1016/j.ins.2013.03.018. [97] A. Hernando, J. Bobadilla, F. Ortega, A. Guti√©rrez, Trees for explaining recommendations made through collaborative Ô¨Åltering, Information Sciences 218 (2013) 1‚Äì16 . [98] K. Heung-Nam, E.S. Abdulmotaleb, J. Geun-Sik, Collaborative error-reÔ¨Çected models for cold-start recommender systems, Decision Support Systems 51 (3) (2011) 519‚Äì531 . [99] Y. Ho, S. Fong, Z. Yan, A hybrid ga-based collaborative Ô¨Åltering model for online recommenders, in: International Conference on e-Business, 2007, pp. 200‚Äì203. [100] L. Hossain, D. Fazio, The social networks of collaborative process, The Journal of High Technology Management Research 20 (2) (2009) 119‚Äì130 . [101] H.R. Hu, P. Pu, Using personality information in collaborative Ô¨Åltering for new users, in: Proceedings of the 2010 ACM Conference on Recommender Systems, 2010, pp. 17‚Äì24. [102] Y. Hu, Y. Koren, C.H. Volinsky, Collaborative Ô¨Åltering for implicit feedback datasets, in: IEEE International Conference on Data Mining (ICDM), 2008, pp. 263‚Äì272. [103] Y.P. Huang, W.P. Chuang, Y.H. KE, F.E. Sandnes, Using back-propagation to learn association rules for service personalization, Expert Systems with Applications 35 (2008) 245‚Äì253 . [104] Z. Huang, D. Zeng, H. Chen, A comparison of collaborative Ô¨Åltering recommendation algorithms for e-commerce, IEEE Intelligent Systems 22 (5) (2007) 68‚Äì78 . [105] N. Hurley, M. Zhang, Novelty and diversity in top-N recommendations- analysis and evaluation, ACM Transactions on Internet Technology 10 (4) (2011) 1‚Äì29 . J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 129 [106] CH.S. Hwang, Y.CH. Su, K.CH. Tseng, Using genetic algorithms for personalized recommendation, Lecture Notes in Computer Science 6422 (2010) 104‚Äì112 . [107] H. Ingoo, J.O. Kyong, H.R. Tae, The collaborative Ô¨Åltering recommendation based on SOM cluster-indexing CBR, Expert Systems with Applications 25 (2003) 413‚Äì423 . [108] A. Jameson, B. Smyth, Recommendation to groups, in: P. Brusilovsky, A. Kobsa, W. Nejdl (Eds.), The Adaptive Web, 2007, pp. 596‚Äì627 (Chapter 20). [109] D. Jannach, Fast computation of query relaxations for knowledge-based recommenders, AI Communications 22 (4) (2009) 235‚Äì248 . [110] R. J√§schke, L. Marinho, A. Hotho, L. Schmidt-Thieme, G. Stumme, Tag Recommendations in Folksonomies, in: Proceedings of the 11th European Conference on Principles and Practice of Knowledge Discovery in Databases, 2007, pp. 506‚Äì514. [111] J.J. Jason, Contextualized mobile recommendation service based on interactive social network discovered from mobile users, Expert Systems with Applications 36 (9) (2009) 11950‚Äì11956 . [112] B. Jeong, J. Lee, H. Cho, User credit based collaborative Ô¨Åltering, Expert Systems with Applications 36 (2009) 7309‚Äì7312 . [113] T. Joachims, Text categorization with support vector machines: learning with many relevant features, in: European Conference on Machine Learning, 1998, pp. 137‚Äì142. [114] A. J√∏sang, R. Ismail, C. Boyd, A survey of trust and reputation systems for online service provision, Decision Support Systems 43 (2) (2007) 618‚Äì644 . [115] CH.W. Jyun, CH.CH. Chui, Recommending trusted online auction sellers using social network analysis, Expert Systems with Applications 34 (3) (2008) 1666‚Äì1679 . [116] C. Kaleli, H. Polat, Privacy-preserving SOM-based recommendations on horizontally distributed data, Knowledge Based Systems 33 (2012) 124‚Äì135 . [117] H.N. Kim, A. Alkhaldi, A.E. Saddik, G.S. Jo, Collaborative user modeling with user-generated tags for social recommender Systems, Expert Systems with Applications 38 (7) (2011) 8488‚Äì8496 . [118] H.N. Kim, A.T. Ji, I. Ha, G.S. Jo, Collaborative Ô¨Åltering based on collaborative tagging for enhancing the quality of recommendations, Electronic Commerce Research and Applications 9 (1) (2010) 73‚Äì83 . [119] J. Kim, B. Lee, M. Shaw, H. Chang, W. Nelson, Application of decision-tree induction techniques to personalized advertisements on internet storefronts, International Journal of Electronic Commerce 5 (3) (2001) 45‚Äì62 . [120] K. Kim, H. Ahn, Using a clustering genetic algorithm to support customer segmentation for personalizad recommender systems, in: Proceedings of the 13th International Conference on AI, Simulation, and Planning in High Autonomy Systems, 2004, pp. 409-415. [121] K. Kim, H. Ahn, A recommender system using GA K-means clustering in an online Shopping market, Expert Systems with Applications 34 (2) (2008) 1200‚Äì1209 . [122] S. Kitisin, C. Neuman, Reputation-based trust-aware recommender system, in: Securecomm and Workshops, 2009, pp. 1‚Äì7. [123] F. Kong, X. Sun, S. Ye, A comparison of several algorithms for collaborative Ô¨Åltering in startup stage, IEEE Transactions on Networks, Sensing and Control (2005) 25‚Äì28 . [124] Y. Koren, R. Bell, CH. Volinsky, Matrix factorization techniques dor recommender systems, IEEE Computer 42 (8) (2009) 42‚Äì49 . [125] G. Koutrika, B. Bercovitz, H. Garcia, FlexRecs: expressing and combining Ô¨Çexible recommendations, in: Proceedings of the 35th SIGMOD International Conference on Management of Data, 2009, pp. 745‚Äì757. [126] B. Krulwich, Lifestyle Ô¨Ånder: intelligent user proÔ¨Åling using large-scale demographic data, ArtiÔ¨Åcial Intelligence Magazine 18 (2) (1997) 37‚Äì45 . [127] K. Kwon, J. Cho, Y. Park, Multidimensional credibility model for neighbor selection in collaborative recommendation, Expert Systems with Applications 36 (2009) 7114‚Äì7122 . [128] S.K. Lam, J. Riedl, Shilling recommender systems for fun and proÔ¨Åt, in: International Conference on World Wide Web, 2004, pp. 393‚Äì402. [129] X.N. Lam, T. Vu, T.D. Le, A.D. Duong, Addressing cold-start problem in recommendation systems, in: Conference On Ubiquitous Information Management And Communication, 2008, pp. 208‚Äì211. [130] N. Landia, S.S. Anand, Personalised tag recommendation, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 83‚Äì36. [131] K. Lang, NewsWeeder: learning to Ô¨Ålter netnews, in: Proceedings 12th International Conference on Machine Learning, 1995, pp. 331‚Äì339. [132] D.H. Lee, P. Brusilovsky, Does trust inÔ¨Çuence information similarity? in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 71‚Äì74. [133] M. Lee, Y. Woo, A hybrid recommender system combining collaborative Ô¨Åltering with neural network, Lecture Notes on Computer Sciences 2347 (2002) 531‚Äì534 . [134] S.K. Lee, Y.H. Cho, S.H. Kim, Collaborative Ô¨Åltering with ordinal scale-based implicit ratings for mobile music recommendations, Information Sciences 180 (11) (2010) 2142‚Äì2155 . [135] C.W. Leung, S.C. Chan, F.L. Chung, An empirical study of a cross-level association rule mining approach to cold-start recommendations, Knowledge Based Systems 21 (7) (2008) 515‚Äì529 . [136] Q. Li, Clustering approach for hybrid recommender system, in: Proceedings of the 2003 IEEE/WIC International Conference on Web Intelligence, 2003, pp. 33‚Äì38. [137] Y.M. Li, CH.W. Chen, A synthetical approach for blog recommendation: combining trust, social relation, and semantic an√°lisis, Expert Systems with Applications 36 (3) (2009) 6536‚Äì6547 . [138] Y.M. Li, CH.P. Kao, TREPPS: a trust-based recommender system for peer production services, Expert Systems with Applications 36 (2) (2009) 3263‚Äì 3277 . [139] Y.M. Li, T.F. Liao, CH.Y. Lai, A social recommender mechanism for improving knowledge sharing in online forums, Information Processing and Management, in press, doi: 10.106/j.ipm.2011.10.004. [140] S. Loh, F. Lorenzi, R. Granada, D. Lichtnow, L.K. Wives, J.P. Oliveira, Identifying similar users by their scientiÔ¨Åc publications to reduce cold start in recommender systems, in: Proceedings of the 5th International Conference on Web Information Systems and Technologies (WEBIST2009), 2009, pp. 593‚Äì600. [141] L. L√º, M. Medo, Ch.H. Yeung, Y.Ch. Zhang, Z.K. Zhang, T. Zhou, Recommender systems, Physics Reports 519 (2012) 1‚Äì49 . [142] X. Luo, Y. Xia, Q. Zhu, Incremental collaborative Ô¨Åltering recommender based on regularizad matrix factorization, Knowledge-Based Systems 27 (2012) 271‚Äì280 . [143] X. Luo, Y. Xia, Q. Zhu, Applying the learning rate adaptation to the matrix factorization based collaborative Ô¨Åltering, Knowledge Based Systems 37 (2013) 154‚Äì164 . [144] H. Ma, I. King, M.R. Lyu, Learning to recommend with explicit and implicit social relations, ACM Transactions on Intelligent Systems and Technology 2 (3) (2011). Article 29 . [145] H. Ma, T. CH. Zhou, M.R. Lyu, I. King, Improving recommender systems by incorporating social contextual information, ACM Transactions on Information Systems 29 (2) (2011). Article 9 . [146] A. Machanavajjhala, A. Korolova, A.D. Sharma, Personalized social recommendations: accurate or private, in: Proceedings of the VLDB Endowment, vol. 4, issue 7, 2011, pp. 440‚Äì450. [147] L.B. Marinho, L. Schmidt-Thieme, Collaborative tag recommendations, in: Proceedings of the 31st Annual Conference of the German ClassiÔ¨Åcation Society, 2008, pp. 533‚Äì540. [148] L. Martinez, L.G. Perez, M.J. Barranco, Incomplete preference relations to smooth out the cold-start in collaborative recommender systems, in: Proceedings of the 28th North American Fuzzy Information Processing Society Annual Conference (NAFIPS2009), 2009a, pp. 1‚Äì6. [149] L. Martinez, R.M. Rodriguez, M. Espinilla, REJA: a georeferenced hybrid recommender system for restaurants, in: IEEE/WIC/ACM International Joint Conference on Web Intelligence and Intelligent Agent Technonolgy (WI-IAT 3), 2009b, pp. 187‚Äì190. [150] P. Massa, P. Avesani, Trust-aware collaborative Ô¨Åltering for recommender systems, Lecture Notes in Computer Science 3290 (2004) 492‚Äì508 . [151] P. Massa, P. Avesani, Trust-aware recommender Systems, in: Proceedings of the 2007 ACM conference on Recommender Systems, 2007, pp. 17‚Äì24. [152] C. Matyas, C. Schlieder, A spatial user similarity measure for geographic recommender systems, in: Proceedings of the 3rd International Conference on GeoSpatial Semantics, 2009, pp. 122‚Äì139. [153] K. Mccarthy, J. Reilly, L. Mcginty, B. Smyth, Thinking positively-explanatory feedback for conversational recommender systems, in: European Conference on Case-based reasoning (ECCBR), 2004, pp. 115‚Äì124. [154] K. Mcnally, M.P. O‚Äômahony, M. Coyle, P. Briggs, B. Smyth, A case study of collaboration and reputation in social web search, ACM Transactions on Intelligent Systems and Technology 3 (1) (2011). Article 4 . [155] D. Mcsherry, Explanation in recommender systems, ArtiÔ¨Åcial Intelligence Review 24 (2) (2005) 179‚Äì197 . [156] F. Mcsherry, I. Mironov, Differentially Private recommender systems: building privacy into the netÔ¨Çix prize contenders, in: Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2009, pp. 627‚Äì636. [157] P. Melville, R.J. Mooney, R. Nagarajan, Content-boosted collaborative Ô¨Åltering for improved recommendations, in: Proceeding Eighteenth National Conference on ArtiÔ¨Åcial Intelligence, 2002, pp. 187‚Äì192. [158] R. Meteren, M. Someren, Using content-based Ô¨Åltering for recommendation, in: Proceedings of ECML 2000 Workshop: Maching Learning in Information Age, 2000, pp. 47‚Äì56. [159] S.E. Middleton, N.R. Shadbolt, D.C. De Roure, Ontological user proÔ¨Åling in recommender systems, ACM Transactions on Information Systems (TOIS) 22 (1) (2004) 54‚Äì88 . [160] R.J. Mooney, L. Roy, Content-based book recommending using learning for text categorization, in: Proceedings of the Fifth ACM Conference on Digital Libraries, 2000, pp. 195‚Äì204. [161] T. Morrison, U. Aickelin, An artiÔ¨Åcial immune system as a recommender for Web sites, in: International Conference on ArtiÔ¨Åcial Immune Systems, 2002, pp. 161‚Äì169. [162] A. Nanolopoulus, D. Rafailidis, P. Symeonidis, Y. Manolopoulus, Music Box: personalizad music recommendation based on cubic analysis of social tags, IEEE Transactions on Audio, Speech and Language Processing 18 (2) (2010) 407‚Äì412 . [163] K. Nehring, C. Puppe, A theory of diversity, Econometrica 70 (3) (2002) 1155‚Äì 1198 . [164] E.R. N√∫√±ez-Vald√©z, J.M. Cueva-Lovelle, O. Sanju√°n-Martƒ±¬¥nez, V. Garcƒ± ¬¥ a-Dƒ±¬¥az, P. Ordo√±ez, C.E. Montenegro-Marƒ±¬¥ n, Implicit feedback techniques on recommender systems applied to electronic books, Computers in Human Behavior 28 (4) (2012) 1186‚Äì1193 . 130 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 [165] J. O‚Äôdonovan, Capturing trust in social web applications, in: J. Golbeck (Ed.), Computing with Social Trust, 2009, pp. 213‚Äì257. [166] J. O‚Äôdonovan, B. Smyth, Trust in recommender systems, in: International Conference on Intelligent user Interfaces, 2005, pp. 167‚Äì174. [167] K. Oku, R. Kotera, K. Sumiya, Geographical recommender system based on interaction between map operation and category selection, in: Workshop on Information Heterogeneity and Fusion in Recommender Systems, 2010, pp. 71‚Äì74. [168] J. Ortega, J. Bobadilla, A. Hernando, A. Guti√©rrez, Incorporating group recommendations to recommender systems: alternatives and performance, Information Processing and Management (2013), http://dx.doi.org/10.1016/ j.ipm.2013.02.003 . [169] J. Ortega, J.L. S√°nchez, J. Bobadilla, A. Guti√©rrez, Improving collaborative Ô¨Åltering-based recommender systems results using Pareto dominance, Information Sciences (2013), http://dx.doi.org/10.1016/j.ins.2013.03.011 . [170] A. Papadimitriou, P. Symeonidid, Y. Manolopoulus, A generalized taxonomy of explanations styles for traditional and social recommender systems, Data Minning Knowledge Discovery 24 (3) (2012) 555‚Äì583 . [171] D.H. Park, H.K. Kim, I.Y. Choi, J.K. Kim, A literature review and classiÔ¨Åcation of recommender Systems research, Expert Systems with Applications 39 (2012) 10059‚Äì10072 . [172] S.T. Park, W. Chu, Pairwise preference regression for cold-start recommendation, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 21‚Äì28. [173] S.T. Park, D.M. Pennock, O. Madani, N. Good, D. Coste, Naƒ±¬®ve Ô¨Ålterbots for robust cold-start recommendations, in: Proceedings of Knowledge Discovery and Data Mining (KDD2006), 2006, pp. 699‚Äì705. [174] Y.J. Park, A. Tuzhilin, The long tail of recommender systems and how to leverage it, in: Proceedings of the 2008 ACM Conference on Recommender Systems, 2008, pp. 11‚Äì18. [175] M. Pazzani, D. Billsus, Learning and revising user proÔ¨Åles: the identiÔ¨Åcation of interesting web sites, Machine Learning 27 (3) (1997) 313‚Äì331 . [176] M.J. Pazzani, D. Billsus, Content-based recommender systems, in: P. Brusilovsky, A. Kobsa, W. Nejdl (Eds.), The Adaptive Web, 2007, pp. 291‚Äì 324 (Chapter 10). [177] M. Pazzani, A framework for collaborative, content-based, and demographic Ô¨Åltering, ArtiÔ¨Åcial Intelligence Review-Special Issue on Data Mining on the Internet 13 (5-6) (1999) 393‚Äì408 . [178] S. Perugini, M.A. Gon√ßalves, E.A. Fox, Recomender systems research: a connection-centric Surrey, Journal of Intelligen Information Systems 23 (2) (2004) 107‚Äì143 . [179] M.C. Pham, Y. Cao, R. Klamma, M. Jarke, A clustering approach for collaborative Ô¨Åltering recommendation using social network analysis, Journal of Universal Computer Science 17 (4) (2011) 583‚Äì604 . [180] G. Pitsilis, S.J. Knapskog, Socila trust as a solution to address sparsity-inherent problems of recommender Systems, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 33‚Äì40. [181] G. Pitsilis, X. Zhang, W. Wang, Clustering recommenders in collaborative Ô¨Åltering using explicit trust information, Advances in Information and Communication Technology 358 (2011) 82‚Äì97 . [182] A. Popescul, L.H. Ungar, D.M. Pennock, S. Lawrence, Probabilistic models for uniÔ¨Åed collaborative and content-based recommendation in sparse-data environments, in: Proceeding UAI ‚Äô01 Proceedings of the 17th Conference in Uncertainty in ArtiÔ¨Åcial Intelligence, 2001, pp. 437‚Äì444. [183] C. Porcel, E. Herrera-Viedma, Dealing with incomplete information in a fuzzy linguistic recommender system to disseminate information in university digital libraries, Knowledge-Based Systems 23 (1) (2010) 32‚Äì39 . [184] C. Porcel, J.M. Moreno, E. Herrera-Viedma, A multi-disciplinar recommender system to advice research resources in university digital libraries, Expert Systems with Applications 36 (10) (2009) 12520‚Äì12528 . [185] C. Porcel, A. Tejeda-Lorente, M.A. Martƒ± ¬¥ nez, E. Herrera-Viedma, A hybrid recommender system for the selective dissemination of research resources in a technology transfer ofÔ¨Åce, Information Sciences 184 (1) (2012) 1‚Äì19 . [186] J. Preece, B. Shneiderman, The reader to leader framework: motivating technology-mediated social participation, AIS Transactions on Human‚Äì Computer Interaction 1 (1) (2009) 13‚Äì32 . [187] P. Pu, L. Chen, Trust-inspiring explanation interfaces for recommender systems, Knowledge Based Systems 20 (2007) 542‚Äì556 . [188] W. Qin, L. Xin, H. Liang, Unifying user-based and item-based algorithm to improve collaborative Ô¨Åltering accuracy, Energy Procedia 13 (2011) 8231‚Äì 8239 . [189] L. Ramaswamy, P. Deepak, R. Polavarapu, K. Gunasekera, D. Garg, K. Visweswariah, S. Kalyanaraman, CAESAR: a context-aware, social recommender system for low-end mobile devices, in: International Conference on Mobile Data Management: Systems, Services and Middleware, 2009, pp. 338‚Äì347. [190] A.M. Rashid, G. Karypis, J. Riedl, Learning preferences of new users in recommender systems: an information theoretic approach, in: ACM SIGKDD Explorations Newsletter, vol. 10, issue 2, 2008, pp. 90‚Äì100. [191] S. Ray, A. Mahanti, Strategies for effective shilling attacks against recommender systems, Lecture Notes in Computer Science 5456 (2009) 111‚Äì125 . [192] L. Ren, L. HE, J. Gu, W. Xia, F. Wu, A hybrid recommender approach based on Widrow‚ÄìHoff learning, in: International Conference on Future Generation Communication and Networking, 2008, pp. 40‚Äì45. [193] T.H. Roh, K.J. Oh, I. Han, The collaborative Ô¨Åltering recommendation based on SOM cluster-indexing CBR, Expert Systems with Applications 25 (2003) 413‚Äì 423 . [194] J.A. Rodrigues, L.F. Cardoso, J. Moreira, G. Xexeo, Bringing knowledge into recommender systems, The Journal of Systems and Software, in press, http:// dx.doi.org/10.1016/j.jss.2012.10.002 . [195] S.B. Roy, S. Amer-Yahia, A. Chala, G. Das, C. Yu, Space efÔ¨Åciency in group recommendation, The International Journal on Very Large Data Bases 19 (6) (2010) 877‚Äì900 . [196] G. Ruffo, R. Schifanella, A peer-to-peer recommender system base don spontaneous afÔ¨Ånities, ACM Transactions on Internet Technology 9 (1) (2009) 1‚Äì34 . [197] P.B. Ryan, D. Bridge, Collaborative recommending using formal concept analysis, Knowledge Based Systems 19 (5) (2006) 309‚Äì315 . [198] G. Salton, Automatic Text Processing: The Transformation, Analysis, and Retrieval of Information by Computer, Addison-Wesley, Reading, MA, 1989 . [199] M. Saranya, T. Atsuhiro, Hybrid recommender systems using latent features, in: Proceedings of the International Conference on Advanced Information Networking and Applications Workshops, 2009, pp. 661‚Äì666. [200] B. Sarwar, G. Karypis, J.A. Konstan, J. Riedl, Item-based collaborative Ô¨Åltering recommendation algorithms, in: 10th International Conference on World Wide Web, 2001, pp. 285‚Äì295. [201] B. Sarwar, G. Karypis, J. Konstan, J. Riedl, Analysis of recommendation algorithms for e-commerce, in: ACM Conference on Electronic Commerce, 2000a, pp. 158‚Äì167. [202] B. Sarwar, G. Karypis, J. Konstan, J. Riedl, Application of dimensionality reduction in recommender system ‚Äì a case study, in: ACM WebKDD Workshop, 2000b, pp. 264‚Äì272. [203] J.B. Schafer, D. Frankowski, J. Herlocker, S. Sen, Collaborative Ô¨Ålltering recommender systems, in: P. Brusilovsky, A. Kobsa, W. Nejdl (Eds.), The Adaptive Web, 2007, pp. 291‚Äì324 (Chapter 9). [204] A.I. Schein, A. Popescul, L.H. Ungar, D.M. Pennock, Methods and metrics for cold-start recommendations, in: Proceeding SIGIR ‚Äô02 Proceedings of the 25th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, 2002, pp. 253‚Äì260. [205] C. Schlieder, Modeling collaborative semantics with a geographic recommender, in: Workshop on Semantic and Conceptual Issues in Geographic Information Systems, 2007, pp. 336‚Äì347. [206] J. Serrano-Guerrero, E. Herrera-Viedma, J.A. Olivas, A. Cerezo, F.P. Romero, A google wave-based fuzzy recommender system to disseminate information in University Digital Libraries 2.0., Information Sciences 181 (9) (2011) 1503‚Äì 1516 . [207] Z. Severac, V. Devedzic, J. Jovanovic, Adaptive neuro-fuzzy pedagogical recommender, Expert Systems with Applications 39 (10) (2012) 9797‚Äì9806 . [208] A. Shepitsen, J. Gemmell, B. Mobasher, R. Burke, Personalized recommendation in social tagging systems using hierarchical clustering, in: Proceedings of the 2008 ACM Conference on Recommender Systems, 2008, pp. 259‚Äì266. [209] S.K. Shinde, U. Kulkami, Hybrid personalizad recommender system using centering‚Äìbunching based clustering algorithm, Expert Systems with Applications 39 (1) (2012) 1381‚Äì1387 . [210] S. Siersdorfer, S. Sergei, Social recommender systems for web 2.0 folksonomies, in: 20th ACM conference on Hypertext and hipermedia, 2009, pp. 261‚Äì269. [211] I. Soboroff, C. Nicholas, Combining content and collaboration in text Ô¨Åltering, in: Proceedings of the IJCAI‚Äô99 Workshop on Machine Learning for Information Filtering, 1999, pp. 86‚Äì91. [212] X. Su, T.M. Khoshgoftaar, A survey of collaborative Ô¨Åltering techniques, Advance in ArtiÔ¨Åcial Intelligence 2009 (2009) 1‚Äì19 . [213] P. Symeonidis, A. Nanopoulus, Y. Manolopoulus, Providing justiÔ¨Åcations in recommender systems, IEEE Transactions on Systems, Man and Cybernet 38 (6) (2008) 1262‚Äì1272 . [214] P. Symeonidis, A. Nanopoulus, Y. Manolopoulus, MovieExplain: a recommender system with explanations, in: Proceedings of the 2009 ACM Conference on Recommender Systems, 2009, pp. 317‚Äì320. [215] G. Tak√°cs, I. Pil√°szy, B. N√©meth, D. Tikk, Scalable collaborative Ô¨Åltering approaches for large recommender systems, Journal of Machine Learning Research 10 (2009) 623‚Äì656 . [216] S. Tan, J. Bu, CH. Chen, X. He, Using rich social media information for music recommendation via hypergraph model, ACM Transactions on Multimedia Computing, Communications, and Applications 7 (1) (2011). Article 7 . [217] N. Tintarev, J. Masthoff, A survey of explanations in recommender systems, in: IEEE 23rd International Conference on Data Engineering Workshop, 2007, 801‚Äì810. [218] T. Tran, R. Cohen, Hybrid recommender systems for electronic commerce, in: Proceedings of the 17th National Conference on ArtiÔ¨Åcial Intelligence, AAAI, 2000, pp. 78‚Äì84. [219] K.H.L. Tso-Sutter, L.B. Marinho, L. Schmidt-Thieme, Tag-aware recommender systems by fusion of collaborative Ô¨Åltering algorithms, in: Proceedings of the 2008 ACM Symposium on Applied Computing, 2008, pp. 1995‚Äì1999. [220] S. Vargas, P. Castells, Rank and relevance in novelty and diversity metrics for recommender systems, in: Proceedings of the 2011 ACM Conference on Recommender Systems, 2011, pp. 109‚Äì116. [221] P. Victor, CH. Cornelis, M. De-Cock, Trust Networks for Recommender Systems, Antalis Press, 2011 . J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132 131 [222] J. Vig, S. Sen, J. Riedle, Tagsplanations: Explaining recommendations using tags, Proceedings of the 13th international conference on Intelligent user interfaces, 2009, pp. 47-56. [223] P. Victor, CH. Cornelis, M. De-Cock, P.P. DA-SILVA, Gradual tust and distrust in recommender systems, Fuzzy Sets and Systems 160 (10) (2009) 1367‚Äì1382 . [224] M.G. Vozalis, K.G. Margaritis, Using SVD and demographic data for the enhancement of generalized collaborative Ô¨Åltering, Information Sciences 177 (2007) 3017‚Äì3037 . [225] Y. Wan-Shiou, CH. Hung-Chi, D. Jia-Ben, A location-aware recommender system for mobile shopping environments, Expert Systems with Applications 34 (1) (2008) 437‚Äì445 . [226] J. Wang, A. Vries, M. Reinders, Unifying user-based and item-based collaborative Ô¨Åltering approaches by similarity fusion, in: Proc. SIGIR Conf., 2006, pp. 501‚Äì508. [227] J. Wang, A.P. Vries, M.J. Reinders, UniÔ¨Åed relevance models for rating prediction in collaborative Ô¨Åltering, ACM Transactions in Information Systems 26 (3) (2008) 1‚Äì42 . [228] L.T. Weng, Y. Xu, Y. Li, R. Nayak, Exploiting item taxonomy for solving cold- start problem in recommendation making, in: Proceedings of the 20th IEEE International Conference on Tools with ArtiÔ¨Åcial Intelligence (ICTAI2008), 2008, pp. 113‚Äì120. [229] B. Widrow, M.E. Hoff, Adaptive switching circuits, in: Convention Record, IRE WESCON, 1960, pp. 96‚Äì104. [230] P. Winoto, T.Y. Tang, The role of user mood in movie recommendations, Expert Systems with Applications 37 (8) (2010) 6086‚Äì6092 . [231] W. Woerndl, G. Groh, Utilizing physical and social context to improve recommender systems, in: IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology, 2007, pp. 123‚Äì128. [232] B. Xie, P. Han, F. Yang, R.M. Shen, H.J. Zeng, Z. Chen, DCFLA: a distributed collaborative-Ô¨Åltering neighbor-locating algorithm, Information Sciences 177 (6) (2007) 1349‚Äì1363 . [233] W. Xin, Q. Jamaliding, T. Okamoto, Discovering social network to improve recommender system for group learning support, in: International Conference on Computational Intelligence and Software Engineering, 2009, pp. 1‚Äì4. [234] R.R. Yager, Fuzzy logic methods in recommender systems, Fuzzy Sets and Systems 136 (2) (2003) 133‚Äì149 . [235] W.S. Yang, H.CH. Cheng, J.B. Dia, A location-aware recommender system for mobile shopping environments, Expert Systems With Applications 34 (1) (2008) 437‚Äì445 . [236] Y. Yang, An evaluation of statistical approaches to text categorization, Information Retrieval 1 (1) (1999) 67‚Äì88 . [237] Z. Yao, Q. Zhang, Item-based clustering collaborative Ô¨Åltering algorithm under high dimensional sparse data, in: International Joint Confeence on Computational Sciences and Optimization, 2009, pp. 787‚Äì790. [238] Z. Yu, X. Zhou, Y. Hao, J. Gu, TV program recommendation for multiple viewers based on user proÔ¨Åle merging, User Modeling and User-Adapted Interaction 16 (1) (2006) 63‚Äì82 . [239] W. Yuan, D. Guan, Y.K. Lee, S. Lee, S.J. Hur, Improved trust-aware recommender system using small-worldness of trust networks, Knowledge Based Systems 23 (3) (2010) 232‚Äì238 . [240] G. Zacharia, A. Moukas, P. Maes, Collaborative reputation mechanisms for electronic marketplaces, Decision Support Systems 29 (2000) 371‚Äì388 . [241] O. Zaiane, Building a recommender agent for e-learning systems, in: Proceedings of the International Conference on Computers Education (ICCE‚Äô02), vol. 1, 2002, pp. 55‚Äì59. [242] J. Zhan, Privacy-preserving collaborative recommender systems, IEEE Transactions on Systems, Man and Cybernetics 40 (4) (2010) 472‚Äì476 . [243] F. Zhang, H.Y. Chang, A collaborative Ô¨Åltering algorithm employing genetic clustering to ameliorate the scalability issue, in: IEEE International Conference on e-Business Engineering, 2006, pp. 331‚Äì338. [244] S. Zhang, W. Wang, J. Ford, F. Makedon, Using singular value decomposition approximation for collaborative Ô¨Åltering, in: IEEE International Conference on E-Commerce Technology, 2005, pp. 1‚Äì8. [245] L. Zhen, G.Q. Huang, Z. Jiang, Recommender systems based on workÔ¨Çow, Decision Support Systems 48 (2009) 237‚Äì245 . [246] L. Zhen, G.Q. Huang, Z. Jiang, Collaborative Ô¨Åltering based on workÔ¨Çow space, Expert Systems with Applications 36 (2009) 7873‚Äì7881 . [247] L. Zhen, Z. Jiang, H. Song, Distributed recommender for peer-to-peer knowledge sharing, Information Sciences 210 (2010) 3546‚Äì3561 . [248] N. Zheng, Q. Li, A recommender system based on tag and time information for social tagging systems, Expert Systems with Applications 38 (4) (2011) 4575‚Äì4587 . [249] Y. Zheng, X. Xie, Learning travel recommendations from user-generated GPS traces, ACM Transactions on Intelligent Systems and Technology 2 (2011) 1. Article 2 . [250] Y. Zheng, L. Zhang, Z. Ma, X. Xie, W.Y. Ma, Recommending friends and locations based on individual location history, ACM Transactions on the Web 5 (2011) 1. Article 5 . [251] J. Zhong, X. Li, UniÔ¨Åed collaborative Ô¨Åltering model based on combination of latent features, Expert Systems with Applications 37 (2010) 5666‚Äì5672 . [252] R.L. Zhu, S.J. Gong, Analyzing of collaborative Ô¨Åltering using clustering technology, international colloquium on computing, in: ISECS International Colloquium on Computing, Communication, Control, and Management, 2009, pp. 57‚Äì59. [253] C.N. Ziegler, S.M. Mcnee, J.A. Konstan, G. Lausen, Improving recommendation lists through topic diversiÔ¨Åcation, in: Proceedings of the 14th International Conference on World Wide Web, 2005, pp. 22‚Äì32. 132 J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109‚Äì132"
Creating synthetic datasets for collaborative filtering recommender systems using generative adversarial networks.pdf,Knowledge-Based Systems | Creating synthetic datasets for collaborative filtering recommender systems | using generative adversarial networks,Recommender systems,"Knowledge-Based Systems 280 (2023) 111016 Available online 23 September 2023 0950-7051/¬© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ). Contents lists available at ScienceDirect Knowledge-Based Systems journal homepage: www.elsevier.com/locate/knosys Creating synthetic datasets for collaborative filtering recommender systems using generative adversarial networks Jes√∫s Bobadilla a , Abraham Guti√©rrez a , Raciel Yera b , Luis Mart√≠nez b , ‚àó a Departamento de Sistemas Inform√°ticos, ETSI Sistemas Inform√°ticos, Universidad Polit√©cnica de Madrid, C/ Alan Turing s/n, 28031, Madrid, Spain b Departamento de Inform√°tica, Universidad of Ja√©n, Ja√©n, Spain A R T I C L E I N F O Keywords: Recommender systems Generative adversarial networks Deep learning Collaborative filtering A B S T R A C T Research and education in machine learning requires diverse, representative, and open datasets that contain sufficient samples to handle the necessary training, validation, and testing tasks. Currently, the Recommender Systems area includes a large number of subfields in which accuracy and beyond-accuracy quality measures are continuously being improved. To feed this research variety, it is both necessary and convenient to reinforce the existing datasets with synthetic ones. This paper proposes a Generative Adversarial Network (GAN)-based method to generate collaborative filtering datasets in a parameterized way by selecting their preferred number of users, items, samples, and stochastic variability. This parameterization cannot be performed using regular GANs. Our GAN model is fed with dense, short, and continuous embedding representations of items and users, instead of sparse, large, and discrete vectors, to ensure fast and accurate learning, as compared to the traditional approach based on large and sparse input vectors. The proposed architecture includes a DeepMF model to extract the dense user and item embeddings and a clustering process to convert the dense GAN generated samples to the discrete and sparse samples necessary to create each required synthetic dataset. The results from three different source datasets show adequate distributions and expected quality values and evolutions in the generated datasets compared to the source datasets. Synthetic datasets and source codes are available to researchers. 1. Introduction Recommender systems (RS) are a relevant area in artificial intel- ligence due to the growing popularity of social networks. The big companies that extensively use RSs are TripAdvisor, Netflix, Spotify, YouTube Music, TikTok, YouTube and Amazon [ 1 ]. These companies make use of the RS models to recommend to users similar items (music, videos, trips, news) to those that they have already consumed; some other companies, such as Facebook, work hard to collect customer activity to provide personalized advertising rather than personalized products or services. RSs are usually classified according to their filter- ing approach [ 2 ]; content-based RSs select the recommended items by looking for similar content [ 3 ]; since most item contents is text, natural language processing models are used. Reviews [ 4 ] and tweets [ 5 ] are two common types of content-based filtered data. Product images can also be processed to make recommendations; convolutional neural networks are the most commonly used models to perform this task [ 6 ]. Social filtering has been extensively used to improve social-based rec- ommendations. This type of filtering uses data such as tags, followers, ‚àó Corresponding author. E-mail addresses: jesus.bobadilla@upm.es (J. Bobadilla), abraham.gutierrez@upm.es (A. Guti√©rrez), ryera@ujaen.es (R. Yera), martin@ujaen.es (L. Mart√≠nez). and being followed, and makes use of the concepts of reputation and trust [ 7 ]. Geographic information, such as GPS coordinates and POI, is mainly used to support context-aware filtering [ 8 ]. Demographic filtering (age, gender, country, etc.) is commonly combined with other types of filtering, implementing recommendation ensembles [ 9 ]. Be- yond the previous filtering strategies, collaborative filtering (CF) [ 10 ] is the most important approach for implementing RSs, since it provides superior accuracy, particularly when combined with some other types of filtering. Effective RS research makes use of innovative models, adequate quality measures, and representative datasets. The historical evolution of CF begins with the use of memory-based models, mainly the K-Nearest Neighbors algorithm [ 11 ]. Memory- based approaches were replaced by model-based machine learning approaches due to their overall performance: they are superior in accu- racy of results, also in time to obtain predictions (once the model has learned); and their output is capable of being explained through post- hoc techniques [ 12 ]. Matrix Factorization (MF) [ 13 ] is the most widely used machine learning model to implement collaborative filtering; it performs a dimensional reduction of users and items, capturing the https://doi.org/10.1016/j.knosys.2023.111016 Received 17 December 2022; Received in revised form 12 September 2023; Accepted 15 September 2023 Knowledge-Based Systems 280 (2023) 111016 2 J. Bobadilla et al. main patterns that relate them to the votes cast. Additionally, by using Non-Negative Matrix Factorization (NFM) [ 14 ], semantic meanings can be assigned to latent factors. Bayesian NMF [ 15 ] allows clustering users and making predictions simultaneously, which opens the door to effective recommendations to user groups and social clustering appli- cations [ 16 ]. Nowadays, CF research is mainly developed by deep learning models, where DeepMF [ 17 ] is the basis for modern approaches . DeepMF is the model that we use in this paper, in which users are coded in a latent space by means of an embedding layer, whereas items are coded in a different latent space by means of a second embedding layer; finally, predictions are made by making the dot product of both, item and user embeddings. DeepMF improves MF due to the inherent competence of neural networks to capture the non-linear relations hips between samples. Neural Collaborative Filtering (NCF) [ 18 ] is extensively used to implement CF; this model replaces the DeepMF dot layer with a Multi-Layer Perceptron (MLP) and outperforms DeepMF when applied to large and complex datasets. Beyond accuracy, deep learning models are emerging to perform some innovative tasks, such as improving fairness, where the DeepFair model [ 19 ] achieves a trade-off between equity and precision; green computing [ 20 ]; results explanation via latent space visualization [ 21 ] and efficient neighborhood identifica- tion [ 22 ]. The adversarial network-based recommendation has recently been introduced in the RS area [ 23 ] and we will focus on it in the ‚ÄôRelated work‚Äô section. Generative Adversarial Networks (GAN) [ 24 ] are responsible for the popular fake faces and fake videos that flood social networks. Their architecture has two separate neural networks that compete against each other (‚Äôadversarial‚Äô), such as an art forger competing against an art expert, ensuring that both improve their work. The GAN ‚Äòforger‚Äô is a generator model that creates fake samples from random noise vectors, while the GAN ‚Äòexpert‚Äô is a discriminator model implemented as a simple binary classifier: fake, non-fake. However, while RS research is mainly focused on proposing novel recommendation models, this paper tries to make progress in CF datasets . In this respect, it is essential to identify quality measures as a key element to carry out adequate research, since they allow the baselines of the state of the art to be compared with the proposed algorithms, methods and models. Beyond the usual prediction and recommendation quality measures (MAE, MSD, precision, recall, F1, NDCG, etc.), some other measures, such as novelty and diversity [ 25 ], have recently acquired growing importance. Of these, diversity is cur- rently the main focus of researchers‚Äô attention, due to the risks of inappropriate recommendations in social networks, such as those that exhibit a lack of variability and promote prefixed ideas and behaviors. Diversity and reliability in RS have been improved by introducing diversity-enhancing constraints in the MF model [ 26 ]; additionally, a deep learning classification model [ 27 ] is proposed to obtain the recommendation reliability values from the softmax output layer of the neural network. Quality values are obtained when a model or method is tested on balanced CF datasets. To obtain balanced training and testing sets, with respect to their user and item distributions, deterministic strategies are proposed in [ 28 ]. Most of the RS research makes use of popular CF datasets such as MovieLens, FilmTrust, MyAnimeList or CiteSeer; CF datasets include different domains such as music, movies, POIs, tourism, news, research papers, tagged data, etc. Some of these datasets have been filled with explicit votes from users, while others contain implicit interactions between users and systems. There are also datasets filled with crawled Web pages or academic PDFs [ 29 ] and some others are enriched with social tags that researchers add to the ar- ticles [ 30 ]. A selection of relevant social CF datasets is provided in [ 31 ] and related to some articles using them. Recently, an educational news dataset [ 32 ] was released; which included contextualized information: time and location. Finally, an RS dataset has also been provided that contains artificial intelligence research data [ 33 ] to obtain segmented information, clustering, and geographical locations. Beyond these works, it is particularly relevant that parameterized synthetic datasets have not yet been used, so consequently the CF research does not benefit from the flexibility that parameterization provides in the experiment design: different dataset sizes, number of users and items, and so on. This paper aims to fill the gap by proposing a procedure, coined as GANRS, which focuses on the use of GANs to generate collaborative filtering recommender systems datasets in a parameterized way. Please note that current RS GAN-based models cannot simultaneously set the number of generated users, items, and rating distributions. Regarding our contribution, two main overall approaches can be identified in the state-of-art: statistical and generative. The main ad- vantage of the statistical approach is that several relevant parameters can be simultaneously set: number of users, number of items, dataset size, etc. The main drawback of this approach is its poor accuracy. On the other hand, current model-based generative approaches improve accuracy compared to statistical frameworks, but they lack flexibility, since parameterization is very limited. In fact, current GAN designs are focused on user profiles, and they can generate as many new fake users as required, but other relevant parameters cannot be set, such as the number of items that is fixed in the source set of user vectors and then, also, in the fake generated set of user vectors. This can be explained with an example: when we run a regular GAN to generate fake images, the synthetic images have the same shape (resolution and number of channels) as the source images. In the RS field, the synthetic user vectors contain the same number of items as the real user vectors. Following the example, there are some specific GAN designs that return ‚Äòsuper-resolution‚Äô images (they can increase resolution), but to our knowledge there are no RS GANs designed to generate fake users containing more items (or fewer items). Our proposed method is designed to simultaneously set some CF relevant parameters, such as the number of users and items. The rest of the paper has been structured as follows: related work is introduced in Section 2 , focusing on the most recent uses of the GAN models applied to RS. Section 3 explains the proposed model and its formalization. Section 4 presents the design, result, and discussion of the experiments. Finally, Section 5 contains the main conclusions of the article and discusses future work. 2. Background 2.1. Basics on generative adversarial networks GANs are designed to generate data from scratch [ 24 ]. They have been commonly used to create fake images, although their use has been spreading to many other domains: music, medicine, financial data, etc. The GAN architecture composes of two deep network models: generator and discriminator. The generator model learns to create samples as similar as possible to those in a dataset (e.g., a dataset with human faces images), whereas the discriminator model learns to detect fake samples (those samples created by the generator). To better understand the GANs we can consider the example of a painting forger and a forgery expert: the more imitations the forger paints, the better their results, and the better the expert‚Äôs ability to detect fake paintings. Both people successively improve their abilities. When the learning begins, the GAN discriminator (the forgery expert, in our example) has an easy job, since the generator does not have the painting patterns. After thousands of learning epochs, the generator has learnt the patterns well enough to confuse the discriminator, who is forced to tune their weights. If the learning loop iterates enough times, both the generator and the discriminator models are well designed and the painting in the dataset contains suitable patterns, the generator will be able to create synthetic (fake) samples that are difficult to distinguish from the originals. Fig. 1 shows the GAN architecture [ 24 ]; the discriminator model makes a binary classification between fake and real samples. The generator model updates its weights (learns) when the discriminator correctly classifies a fake sample. The discriminator model updates its weights when it incorrectly classifies a sample. Note that the generator takes a random noise distribution as input to generate samples; then, Knowledge-Based Systems 280 (2023) 111016 3 J. Bobadilla et al. Fig. 1. Generative Adversarial Networks architecture. once it has learnt, for each input random noise vector that feeds the generator a sample is created with the patterns of the dataset samples. For this reason, by providing random noise vectors we can create as many samples as required, which means that, in our context, we can create fake CF datasets of any size by creating fake profiles. To measure GAN loss, we use cross-entropy. The discriminator ( ùê∑ ) loss can be expressed as the sum of the expectations: ùëöùëéùë• ùê∑ ùëâ ( ùê∑ ) = ùê∏ ùë• ‚àº ùëù ùëëùëéùë°ùëé ( ùë• ) [ ùëôùëúùëîùê∑ ( ùë• )] + ùê∏ ùëß ‚àº ùëù ùëß ( ùëß ) [ ùëôùëúùëî (1 ‚àí ùê∑ ( ùê∫ ( ùëß )))] (1) Where the first term of the equation is used to recognize real images, and the second term recognizes generated images. ùëç represents the noisy vector, ùê∫ is the generator, and ùê∫ ( ùëç ) is the generated sample. ùê∑ ( ùê∫ ( ùëß )) is the classification result of the discriminator when its input is a fake sample. ùê∑ ( ùë• ) is the classification result of the discriminator when its input is a real sample. The generator loss is designed to learn when the discriminator correctly classifies (the true label is 1, and the fake label is 0). Its equation is: ùëöùëñùëõ ùê∫ ùëâ ( ùê∫ ) = ùê∏ ùë• ‚àº ùëù ùëëùëéùë°ùëé ( ùë• ) [ ùëôùëúùëî (1 ‚àí ùê∑ ( ùê∫ ( ùëß )))] (2) The GAN is a minimax in which ùê∫ wants to minimize ùëâ while ùê∑ wants to maximize it: ùëöùëñùëõ ùê∫ ùëöùëéùë• ùê∑ ùëâ ( ùê∑, ùê∫ ) = ùê∏ ùë• ‚àº ùëù ùëëùëéùë°ùëé ( ùë• ) [ ùëôùëúùëîùê∑ ( ùë• )] + ùê∏ ùëß ‚àº ùëù ùëß ( ùëß ) [ ùëôùëúùëî (1 ‚àí ùê∑ ( ùê∫ ( ùëß )))] (3) As in the previous example, GAN models act on non sparse values (e.g., pixels in a picture), but they are not designed to work with sparse vectors or matrices. Our problem here is that CF datasets contain extraordinarily sparse matrices of ratings (users only vote or consume a very limited number of the available items). Using the regular GAN architecture is not an adequate approach to addressing CF-based RS. This paper proposes an extended GAN architecture where embeddings are introduced to code the sparse and discrete vectors of votes to dense and continuous vectors. This innovation makes it possible to use a regular GAN to generate dense and continuous vectors efficiently and accurately. This compression stage forces us to design the corre- sponding stage to decompress the generated dense vectors. The adopted solution makes it possible to set both the number of users and items in the generated dataset, which is a relevant innovation in the state-of-art. 2.2. Related works Generative deep learning is an innovative field in the CF RS area. Although some variational autoencoder approaches have been pub- lished [ 34 , 35 ], current research is mainly focused on GAN models [ 36 ]. A CF subfield where GANs are used is the attack/defense strate- gies [ 37 ], where these models can reinforce security in RS. Neverthe- less, the most extended uses of CF GANs are: (a) to solve the issue of noisy data, and (b) to tackle the data sparsity problem, and implement a data augmentation framework by capturing the distribution of real data. CFGAN [ 38 ] is a model that generates purchase vectors rather than the IDs of items and then uses the generated fake purchase vectors to augment the real vectors. The Wasserstein version of CFGAN is the unified GAN (UGAN) [ 39 ] and reports improvements compared to CFGAN. To prioritize long and short-term RS information (inter- actions between users and items that change quickly or slowly), the PLASTIC [ 40 ] model trains a generator and uses it as a reinforcement learning agent. The recurrent GAN: RecGAN [ 41 ], learns temporal patterns in ratings; combining GAN and recurrent neural networks (RNNs) models. To capture negative sampling information in the CF datasets, IPGAN [ 42 ] implements two different generative models: one for positive instances and another for negative instances. IPGAN considers the relations between the positive ratings sampled and the negative ones selected. Currently, the DCGAN model [ 43 ] combines GAN and reinforcement learning models to catch the information of the RS sessions, rather than the traditional historical matrices of votes from users to items. Session information includes the responses of users to current recommenda- tions. The user‚Äôs immediate feedback is managed by the reinforcement learning model combined with the GAN. The NCGAN [ 44 ] incorporates a neural network to extract nonlinear features from users, and a GAN to guide the recommendation training; the generator model makes user recommendations, whereas the discriminator model measures distances between real and generated distributions. An innovative method to improve the information flow from generator to discriminator [ 45 ] reduces the discrepancies between both models in the CF GAN. A regularization Wasserstein GAN model is used in [ 46 ], combined with an autoencoder acting as a generator, reporting accuracy improvement when applied to high-dimensional and sparse CF matrices. A CGAN (Conditional GAN) is used [ 47 ] to improve CF recommendations, and the sizes of the rating vectors can be set, simplifying the generator and discriminator tasks. Additionally, it allows conditional rating gen- eration to be established. For datasets that do not follow standard Gaussian distributions, a missing data imputation based on GAN [ 48 ] is proposed; results show improved quality in several representative clas- sification data sets. Trust information is used in [ 49 ] to make effective recommendations. They propose a GAN where the discriminator is an MLP model, and the generator is a long-short term memory network (LSTM) model [ 50 ]. Finally, CF datasets are usually imbalanced due to their social data collection (e.g: more young people than old people). To address this limitation, [ 51 ] proposes a Wasserstein GAN model in the generator, and the PacGAN concept in the discriminator [ 52 ], to minimize the mode collapse problem. A platform for multi-agent RS simulation is the probabilistic-based RecSim [ 53 ], which generates synthetic profiles of users and items, and uses Markov chains and recurrent neural networks. The Virtual- Taobao [ 54 ] is a multiagent reinforcement learning system designed to improve search in the social Taobao website; it makes use of a Knowledge-Based Systems 280 (2023) 111016 4 J. Bobadilla et al. GAN to simulate internal distributions. A simple matrix factorization is used [ 55 ] to inject topic diversification into the recommendation pro- cess. The DataGenCars [ 56 ] is a Java-based generator of RS synthetic data; it contains a statistical basement that provides flexibility, but it returns low accuracy compared to deep learning generative models. Finally, the SynEvaRec framework [ 57 ] provides the generation of synthetic RS datasets using the Synthetic Data Vault (SVD) library. This library models multivariate distributions using copula functions; its CTGAN sub-library includes GAN models. The main advantage of SynEvaRec, compared to previous frameworks, is that it can use different RSs as a source; its main drawbacks are the poor quality of the results in most of the cases, and the excessive time it takes to perform the training stage. Previous research mainly focuses on improving different objectives such as noise reduction, recommendation quality, prediction values, defense against attacks, or balancing data. To make this happen, many different approaches and information sources have been combined: the use of GAN, CGAN, Wasserstein GAN, etc. GAN models have been combined with Recurrent Neural Networks [ 58 ] and LSTM net- works [ 50 ], and reinforcement learning has been introduced in the GAN-based architectures. Long and short data have been introduced to the proposed models, in addition to trust information, session logs, including responses of the users to previous recommendations, and inferred negative votes. The pure generation of synthetic datasets does not seem to be a goal in this novel field of GAN applied to CF RS, which is currently focused on improving prediction and recommendation quality results by means of data augmentation based on the inherent ability of the GAN model to capture the complex nonlinear patterns of high-dimensional and sparse CF datasets. The innovation of our proposal is to generate representative and useful CF synthetic datasets, rather than to improve the existing results that are of varying quality. Additionally, it allows representative parameters to be set and a whole ‚Äòfamily‚Äô of synthetic datasets to be obtained, taking real datasets as a source, such as Movielens, Netflix, or MyAnimeList. These parameters are the number of users, the number of items, the number of samples and the variability of the generated data. By varying the parameter values, we can generate different versions of the same CF pattern, such as a Movielens-based dataset containing 8000 users and 3000 items, or another that contains 2000 users and 1000 items, among others. In this way, we can test the accuracy and performance impact of the dataset size, its sparsity, its number of users and items, as well as check the improvement of the MAE when the number of users increases. As far as we know, there are no published methods or models for creating, in a parameterized deep learning model, accurate and scalable synthetic datasets from diverse sources. 3. The generative adversarial networks-based approach for data- sets building in collaborative filtering As previously mentioned in the Introduction section, our research problem is defined as obtaining a larger, scalable synthetic dataset from an original RS dataset that synthesizes similar user behavior and valuation patterns in relation to the original dataset. In addition, it is desirable that such generation be parameterized, allowing the number of users, items, samples and variability of the distribution to be controlled. Next, this section proposes the GANRS method, which uses a GAN network to generate synthetic CF datasets; the GAN is fed with a real CF dataset and the model learns its internal patterns. The most innovative contribution is to feed the GAN with dense and small embedding representations of users and items, instead of the traditional approach where the GAN inputs are large and comprise sparse vectors containing the votes cast for each user. The main advantage of the GANRS method is that it greatly reduces the complexity of the GAN architecture, its convergence speed, and its performance. The traditional and sparse-based GAN architectures deal with very large input vectors: as large as the number of items in the dataset, which can be in the tens of thousands, and require a very large dense layer in the model to hold this huge amount of data. What is more, between 97% to 99% of the data is usually missing, since users only vote for or consume a tiny proportion of the available products or services, hence the extraordinary sparsity in the CF datasets. Following the huge dense layer, in classical GAN architectures, it is necessary to stack a large multilayer perceptron to reduce dimensionality. By comparison, the proposed model replaces the large dense layer with two embeddings, one to code users and the other to code items (bor- rowed from the DeepMF model in the first stage of the proposed method). Embedding layers are specifically designed to deal with sparse data; they receive integer values (user and item IDs, in our case), and they provide small embedding representations (typically 5 to 15 float values in the CF scenarios). Related users or items share similar embedding representations, and this feature allows for extraordinarily simplification of the model. Overall, the proposed architecture is much smaller than traditional architectures, it contains far fewer parameters, and consequently, learns faster. Additionally, it better captures the complex nonlinear relations between items and users, in the same way that non-GAN RS models do to improve predictions. The formalization of the GANRS method is presented and structured according to the following seven stages, also illustrated in Fig. 2 : ‚Ä¢ Stage 0. CF definitions 1 Let ùëà be the set of users who make use of a CF RS. 2 Let ùêº be the set of items available for voting in the CF RS. 3 Let ùëâ be the range of allowed votes; usually ùëâ = {1 , 2 , 3 , 4 , 5} . 4 Let ùëÜ be the set of samples contained in the CF dataset; in which ùëÅ = | ùëÜ | = ùë°‚Ñéùëíùë°ùëúùë°ùëéùëôùëõùë¢ùëöùëèùëíùëüùëúùëìùë£ùëúùë°ùëíùë†ùëêùëéùë†ùë° . 5 ùëÜ = { ‚ü® ùë¢, ùëñ, ùë£ ‚ü© 1 , ‚ü® ùë¢, ùëñ, ùë£ ‚ü© 2 , ‚Ä¶ ., ‚ü® ùë¢, ùëñ, ùë£ ‚ü© ùëÅ }; where each ùë¢ ‚àà {1 , ‚Ä¶ , | ùëà | } , each ùëñ ‚àà{1 , ‚Ä¶ , | ùêº | } , and each ùë£ ‚àà{1 , ‚Ä¶ , | ùëâ | } . ‚Ä¢ Stage 1. DeepMF training 6 Let E be the size of two neural layer embeddings used to vectorize each user and each item belonging to ùëà and ùêº , respectively. 7 Let ùëì ùëíùë¢ ( ùë¢ ) = [ ùëí ùë¢ 0 , ùëí ùë¢ 1 , ‚Ä¶ , ùëí ùë¢ ùê∏ ] , where ùëì ùëíùë¢ is the embedding layer output of the users, where ùë¢ ‚àà{1 , ‚Ä¶ , | ùëà | } . 8 Let ùëì ùëíùëñ ( ùëñ ) = [ ùëí ùëñ 0 , ùëí ùëñ 1 , ‚Ä¶ , ùëí ùëñ ùê∏ ] , where ùëì ùëíùëñ is the embedding layer output of the items, where ùëñ ‚àà{1 , ‚Ä¶ , | ùêº | } . By com- bining both dense vectors of user and item embeddings: ( [ ùëí ùë¢ 0 , ùëí ùë¢ 1 , ‚Ä¶ , ùëí ùë¢ ùê∏ ] and [ ùëí ùëñ 0 , ùëí ùëñ 1 , ‚Ä¶ , ùëí ùëñ ùê∏ ] ), we can make rating pre- dictions in the DeepMF training stage. The dot product of the user embedding and the item embedding in each ‚ü® ùë¢, ùëñ, ùë£ ‚ü© ùëó ‚àà ùëÜ provides its rating prediction: 9 ÃÇùë¶ ùëó = ùëì ùëíùë¢ ( ùë¢ ) ‚ãÖ ùëì ùëíùëñ ( ùëñ ) = [ ùëí ùë¢ 0 , ùëí ùë¢ 1 , ‚Ä¶ , ùëí ùë¢ ùê∏ ] ‚ãÖ [ ùëí ùëñ 0 , ùëí ùëñ 1 , ‚Ä¶ , ùëí ùëñ ùê∏ ] 10 1 2 ( ùë¶ ùëó ‚àí ÃÇùë¶ ùëó ) 2 is the output error used in the DeepMF neural network to start the backpropagation algorithm, where the neural weights are iteratively improved from the ùõø ùëó values: ‚ñµ ùë§ ùëóùëñ = ùõºùë¶ ùëó ùëì ‚Ä≤ ( ùëÅùëíùë° ùëñ ) ‚àë ùëò ùë§ ùëñùëò ùõø ùëò , when ùëò is a hidden layer, and ‚ñµ ùë§ ùëóùëñ = ùõºùë¶ ùëñ ùëì ‚Ä≤ ( ùëÅùëíùë° ùëñ ) 1 2 ( ùë¶ ùëò ‚àí ÃÇùë¶ ùëò ) 2 , if ùëò is the output layer. i, j, and k are successive sequential layers. ùëÅùëíùë° ùëñ represents the cumulative input received for an artificial neuron, ùëÅùëíùë° ùëñ = ‚àë ùëó ùë¶ ùëó ‚àó ùë§ ùëó , where ùëó is the index of the neurons in the layer preceding the current neuron. ‚Ä¢ Stage 2. DeepFM feedforward Once the DeepMF has learned, we can collect the embedding representation of each user and each item in the CF RS. 11 Let ùê∏ ‚àó = { ‚ü® ùë¢, [ ùëí ùë¢ 0 , ùëí ùë¢ 1 , ‚Ä¶ , ùëí ùë¢ ùê∏ ] ‚ü© , ‚àÄ ùë¢ ‚àà ùëà } , be the set of embeddings for all the RS users. ( ùë¢ ‚àà[1 ... # ùëà ] , one to u) 12 Let ùê∏ ‚àó ( ùë¢ ) = [ ùëí ùë¢ 0 , ùëí ùë¢ 1 , ‚Ä¶ , ùëí ùë¢ ùê∏ ] Knowledge-Based Systems 280 (2023) 111016 5 J. Bobadilla et al. Fig. 2. The stages of the proposed GANRS method. 13 Let ùê∏ ‚àó‚àó = { ‚ü® ùëñ, [ ùëí ùëñ 0 , ùëí ùëñ 1 , ‚Ä¶ , ùëí ùëñ ùê∏ ] ‚ü© , ‚àÄ ùëñ ‚àà ùêº } , be the set of embed- dings for all the RS items. ( ùëñ ‚àà[1 ... # ùêº ] , one to i) 14 Let ùê∏ ‚àó‚àó ( ùëñ ) = [ ùëí ùëñ 0 , ùëí ùëñ 1 , ‚Ä¶ , ùëí ùëñ ùê∏ ] ‚Ä¢ Stage 3. Setting the dataset of embeddings 15 Let ùëÖ = [ ‚ü® ùê∏ ‚àó ( ùë¢ ) , ùê∏ ‚àó‚àó ( ùëñ ) , ùë£ ‚ü© ] , ‚àÄ ‚ü® ùë¢, ùëñ, ùë£ ‚ü© ùëó ‚àà ùëÜ be the embedding- based dataset of real samples. ‚Ä¢ Stage 4. GAN training 16 Let ùëì ùê∑ be the discriminator D model belonging to a GAN model. 17 Let ùëì ùê∫ be the generator G model belonging to a GAN model. 18 Let ùëì ùê∫ùê∑ be the optimization function of the GAN model; ùëì ùê∫ùê∑ = ùëÄùëñùëõ ùê∫ ùëÄùëéùë• ùê∑ ùëì ( ùê∑, ùê∫ ) = ùê∏ ùëÖ [ ùëôùëúùëî ( ùê∑ ( ùëÖ ))] + ùê∏ ùëß [ ùëôùëúùëî (1 ‚àí ùê∑ ( ùê∫ ( ùëß )))] , where ùê∏ ùëÖ is the expected value for real samples, ùëß is the random noise that feeds the generator ùê∫ , and ùê∏ ùëß is the expected value for the generated fake profiles ùê∫ ( ùëß ) . Note that ùëÖ refers to [15]. ‚Ä¢ Stage 5. GAN generation 19 Let ùêπ = ùëì ùê∫ ( ùëß ) be the generated dataset of fake samples from different random noise vectors ùëß . ‚Ä¢ Stage 6. Clustering of items and users. 20 Let ùêæ ‚àó be the number of clusters used to group the embed- dings of the users. 21 Let ùêæ ‚àó‚àó be the number of clusters used to group the embeddings of the items. 22 Let ‚Ñé ‚àó ( ùë¢ ) = ùëê | ùëê ‚àà{1 , ‚Ä¶ , ùêæ ‚àó } , be the clustering operation that assigns a centroid to each user. 23 Let ‚Ñé ‚àó‚àó ( ùëñ ) = ùëê | ùëê ‚àà{1 , ‚Ä¶ , ùêæ ‚àó‚àó } , be the clustering operation that assigns a centroid to each item. ‚Ä¢ Stage 7. Setting dataset of item IDs and user IDs 24 Let H be the item IDs and users IDs discrete dataset ob- tained from the embedding-based dataset F of fake sam- ples. ùêª = { ‚ü® ‚Ñé ‚àó ( ùë¢ ) , ‚Ñé ‚àó‚àó ( ùëñ ) , ùë£ ‚ü©| ‚àÄ ‚ü® ùê∏ ‚àó ( ùë¢ ) , ùê∏ ‚àó‚àó ( ùëñ ) , ùë£ ‚ü© ‚àà ùêπ } 25 Let ùëÜ = { ùêª } be the synthetic generated dataset version of H where duplicated samples are removed. 26 Let ùê∫ ‚Ä≤ = { ‚ü® ‚Ñé ‚àó ( ùë¢ ) , ‚Ñé ‚àó‚àó ( ùëñ ) , ùë£ ‚ü© ‚àà ùêª | ‚àÑ ‚ü® ‚Ñé ‚àó ( ùë¢ ‚Ä≤ ) , ‚Ñé ‚àó‚àó ( ùëñ ‚Ä≤ ) , ùë£ ‚Ä≤ ‚ü© ‚àà ùêª where ‚Ñé ‚àó ( ùë¢ ) = ‚Ñé ‚àó ( ùë¢ ‚Ä≤ ) ‚àß ‚Ñé ‚àó ( ùëñ ) = ‚Ñé ‚àó‚àó ( ùëñ ) ‚àß ùë£ ‚â† ùë£ ‚Ä≤ } Fig. 2 shows the seven designed stages to generate different syn- thetic datasets from real datasets (Movielens, Netflix, etc.). Stage 1 (top left graph in Fig. 2 ) shows the training of a DeepMF model used to set both the embedding layer of users and the embedding layer of items. Basically, embedding layers in a neural network efficiently convert an input from a sparse representation into an output dense representation. For each input sample ‚ü® ùë¢ùë†ùëíùëü, ùëñùë°ùëíùëö, ùëüùëéùë°ùëñùëõùëî ‚ü© in the training set, the output dot layer combines the embedding layer values to predict the rating value and to obtain the output error ‚Äò‚Äò(rating - prediction)‚Äô‚Äô that will we backpropagated to update the learning parameters. Steps 6 to 10 formalize these concepts. Once the DeepMF model has learned, Stage 2 (top right graph in Fig. 2 ) shows the DeepMF feedforward process where each item ID (from one to the number of items in the dataset, range [1 ... # ùêº ] ) feeds the item embedding, which outputs the item ID dense representation; usually, CF embedding vectors have a size from 5 to 10. The same applies to user IDs as input and their output dense representations. Please note that the number of items in the dataset will be different from the number of users. Steps 11 to 14 explain this second stage. The purpose of the third stage is to convert the source sparse CF dataset into its dense representation. To accomplish the task, for each source ‚ü® ùë¢ùë†ùëíùëü, ùëñùë°ùëíùëö, ùëüùëéùë°ùëñùëõùëî ‚ü© sample in the dataset (e.g.: ‚ü® 8920 , 345 , 4 ‚ü© ) we replace the user ID (8920 in this example) with its related dense representation; the same applies for the item ID. Using embeddings of size 5, the result in the example could be such as: Knowledge-Based Systems 280 (2023) 111016 6 J. Bobadilla et al. Table 1 Example of samples representation. Sparse Dense < ùüñùüóùüé , 47 , 5 > < [ ùüé . ùüéùüë , ùüé . ùüóùüí , ùüè . ùüéùüê , ùüé . ùüñùüï , ‚àí ùüé . ùüïùüñ ] , [‚àí1 . 23 , 0 . 99 , 1 . 02 , 0 . 65 , ‚àí0 . 48] , 5 > < ùüñùüóùüé , ùüëùüè , 4 > < [ ùüé . ùüéùüê , ùüé . ùüóùüì , ùüé . ùüóùüó , ùüé . ùüñùüè , ‚àí ùüé . ùüîùüó ] , [ ùüé . ùüíùüì , ‚àí ùüé . ùüïùüñ , ùüé . ùüñùüë , ‚àí ùüé . ùüèùüì , ùüé . ùüéùüó ] , 4 > < 968 , ùüëùüè , 4 > < [‚àí1 . 04 , 0 . 04 , 0 . 66 , ‚àí0 . 67 , 0 . 11] , [ ùüé . ùüíùüê , ‚àí ùüé . ùüïùüè , ùüé . ùüñùüé , ‚àí ùüé . ùüèùüé , ùüé . ùüèùüí ] , 4 > < 123 , ùüëùüè , 2 > < [1 . 56 , ‚àí1 . 12 , 0 . 33 , 1 . 22 , ‚àí0 . 87] , [ ùüé . ùüíùüë , ‚àí ùüé . ùüïùüì , ùüé . ùüñùüé , ‚àí ùüé . ùüèùüè , ùüé . ùüéùüî ] , 2 > ‚ü® [0 . 03 , 0 . 94 , 1 . 02 , 0 . 87 , ‚àí0 . 78] , [‚àí1 . 23 , 0 . 99 , 1 . 02 , 0 . 65 , ‚àí0 . 48] , 4 ‚ü© . Stage 3 in Fig. 2 shows an illustrative example. Step 15 formalizes the operation. The dense dataset obtained will be used in Stage 4 to train a GAN capable of generating fake user and item profiles, as well as their associated rating values, which will be, even at this stage, the ratings that will be in the dataset generated at the end of the proposal. Our GAN will use the Stage 3 dense dataset to train the discriminator by providing it with the necessary real samples. The GAN generator takes Gaussian random noise as input and iteratively learns how to generate increasingly good fake profiles capable of cheating the discriminator model. Once the generator and the discriminator have learnt, the generator can convert input noise vectors into dense samples that mimic the patterns of the real dataset provided in stage 3. Stage 4 is formalized in steps 16 to 18 . The last stage in Fig. 2 (bottom left graph) uses the trained GAN generator model (Stage 4) to generate as many fake samples as desired. We feed the generator with successive vectors of random noise values following a Gaussian distribution, and the generator outputs successive fake dense samples following the patterns of the real dataset (obtained in Stage 3). The higher the standard deviation of the Gaussian distribu- tion, the higher the variety of individual values in the generated dense fake samples. As an example, a low standard deviation value in the ran- dom noise Gaussian distribution leads to a higher proportion of votes ‚Äò3‚Äô (ranging from 1 to 5), while choosing a high standard deviation value will produce a higher density of votes ‚Äò5‚Äô and ‚Äò1‚Äô. Ratings are generated in the same way as items and users: they are coded in the dense embedding generated by the GAN. Synthetic ratings are continuous values, whereas real ratings are discrete, usually in the range {1 , ‚Ä¶ , 5} . To make this conversion, a function assigns the maximum value in the range (usually ‚Äò5‚Äô) to the synthetic continuous values greater than it; analogously the function assigns the minimum value (usually ‚Äò1‚Äô) to the continuous values lower than it. Finally, a round function is performed to ensure discrete values. Step 19 formalizes the generation of fake samples. Although the GANRS method could be considered complete, this is not the case because our goal is to generate fake datasets of sparse samples (such as Movielens or Netflix); it is then necessary to convert from the obtained dense representation in Stage 5 to the usual sparse representation seen in Stage 1. The process is not straightforward, since all the dense representations of the fake samples are different from each other; this will be better explained using the example in Table 1 : it can be observed that user 890 (two first rows) has very similar dense embedding values, but there are not identical, since the GAN generator is not able to create the same exact values from the noise input vectors. The same situation occurs in Table 1 for the item with ID 31. Consequently, the GANRS method provides a way to ‚Äògroup‚Äô similar dense embeddings into a unique ID; that is, to convert the dense bold vectors of the user in Table 1 into a unique user ID (need not be 890), and the dense bold vectors of the item into a unique item ID (need not be 31, either). To group similar dense embeddings into a unique ID, a K-Means clustering [ 59 ] has been chosen. This algorithm has the relevant feature that a number K of clusters must be chosen a priori, and it is very con- venient in this context, since, in this way, we will have the opportunity to establish the number of users and the number of items in the GANRS synthetic generated dataset. Stage 6 of Fig. 2 shows this concept, where Table 2 Main parameter values of the tested datasets. Dataset #users #items #ratings Scores Sparsity Movielens 100K 943 1682 99,831 1 to 5 93.71 Netflix* 23,012 1,750 535,421 1 to 5 98.68 MyAnimeList 19,179 2,692 548,967 1 to 10 98.94 ùêæ ‚àó has been selected as number of users and ùêæ ‚àó‚àó has been selected as number of items. Two separate K-Means processes are run: one to group user embeddings, and the other to group item embeddings. Steps 20 to 23 formalize these two clustering processes. To better understand this stage, we can consider an example where one million fake samples have been generated and we want to create a synthetic dataset containing two thousand fake users and one thousand fake items. To accomplish this task, we should obtain two thousand groups collected from the one million user vectors (the same for the one thousand item groups). On average, five hundred user vectors could be assigned to each user group (and, analogously, one thousand item vectors to each item group), but we know that this depends on the user and item vector patterns. To adequately accomplish the grouping task, machine learning provides us with clustering algorithms, of which the k-means allow us to set the number of desired groups (two thousand for users and one thousand for items, in our example). Running both clustering processes (one for users and the other for items) we can assign a fake user ID to all the fake user vectors in each cluster. Please note that the ID number can be assigned at random to each of the two thousand clusters (the same for the one thousand item IDs). Fig. 3 illustrates the concept where graphs at the top show the two k-means clustering processing performed in the proposed model: one to group item vectors (yellow circles), and the other one to group user vectors (orange circles). Gray ellipses represent the k-means clustering groups. All the fake user vectors in each cluster collapse into the same user vector, which codes a sample representative of its group, and is different from the samples in the rest of the clusters (same for items). In this way, we obtain the selected representative K* users and K** items. The graphs at the bottom in Fig. 3 show the final stages of the proposed method; Stage 6 draws the K* clusters of users and the K** clusters of items, from the previous clustering with blue circles. Each of the K* clusters (of users) groups a set of user vectors (columns of orange squares), and each of the K** clusters (of items) groups a set of item vectors (columns of yellow squares). Each cluster of user vectors collapses into a representative user: at the bottom of Stage 6 graph (the same for items). Once the representative users and items are set, we can generate the fake dataset of embeddings by translating each generated embedding sample (bottom-right graph) to its equivalent representative concatenated embedding of representative (collapsed) users and items (at the bottom of the Stage 6 graph). Previously, we illustrated a case where a generated sample collapses its item vector in the ‚Äò‚Äòitem 3‚Äô‚Äô representative code (vector of red squares), and it collapses its user vector in the ‚Äò‚Äòuser 1‚Äô‚Äô representative code (vector of brown squares). In stage 7 the complete embedding, and the translation to the ‚ü® 1 , 3 , 5 ‚ü© sparse tuple codification can be seem. This is also true for the following fake embedding, which collapses in the ‚Äò‚Äòitem 1‚Äô‚Äô (green) and ‚Äò‚Äòuser 3‚Äô‚Äô (blue), generating the sparse tuple < 3,2,4>. Note that the GAN- generated profiles (bottom-right in Fig. 3 ) are not limited to a fixed number of users and items, whereas their Stage 7 version (bottom-left in Fig. 3 ) are limited to the ranges {1 , ‚Ä¶ , ùêæ ‚àó} , and {1 , ‚Ä¶ , ùêæ ‚àó‚àó} , making it possible to preset the number of users and items of the synthetic dataset. The seventh stage in Fig. 2 converts dense fake samples (coming from Stage 5) into sparse samples ‚ü® ùë¢ùë†ùëíùëü, ùëñùë°ùëíùëö, ùëüùëéùë°ùëñùëõùëî ‚ü© . To accomplish this task, for each sample in the dense representation we replace its user vector with its centroid number (from 1 to ùêæ ‚àó ) and its item vector with its centroid number (from 1 to ùêæ ‚àó‚àó ); the rating value remains the same as that already generated by the framework in Stages 4‚Äì5. Knowledge-Based Systems 280 (2023) 111016 7 J. Bobadilla et al. Fig. 3. Top graphs: clustering process to collapse user and item similar vectors into their representative user and item representations. Bottom graphs: translation from unlimited fake limited profiles to profiles in the range ‚ü® {1 ‚Ä¶ ùêæ ‚àó } , {1 ..ùêæ ‚àó‚àó } , ùëüùëéùë°ùëñùëõùëî ‚ü© . Fig. 2 shows an example of this operation, formalized in Step 24 . Please note that repeated samples will appear in the previous discretization process, since the GAN generator can create very similar dense samples that will be converted to the same discrete encoding. There are several factors that modulate the number of repeated samples, such as the number of generated samples, the embedding size, the size of the noise vector and the standard deviation of the Gaussian distribution, but the most relevant factor is the number of chosen users or items ( ùêæ ‚àó and ùêæ ‚àó‚àó ): the lower the ùêæ , the higher the number of repeated samples. When the number of users or items is low, the average number of samples grouped in each cluster is high. Step 25 formalizes the process of removing repeated discrete samples. Finally, the GANRS method can generate a small proportion of samples in which different votes are cast from the same user to the same item; e.g.: ‚ü® 879 , 56 , 4 ‚ü© , ‚ü® 879 , 56 , 5 ‚ü© . This could be considered as a convenient behavior: code a higher range of votes (4.5 in the example) or express a change in the user‚Äôs opinion. These cases can be unchanged, changed, or removed. Step 26 formalizes their removal operation. Overall, it is important to keep in mind that new rating values are initially calculated in the context of Stage 4 of the proposal, where GAN is used for generating the fake samples of pairs ‚ü® ùë¢ùë†ùëíùëü, ùëñùë°ùëíùëö, ùëüùëéùë°ùëñùëõùëî ‚ü© , using the user and item embeddings obtained in the previous stages as a base. Afterward, our methodology refines the obtained data to assure consistency (Stages 5‚Äì7). Appendix A ( Table 4 ) shows the main parameter and hyperparam- eter values used to design both the models DeepMF and GAN involved in the proposed GANRS method. 4. Experiments and results Evaluating the quality of the generated datasets and comparing them with state-of-art synthetic datasets is not straightforward, since the traditional measures only cover distribution probabilities. This is the case of the Kullback‚ÄìLeibler (KL) divergence ùê∑ ùêæùêø ( ùëÉ ‚à• ùëÑ ), where ùëÉ and ùëÑ are two probability distributions. In our context, we face two main drawbacks to applying the KL divergence or any similar divergence measure: 1) ùëÉ and ùëÑ are not distribution probabilities; they are datasets, and (2) a low ùê∑ ùêæùêø value does not mean that ùëÑ (the generated dataset) is a good synthetic dataset obtained from ùëÉ (the source dataset). In fact, if ùê∑ ùêæùêø = 0 , usually ùëÉ = ùëÑ , which is not a useful result. Of course, each CF dataset contains a reduced number of representative distribution probabilities, including: rating, user, and item distributions ( ùëÑ ùë¢ , ùëÑ ùëñ , ùëÑ ùëü ); but comparing each distribution of the generated dataset with the corresponding distribution of the source dataset has the same intrinsic problem as explained above: ùê∑ ùêæùêø ( ùëÉ ùë¢ ‚à• ùëÑ ùë¢ ) = 0 , ùê∑ ùêæùêø ( ùëÉ ùëñ ‚à• ùëÑ ùëñ ) = 0 , ùê∑ ùêæùêø ( ùëÉ ùëü ‚à• ùëÑ ùëü ) = 0 , does not mean that ùëÑ is a suitable synthetic dataset with regard to ùëÉ , indeed ùëÑ must have a certain degree of variability regard to ùëÉ . A common alternative approach that is used to deal with these situations is testing the quality results in the specific domain; in our case MAE, precision, recall, etc. Results should be interpreted according to graph trends rather than absolute values, since better results just mean that ùëÑ patterns are less complicated than ùëÉ ones, and worse results tell us that ùëÑ patterns are more complicated than ùëÉ ones. Which scenario is better? It depends on the objectives of the scientist that generates the synthetic datasets. Addressing the concerns explained, we provide a complete set of comparative graphs between the source ( ùëÉ ) and generated datasets ( ùëÑ ), including probability distributions of the user, item, rating, and precision and recall trends. Designing specific quality measures that maximize each scientist‚Äôs objectives (required distribution variability, required complexity in the resulting patterns, etc.) is challenging re- search, and would help compare state-of-art generative approaches, but this is out of the scope of this paper. In this paper we evaluate the suitability of the presented procedure focused on building synthetic datasets. First, the traditional data sets to be used as a starting point for the present procedure are presented, as well as a description of the experiments to be performed. Subsequently, the obtained results are presented and discussed. 4.1. Experiments To test the behavior of the proposed GANRS method, we will use three representative and open datasets in the CF field: Movielens [ 60 ], Netflix and MyAnimeList. We have chosen the 100K version of Movie- lens and a reduced version of the complete Netflix dataset: Netflix*, available in [ 61 ]. Table 2 shows the main parameter values for these datasets. A complete set of experiments has been run using Netflix*, whereas only a subset of these experiments is shown for Movielens and MyAnimeList, to reduce the size of the paper. Results from the Movielens and MyAnimeList tests are summarized at the end of this section. Each of the three source datasets is used to generate its corre- sponding synthetic version: setting different numbers of users, items, and samples, and changing the standard deviation of the Gaussian random noise. Knowledge-Based Systems 280 (2023) 111016 8 J. Bobadilla et al. Table 3 Parameter values of the synthetic datasets generated by GAN. Source: Netflix*. # std #users #items # std #users #items # std #users #items #samples 1 2.0 100 4000 6 2.5 100 4000 11 3.0 100 4000 1.5M 2 2.0 1000 4000 7 2.5 1000 4000 12 3.0 1000 4000 3 2.0 2000 4000 8 2.5 2000 4000 13 3.0 2000 4000 4 2.0 4000 4000 9 2.5 4000 4000 14 3.0 4000 4000 5 2.0 8000 4000 10 2.5 8000 4000 15 3.0 8000 4000 16 1.5 4000 2000 17 1.5 4000 8000 18 1.2 2000 4000 150K 19 1.2 2000 4000 500K 20 1.2 2000 4000 1M 21 1.2 2000 4000 3M Experiments have been carried out using the neural DeepMF model. Training, validation, and testing sets have been obtained for all the real datasets (Netflix*, MyAnimeList, and Movielens 100K), and their corresponding synthetic datasets. The source code to train the model and test the results is the same for both the real and generated datasets; ensuring the consistency of the graphs in the comparative figures ( Figs. 4 a, 4 b, 7 b, 7 e, 8 b and 7 e). Table 3 shows the GAN generated synthetic datasets used to test the proposed GANRS method, using Netflix* as source data. The ‚Äô#‚Äô columns show the number of the generated datasets; ‚Äòstd‚Äô is the stan- dard deviation used in the random noise Gaussian distribution; #users and #items are the total number of users and items chosen to generate each dataset; #samples is the number of fake samples created by the GAN generator. Please note that the final number of samples contained in each of the datasets is lower than #samples, due to the removing process of repeated samples. Cases 1 to 15 in Table 3 are used to test the effect of changing standard deviation and number of users. Cases 16 and 17 test the consequences of increasing the number of items. Finally, cases 18, 19 and 20 test the behavior of the synthetic datasets when they have different sizes (number of samples). All generated datasets and the source code of the proposed GANRS method are fully available in http://suleiman.ujaen.es:8061/gitlab-instance-981c80cc/ ganrs . Additionally, Appendix B ( Fig. 9 ) shows an example of the dis- tribution graphs obtained for each of the synthetic datasets. Following the link provided, each generated dataset is located in its specific directory where a ‚Äôreadme.txt‚Äô file is provided along the synthetic dataset distribution graphs. Using the parameter values of Table 3 , a variety of experiments have been conducted. The classification of the experiments is as follows: 1. Number of users (a) Distribution of users versus ratings (b) Distribution of the user ratings (c) Number of repeated samples (d) Proportion of samples with the same user and item (e) MAE and accuracy of the data set (f) Users‚Äô precision and recall 2. Number of items (a) MAE and accuracy of the dataset (b) Item‚Äôs precision and recall 3. Number of samples (a) Number of samples generated (b) Precision and Recall These experiments refer to well-known metrics in collaborative filtering. Precision is focused on measuring the proportion of relevant rec- ommendations (i.e. the user rated the item with a rating value equal or greater than a threshold ùúÉ ) among the top ùëÅ items recommended to the user ùë¢ , collected in the list ùëá ùëÅ ùë¢ (Eq. (4) ). On the other hand, Recall mea- sures the proportion of correctly predicted relevant recommendations among the total number of relevant votes of each user; therefore, recall is sensitive to the existing proportions of relevant ratings (Eq. (5) ). ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ = 1 # ùëà ‚àë ùë¢ ‚àà ùëà | { ùëñ ‚àà ùëá ùëÅ ùë¢ | ùëü ùë¢ùëñ ‚â• ùúÉ } | ùëÅ (4) ùëÖùëíùëêùëéùëôùëô = 1 # ùëà ‚àë ùë¢ ‚àà ùëà | { ùëñ ‚àà ùëá ùëÅ ùë¢ | ùëü ùë¢ùëñ ‚â• ùúÉ } | | { ùëñ ‚àà ùëá ùëÅ ùë¢ | ùëü ùë¢ùëñ ‚â• ùúÉ } | + | { ùëñ ‚àâ ùëá ùëÅ ùë¢ | ùëü ùë¢ùëñ ‚â• ùúÉ } | (5) Where ùëà is the set of training users, ùëü ùë¢ùëñ is the rating of the training user ùë¢ for the item ùëñ , ùëÅ is the number of recommendations, and ùëá ùëÅ ùë¢ is the set of ùëÅ recommendations for the test user ùë¢ : ùëÅ highest predictions of the user ùë¢ above the relevancy threshold ùúÉ . Please note that Precision measures the proportion of recommenda- tion hits (hits with respect to number of recommendations), whereas Recall measures the proportion of recommendation hits with respect to the total number of relevant items. Precision takes into consideration the number of true positives, whereas Recall combines both the true positives and the false negatives. The importance of the precision and the recall quality measures largely depends on the scenario in which they are applied, e.g. Recall seems to be crucial in medicine, where a false negative is a serious mistake (i.e. not detecting cancer). Nevertheless, Recall is less important in RS since missing a relevant film (false negative) is not serious; the objective is to maximize a correctly recommended film (true positives). The F1 quality measure combines both Precision and Recall (Eq. (6) ). ùêπ 1 = 2 ‚àó ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ ‚àó ùëÖùëíùëêùëéùëôùëô ùëÉùëüùëíùëêùëñùë†ùëñùëúùëõ + ùëÖùëíùëêùëéùëôùëô (6) Finally, this paper also tests accuracy (Eq. (7) ), where true negatives are also considered. In this case both the positive and the negative hits contribute to the results (to positively recommend relevant items and to negatively recommend non relevant items). ùê¥ùëêùëêùë¢ùëüùëéùëêùë¶ = | { ùëñ ‚àà ùëÜ ùë° | ùëù ùë¢ùëñ ‚â• ùúÉ ‚àß ùëü ùë¢ùëñ ‚â• ùúÉ } | + | { ùëñ ‚àà ùëÜ ùë° | ùëù ùë¢ùëñ < ùúÉ ‚àß ùëü ùë¢ùëñ < ùúÉ } | | ùëÜ ùë° | (7) Where ùëÜ ùë° is the set of test samples, and each sample is the tuple ‚ü® ùë¢, ùëñ, ùëü ‚ü© containing the user ID, item ID and rating of the user u for the item ùëñ ( ùëü ùë¢ùëñ ). The model prediction of the rating is ùëù ùë¢ùëñ . Two values of the threshold ùúÉ will be explored across the experimen- tal scenario when precision, recall, F1, and accuracy quality measures are tested Note that the accuracy quality measure does not use the term ùëá ùëÅ ùë¢ since the typical RS does not include negative recommendations. Accordingly, this accuracy formulation does not average users‚Äô results and acts on the entire training data, such as we have done with the Mean Absolute Error (Eq. (8) ). ùëÄùê¥ùê∏ = 1 | ùëÜ ùë° | ‚àë ùë† ‚àà ùëÜ ùë° | ùëù ùë¢ùëñ ‚àí ùëü ùë¢ùëñ | , ùë¢, ùëñ ‚àà ùëÜ (8) Knowledge-Based Systems 280 (2023) 111016 9 J. Bobadilla et al. Fig. 4. (a) Distribution of users versus ratings. Number of items: 4000. Datasets 8 and 10 in Table 3 ;(b) Distribution of user ratings. Number of users: 2000; number of items: 4000. Datasets 3, 8 and 13 in Table 3 ;(c) Number of samples remaining after removing the repeated ones. items: 4000. Datasets 6 to 10 in Table 3 ;(d) Proportion of samples in which the same user has cast different votes for the same item. items: 4000. Datasets 6 to 10 in Table 3 ;(e) MAE and accuracy. Number of users: 2000; number of items: 4000; ‚Äòstd‚Äô is the standard deviation of the Gaussian random noise distribution. Datasets 1 to 15 in Table 3 ;(f) Precision, recall, and F1. Standard deviation of the random noise Gaussian distribution: 2.5. Number of recommendations ùëÅ = [2,4,6,8,10]. Datasets 6 to 10 in Table 3 . 4.2. Results This subsection shows the graphs obtained when the designed ex- periments (previous subsection) are run. The synthetic datasets de- scribed in Table 3 are used to obtain results that allow us: (1) to compare the distributions of users, items and ratings belonging to the source datasets, in relation to those obtained using the synthetic datasets, (2) to measure the number of repeated samples returned in the clustering stage, and (3) to test the prediction and recommendation qualities and trends obtained by running the proposed RSGAN method and comparing them to those shown by the source datasets. 4.2.1. Experiment 1a. Number of users: Distribution of users versus ratings Fig. 4 a shows the density of users (y-axis) that have cast different numbers of votes (x-axis). (Selected datasets: 8 and 10 in Table 3 ). As expected, for a fixed number of ratings in the dataset, we can observe that the higher the number of users, the lower the number of ratings. If the fixed number of samples in the dataset is distributed among a high number of users, each user centroid in the clustering stage receives a lower number of samples. Please, note that Netflix* contains around 23,000 users. 4.2.2. Experiment 1b. Distribution of the user ratings Fig. 4 b shows the proportion of each rating 1,...,5 (x-axis) when different random noise Gaussian distributions are applied. (Selected datasets: 3, 8 and 13). It can be observed that the standard deviation 2.5 generates a more similar distribution of votes, compared to the Netflix* original, than the adjacent standard distributions 2 and 2.5. Fig. 4 b also shows the impact of the Gaussian standard deviation in the layout of the individual values of the GAN-generated samples. 4.2.3. Experiment 1c. Number of repeated samples As explained in the ‚ÄòMethod‚Äô section, the trained GAN generator predicts from random noise vectors as many dense samples as we want; all these samples are then converted from continuous dense values to discrete sparse ones. In the discretization process, repeated samples will appear that must be removed ( Table 1 contains an example). Fig. 4 c shows the number of samples remaining in the dataset after the removal process. The lower the number of users, the higher the number of samples assigned to each user (to its centroid in the clustering process), and therefore the higher the probability of repeating discrete samples. Overall, the smaller the number of users, the smaller the number of remaining samples. Selected datasets: 6 to 10 in Table 3 . 4.2.4. Experiment 1d. Proportion of samples with the same user and item The GANRS generated datasets possess one attribute that does not exist in the source datasets (Movielens, etc.): they contain a proportion of samples where the same user has cast different votes for the same item; e.g.: ‚ü® 348 , 90 , 5 ‚ü© , ‚ü® 348 , 90 , 4 ‚ü© , as explained in the ‚ÄòMethod‚Äô section. This can be seen as a mechanism to allow intermediate votes (4.5 in the example) or to allow users to change their minds. This makes sense if, the number of repeated votes is two or three. The rare cases of four or five repeated votes should be removed, just as we have done in all the generated datasets. From the standard quality metrics to measure the accuracy of predictions: Mean Absolute Error (MAE) and Root Mean Square Error (RMSE), we have chosen the former since it is the most widely used in RS state-of-art research. Some papers provide both measures, but experimental research shows that in the CF field, results for RMSE and MAE are very similar. This is because the distribution of the errors in the CF field usually has little variance. The MAE returns the absolute difference between the predicted values and the real values in the testing set: ùëÄùê¥ùê∏ = 1 ùëõ ‚àë ùëñ | ùë¶ ùëñ ‚àí ÃÇùë¶ ùëñ | . The lower the MAE, the better the model fits a dataset. The RMSE uses the square of the error instead of the absolute value: ùëÄùê¥ùê∏ = 1 ùëõ ‚àë ùëñ ( ùë¶ ùëñ ‚àí ÃÇùë¶ ùëñ ) 2 ; therefore, the RMSE is more sensitive to observations that are further from the mean, and this is not the case in CF. Fig. 4 d shows that for regular CF RS (1000 or more users), the proportion of four or five repetitions is not significant, and as the number of users increases, the proportion of repetitions drops very fast. 4.2.5. Experiment 1e. MAE and accuracy of the dataset Whereas the previous experiments analyze the internal composition and distribution of the synthetic datasets, this experiment and the Knowledge-Based Systems 280 (2023) 111016 10 J. Bobadilla et al. following experiment test the behavior of the generated datasets on the prediction and recommendation tasks. Fig. 4 e shows the prediction quality (MAE) and the accuracy of the recommendation obtained from each set of individual samples in Datasets 1 to 15 in Table 3 . Please note that these measures are not obtained by analyzing and averaging the results of users. The graphs in Fig. 4 e show an improvement in accuracy (and its corresponding decrease in MAE error) as the number of users increases. This behavior is expected in the CF RS, where a high number of users leads to better predictions, and it tells us that the GAN- generated samples follow a CF convenient pattern. The MAE values in the top graph of Fig. 4 e are closely related to the distribution of ratings for each of the standard deviations 2.0, 2.5 and 3.0. MAE/accuracy results can be used to select the most appropriate standard deviation; in this case: std = 2.5. 4.2.6. Experiment 1f. Users‚Äô precision and recall This experiment provides the most significant results to test the generated datasets: we extract the values and evolutions of two repre- sentative recommendation quality measures: precision and recall. The top graphs in Fig. 4 f show the quality values obtained testing several numbers of recommendations N: [2,4,6,8,10] (x-axis), two different relevancy thresholds ùúÉ : [4,5], and two number of users: 2000 (green lines), and 8000 (blue lines). The standard deviation of the Gaussian random noise has been set to 2.5. Selected datasets: 6 to 10 in Table 3 . The values and evolutions obtained from the synthetic datasets fit with the source dataset: Netflix* (black lines). Additionally, as expected, the overall results of the dataset generated by 8000 users outperform those of the 2000 users and are closer to the Netflix* reference (please note that Netflix* contains around 23,000 users). The two bottom graphs in Fig. 4 f represent the F1 combination of precision and recall; they clearly show the similarity in the behavior of the generated datasets compared to the source dataset. 4.2.7. Experiment 2a. MAE and accuracy when the number of items varies. Experiment 1e tested MAE and accuracy quality measures on datasets with different numbers of users. Now we will test both quality measures on datasets with different numbers of items: [100, 1K, 2K, 4K, 8K]. The results in Fig. 5 show adequate values for both MAE and accuracy, and consistent evolutions where accuracy increases and MAE decreases as the number of items (x-axis) increases. Thus, the higher the number of items, the better the accuracy: this shows that the GAN generator can enrich the data. The Netflix * source dataset contains 1750 items and we can observe in Fig. 5 a how the improvement slows down around this value (x-axis). Selected datasets: 16 and 17 in Table 3 , and the 100, 1000, 4000 user versions not included in Table 3 . 4.2.8. Experiment 2b. Items‚Äô precision and recall Experiment 2b is similar to Experiment 1f; now we will test the behavior of datasets that contain different numbers of items (instead of different numbers of users). Fig. 5 b shows the performance of Netflix* (1750 items), represented using black lines, and compares it with the 2000 item dataset (green lines) and the 8000 item dataset (blue lines). We can observe that evolutions and values are consistent with the source datasets (black lines); furthermore, both the 2K and 4K items versions perform well: the first one conveniently captures the Neflix* patterns of items, since both contain a similar number of items. The dataset generated second (8K items) can enrich the data and show better accuracy than the 2K items version. Selected datasets: 16 and 17 in Table 3 , and the 100, 1000, 4000 user versions not included in Table 3 . 4.2.9. Experiment 3a. Number of samples generated in datasets with differ- ent sizes Here we will test the number of samples that the GANRS method obtains when different numbers of generated samples and different numbers of users have been set. For this purpose, we define four different numbers of samples: 150K, 500K, 1M and 3M (Datasets 18 to 21 in Table 3 , and their equivalent datasets for 100, 1000, 4000 and 8000 users) in the GAN generation process. The number of items is fixed at 4K for all experiments. In Fig. 6 a we can observe that the smaller the number of users, the smaller the number of generated samples; this is due to the fact that the smaller the number of users, the higher the number of samples assigned to each user (to each centroid in the clustering stage), and therefore the higher the probability of repeated samples that will be removed. As an example, Fig. 6 shows that the 8K user dataset preserves, approximately, 1M samples from the GAN generated (version 3M), and 600K in version 1M. 4.2.10. Experiment 3b. Precision and recall on datasets with different sizes This experiment shows the impact of increasing the number of samples in datasets with fixed parameters, in this case: 2000 users, 4000 items, and a standard deviation of 1.2 ( Table 3 ; Datasets 18, 19 and 21). It is important to realize that we are using the same source dataset Netflix* to generate the three cases shown in Fig. 6 b: 150K samples (yellow lines), 500K samples (magenta lines), and 3M samples (red lines). Please note that 150K, 500K and 3M samples refer to the dense and continuously generated samples, prior to the removal stage to convert them into their sparse, discrete version. Fig. 6 a shows the final sizes of the datasets in the 2000 user data (x-axis). Fig. 6 b compares the precision and recall values obtained in the Netflix* dataset (black lines) with the generated values. Overall: (1) precision increases and recall decreases; (2) the bigger the generated dataset, the better its precision; (3) the higher the dataset, the lower its recall. Precision results improve when using large datasets, as there are more relevant samples to choose from, and therefore it is easier to succeed in the fixed number ùëÅ of recommended predictions. On the other hand, recall gets worse using large datasets because they contain more variability in the samples, particularly when large standard devi- ations have been chosen for the random noise Gaussian distribution. Unlike precision, whose denominator is the constant ùëÅ (number of recommendations), the recall quality measure depends on the variable: ‚Äònumber of relevant votes‚Äô in the set of test items for each user tested. As the number of samples increases, the number of user votes also increases (and, from them, the number of relevant votes); this is the reason why recall is lower in the 3M synthetic dataset in Fig. 6 b, and higher in the 150K version. Figs. 7 and 8 show, respectively, the results obtained from the MyAnimeList and Movielens 100K test datasets. Graph ‚Äô(a)‚Äô compares the rating distribution of each source dataset (in blue) with the gen- erated rating distributions obtained by setting different values of the Gaussian random noise standard deviation. We have chosen the stan- dard deviation value of 1.2 for MyAnimeList, and the standard devia- tion value of 2.5 for Movielens 100K, since the obtained distributions of ratings are closest to their respective baselines. Results ‚Äò(b)‚Äô, ‚Äò(c)‚Äô and ‚Äò(e)‚Äô are obtained using the selected standard deviation values. Graph ‚Äò(b)‚Äô shows the distribution of users according to their number of casted ratings (x-axis). As expected, they follow the same pattern as the one in Netflix*. To compare results, please note that MyAnimelist dataset contains 19,179 users, and Movielens 100K contains 943 users. Graph ‚Äò(c)‚Äô shows the number of samples left after removing repeated instances. The higher the number of users, the lower the probability of generating samples containing the same user ID, item ID, and rating. In the MyAnimeList case, we started with 1.5 million generated samples, whereas for Movielens we selected 1 million generated samples. Graph ‚Äò(d)‚Äô refers to MAE error and accuracy values obtained by processing the individual samples contained in each dataset. As usual in the CF context, the higher the number of users, the lower the error, and the higher the accuracy. Finally, Graphs ‚Äò(e)‚Äô tests the recommendations obtained by processing the users in each dataset. As is with Netflix*, compared to baselines, precision improves and recall gets worse. Knowledge-Based Systems 280 (2023) 111016 11 J. Bobadilla et al. Fig. 5. (a) MAE and accuracy obtained from the dataset samples when the number of items varies. Number of users: 4000. Standard deviation of the Gaussian random noise: 1.5. Datasets 16 and 17 in Table 3 ;(b) Precision, recall, and F1 when the number of items varies. Standard deviation of the random noise Gaussian distribution: 1.5. Number of recommendations ùëÅ = [2, 4, 6, 8, 10]. Datasets 16 and 17 in Table 3 . Fig. 6. (a) Number of generated samples using different number of users (x axis) and different number of GAN generated samples (legend). Standard deviation of the random noise Gaussian distribution: 1.2. Number of items: 4000. Datasets 18 to 21 in Table 3 ;(b) Precision and recall using a different number of recommendations (x axis) and a different number of GAN generated samples (legend). Standard deviation of the random noise Gaussian distribution: 1.2. Datasets 18, 19 and 21 in Table 3 . The results obtained in this section highlight the importance of those that test the performance of the synthetic datasets against the source datasets, particularly when specific RS metrics are used. To check the consistency between synthetic and real data, two types of Knowledge-Based Systems 280 (2023) 111016 12 J. Bobadilla et al. Fig. 7. MyAnimeList. 1.5 million generated samples, (a) distribution of the MyAnimeList ratings 1 to 10, (b) distribution of users according to their number of casted ratings, (c) number of samples after the removing process of the repeated ones, (d) error and accuracy by processing the samples of the dataset, (e) CF precision and recall (by testing the dataset users). The GANRS std = 1.2 value has been set to test experiments (b) to (e). experiments have been conducted: direct and indirect. In direct com- parisons, rating distributions have been obtained and compared from both source datasets and their synthetic versions. Figs. 4 b and 4 c show the Netflix* results by varying the number of generated users and the standard deviation of the random Gaussian distribution used to feed the proposed GAN. Figs. 7 b and 8 b show, respectively, comparison of the MyAnimeList and the Movielens 100K datasets, in this case by varying the number of users in the synthetic datasets versus their equivalent source counterparts. Indirect experiments tested and compared the recommendation performance on both the synthetic and the source datasets. We have chosen the recommendation quality measures of pre- cision, recall and F1, obtained using the classical neural model DeepMF. Results can be found in Fig. 4 f (Netflix* vs. its synthetic version), Fig. 7 e (MyAnimeList vs. its synthetic version), and Fig. 8 e (Movielens 100K vs. its synthetic version). Overall, as expected, the results show that synthetic datasets behave like their source datasets. The more similar the results are, the more suitable the generated datasets will be, as this means that the original datasets can be effectively replaced by synthetic ones. 4.3. Comparison of the proposed framework with previous work The Related Works section identifies some previous work focused on data generation methods for recommender systems. In this subsection, a brief analysis will be performed, which will focus on showing how this previous work is not truly comparable with our current proposal in a fair way, since it is focused on different objectives and also generates data of a different nature. ‚Ä¢ Mladenov et al. [ 53 ] presented RecSim NG, an architecture cen- tered on the generation of synthetic profiles of users and items as part of the recommendation environment. Overall, the goal of the work is the development of a configurable platform for both authoring and learning RS simulation environments. The aim of this simulation is to evaluate existing RS policies, or generate data to train new policies (in either a tightly coupled online fashion, or in batch mode). Furthermore, this paper lacks information about the presented method, and therefore does not allow reproducibility. ‚Ä¢ Shi et al. [ 54 ] introduce a multi-agent reinforcement learning architecture tailored to Taobao-specific website search improve- ment, and uses a GAN to simulate the internal rating distribu- tion. Therefore, considering that it is focused on data generation for a specific context, it is not comparable with the framework proposed in the current paper. ‚Ä¢ Del Carmen et al. [ 56 ] introduce DataGenCars, a Java-based generator of RS synthetic data. Here it is important to remark that this work is specifically focused on the context-aware recom- mendation scenario. In this sense, even though the proposed tool supports the generation of synthetic datasets of users, items, con- texts, and ratings; this generation always relies on context-related characteristics through criteria introduced throughout the work, such as the uncertainty of the content, the user‚Äôs expectations or the item‚Äôs attributes. As result, this work is not comparable with the methodology presented in our current paper, which mainly uses rating values as input and does not consider datasets with contextual information. Knowledge-Based Systems 280 (2023) 111016 13 J. Bobadilla et al. Fig. 8. Movielens 100K results. 1 million generated samples, (a) Distribution of the Movielens 100K ratings 1 to 5, (b) Distribution of users according to their number of casted ratings, (c) Number of samples after the removal process of the repeated ones, (d) error and accuracy by processing the samples of the dataset, (e) CF precision and recall (by testing the dataset users). The GANRS std = 2.5 value has been set to test experiments (b) to (e). ‚Ä¢ Provalov et al. [ 57 ] introduce the SynEvaRec framework, focused on the presentation of a novel paradigm for evaluating recom- mendations based on the generation of synthetic RS datasets. In contrast to our current paper, this approach is mainly focused on generating synthetic user and item profiles that are internally used by SynEvaRec to guarantee user privacy protection, mitigate the data insufficiency problem, and measure the effect of the no-free-lunch problem. Regarding the aim of the architecture proposed in Provalov et al. [ 57 ] is not the proper retrieval of the whole synthetic rating datasets to be used in further evaluations (i.e. an evaluation protocol is presented rather than a dataset generation method), a major transformation of their work is needed to make it comparable with this paper. A fair comparison is then not possible at this stage. 4.4. Overall discussion A large number of synthetic datasets have been generated to test the performance of the proposed GANRS method. These datasets have been created setting different values for the main parameters of the method: number of users, number of items, Gaussian random noise variation, and number of generated samples. To generalize the conclusions of this paper, three open and representative CF datasets have been used as sources for the generative process. Finally, a variety of quality measures have been tested on the generated datasets; of these, precision and recall are the most relevant. A key issue is that we are not able to visually test the quality of the generated samples, as can be done, for example, with the popular fake faces; in fact, in the CF context, we only can adequately test the generated datasets by comparing their CF quality results with those typically obtained in real CF datasets. For this reason, we have focused on the designed experiments in which the quality measures of precision and recall are tested: using datasets containing different numbers of users, different numbers of items, and different numbers of samples (sizes). In all cases, comparatively, we obtain excellent precision results and moderate recall values. Overall, it can be considered positive in the CF context, where precision errors are serious and recall errors are less important: it is worse to recommend a trip you will not like (sorry, no refunds!) than not to recommend a trip that you probably would enjoy. Please note that it is the opposite for a deep learning model detecting malignant tumors: it is worse to make precision errors (no early detection of the tumor) than making recall errors (to erroneously detect a tumor). Additionally, experiments show the relevant impact of the standard deviation on the quality of the results. The GAN network learning has been based on a vector containing noise values that serves as a seed to generate the different samples in the synthetic dataset. Each ‚Äòfake‚Äô sample is generated from the list of random values in the ‚Äònoise‚Äô vector. As usual in the GAN context, random values have been created from Knowledge-Based Systems 280 (2023) 111016 14 J. Bobadilla et al. a Gaussian distribution with a mean of 0 and a standard deviation of 1. Each generated sample contains a dense item representation, a dense user representation, and an individual value that codes the user‚Äôs rating of the item. Once the GAN has learned, its generative model can be used, in a feedforward process, to generate as many samples as we want, starting from a different random noise vector for each sample generated. Experimental results show that using a Gaussian distribution with a standard deviation of 1 leads to many ratings in the middle of the voting range: rating 3 and closest in the 1 to 5 voting range (Movielens and Netflix), and rating 5 and closests in 1 to 10 voting range (MyAnimeList). Several experiments in this section demonstrate that we can modulate the standard deviation of the Gaussian distribution of random noise to generate a wider range of ratings using feedforward. As expected, when the standard deviation increases, the range of ratings also increases proportionally. Finally, the experiments presented include the existing relationship between the number of fake samples generated for the GAN and the number of samples that the dataset will eventually contain. As explained in the ‚ÄòMethod‚Äô section, the conversion from dense and continuous values to sparse and discrete values leads to a probability of sample repetitions. Results show that, as expected, the larger the number of users and items in the synthetic dataset, the lower the number of repeated samples. It has also been shown that for a typical number of users, say 4000 or more, the probability of more than two different ratings from one user for the same item can be considered negligible. 5. Conclusions This paper provides an innovative method for generating synthetic parameterized collaborative filtering datasets from real datasets. Syn- thetic datasets can be generated by selecting different numbers of users, items, samples, and distribution variability. This means that comparative experiments can be designed on the basis of a whole ‚Äòfamily‚Äô of generated datasets, for example, to test the accuracy of a new matrix factorization model when the number of users increases. A GAN is used to obtain ‚Äòfake‚Äô samples from real samples, benefitting from the inherent capacity of GAN networks to capture complex patterns in the source datasets. The GAN learns from dense and continuous embedding representations of items and users, rather than the sparse and discrete representations of the collaborative filtering datasets. The effect is a fast and accurate learning process. The proposed GANRS method contains a clustering stage to convert from the dense generated ‚Äòfake‚Äô samples to the sparse and discrete values necessary to fill the generated dataset. This clustering stage implements a k-means algorithm to group items and another k-means to group users. In a natural way, both ‚Äòk‚Äô parameters set the chosen num- ber of users and items in the dataset. A drawback of the discretization process is the generation of identical samples that our method merely removes. A complete set of experiments have been made using three representative source datasets. We have tested the distribution values and evolutions of the results, as well as prediction and recommendation qualities. Although precision tends to improve, while recall tends to worsen, overall accuracy can be considered correct, since precision is more relevant than recall in the RS context. The results show that the generated datasets conveniently mimic the behavior of the source datasets Movielens, MyAnimeList, etc. The source code for the proposed GANRS method is available to ensure the reproducibility of the experiments. Similarly, a complete set of generated datasets has been made available for research. This paper and its related documentation open the door to address some future work, such as designing alternative options to the clustering stage, implementing the PacGAN concept in the GAN discriminator, testing generated datasets using a complete range of machine learning and deep learning collaborative filtering models, replacing the GAN model with a CGAN one, generating demographically balanced datasets, and performing an in-depth study of the impact of the random noise vector variations in the generated set of samples. CRediT authorship contribution statement Jes√∫s Bobadilla: Conceptualization, Validation, Formal analysis, Investigation, Software, Writing ‚Äì original draft, Writing ‚Äì review & editing, Visualization. Abraham Guti√©rrez: Conceptualization, Valida- tion, Formal analysis, Investigation, Software, Writing ‚Äì original draft, Visualization. Raciel Yera: Methodology, Validation, Formal analysis, Writing ‚Äì original draft , Visualization. Luis Mart√≠nez: Methodology, Validation, Writing ‚Äì original draft. Declaration of competing interest The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper. Data availability The link to the data and code has been shared in the manuscript. Acknowledgments This work was partially supported by Ministerio de Ciencia e Inno- vaci√≥n of Spain under the project PID2019-106493RB-I00 (DL-CEMG); the Comunidad de Madrid, Spain under Convenio Plurianual with the Universidad Polit√©cnica de Madrid, Spain in the actuation line of Programa de Excelencia para el Profesorado Universitario; and the Plan Andaluz de Investigaci√≥n, Desarrollo e Innovaci√≥n (PAIDI 2020), Spain under the project PROYEXCEL_00257. Appendix A See Table 4 . Table 4 Main parameter and hyperparameter values set for the neural models involved in the RSGAN method. DeepMF values Embedding size (both for users an items) 5 Optimizer Adam Loss function Mean squared error Epochs 20 GAN generator Input shape, noise vector size 100 Block 1 dense layer #neurons 10 Block 1 activation function LeakyRelu, alpha 0.2 Block 1 normalization BatchNormalization, momentum 0.8 Block 2 dense layer #neurons 20 Block 2 activation function LeakyRelu, alpha 0.2 Block 2 regularization Dropout 0.2 Block 3 dense layer #neurons 2 ‚àó ùëíùëöùëèùëíùëëùëëùëñùëõùëîùë†ùëñùëßùëí + 1 Block 3 activation function linear GAN discriminator Input: shape 2 ‚àó ùëíùëöùëèùëíùëëùëëùëñùëõùëîùë†ùëñùëßùëí + 1 Block 1 dense layer #neurons 6 Block 1 activation function LeakyRelu, alpha 0.2 Block 2 dense layer 1 Block 2 activation function Sigmoid GAN train Epochs 20 Batch size 64 Stochastic noise Gaussian (0,1) Loss function ( ùëüùëíùëéùëôùë†ùëéùëöùëùùëôùëíùë†ùëôùëúùë†ùë† + ùëìùëéùëòùëíùë†ùëéùëöùëùùëôùëíùë†ùëôùëúùë†ùë† )‚àï2 Knowledge-Based Systems 280 (2023) 111016 15 J. Bobadilla et al. Appendix B See Fig. 9 . Fig. 9. Main distributions of the data in the synthetic dataset generated from Movielens 100K compared to the distributions of the data in the source dataset. Number of users: 8000, number of items: 4000, initial number of samples: 800,000, standard deviation of the Gaussian noise: 2.5. Graph (a) shows the distribution of the fake users (y axis) versus the number of ratings belonging to each of the users (x axis). Graph (b) shows the distribution of the fake items (y axis) versus the number of ratings belonging to each of the items (x axis). Graph (c) shows the percentage of ratings (y axis) for each of the available vote values 1, 2, 3, 4, 5 (x axis) in the dataset. References [1] Z. Fang, L. Zhang, K. Chen, A behavior mining based hybrid recommender system, in: 2016 IEEE International Conference on Big Data Analysis, ICBDA, IEEE, 2016, pp. 1‚Äì5. [2] R. Yera, L. Mart√≠nez, Fuzzy tools in recommender systems: A survey, Int. J. Comput. Intell. Syst. 10 (1) (2017) 776‚Äì803. [3] R. Yera, A.A. Alzahrani, L. Mart√≠nez, A fuzzy content-based group recommender system with dynamic selection of the aggregation functions, Internat. J. Approx. Reason. 150 (2022) 273‚Äì296. [4] L. Zheng, V. Noroozi, P.S. Yu, Joint deep modeling of users and items using reviews for recommendation, in: Proceedings of the Tenth ACM International Conference on Web Search and Data Mining, 2017, pp. 425‚Äì434. [5] Y. Gong, Q. Zhang, Hashtag recommendation using attention-based convolutional neural network., in: IJCAI, 2016, pp. 2782‚Äì2788. [6] H. Kanwal, M. Assam, A. Jabbar, S. Khan, et al., Convolutional neural network and topic modeling based hybrid recommender system, Int. J. Adv. Comput. Sci. Appl. 11 (7) (2020). [7] K. McNally, M.P. O‚ÄôMahony, B. Smyth, A comparative study of collaboration- based reputation models for social recommender systems, User Model. User-Adapt. Interact. 24 (3) (2014) 219‚Äì260. [8] N.M. Villegas, C. S√°nchez, J. D√≠az-Cely, G. Tamura, Characterizing context-aware recommender systems: A systematic literature review, Knowl.-Based Syst. 140 (2018) 173‚Äì200. [9] M. Moradi, J. Hamidzadeh, Ensemble-based top-k recommender system considering incomplete data, J. AI Data Min. 7 (3) (2019) 393‚Äì402. [10] M. Jalili, S. Ahmadian, M. Izadi, P. Moradi, M. Salehi, Evaluating collaborative filtering recommender algorithms: a survey, IEEE Access 6 (2018) 74003‚Äì74024. [11] B. Zhu, R. Hurtado, J. Bobadilla, F. Ortega, An efficient recommender system method based on the numerical relevances and the non-numerical structures of the ratings, IEEE Access 6 (2018) 49935‚Äì49954. [12] R. Yera, A.A. Alzahrani, L. Mart√≠nez, Exploring post-hoc agnostic models for explainable cooking recipe recommendations, Knowl.-Based Syst. 251 (2022) 109216. [13] E. D‚ÄôAmico, G. Gabbolini, C. Bernardis, P. Cremonesi, Analyzing and improving stability of matrix factorization for recommender systems, J. Intell. Inf. Syst. 58 (2) (2022) 255‚Äì285. [14] M.H. Aghdam, A novel constrained non-negative matrix factorization method based on users and items pairwise relationship for recommender systems, Expert Syst. Appl. 195 (2022) 116593. [15] G. Ayci, A. K√∂ksal, M.M. Mutlu, B. Suyunu, A.T. Cemgil, Active learning with Bayesian nonnegative matrix factorization for recommender systems, in: 2019 27th Signal Processing and Communications Applications Conference, SIU, IEEE, 2019, pp. 1‚Äì4. [16] J. Bobadilla, R. Bojorque, A.H. Esteban, R. Hurtado, Recommender systems clustering using Bayesian non negative matrix factorization, IEEE Access 6 (2017) 3549‚Äì3564. [17] H.-J. Xue, X. Dai, J. Zhang, S. Huang, J. Chen, Deep matrix factorization models for recommender systems, in: IJCAI, Vol. 17, Melbourne, Australia, 2017, pp. 3203‚Äì3209. [18] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, T.-S. Chua, Neural collaborative filtering, in: Proceedings of the 26th International Conference on World Wide Web, 2017, pp. 173‚Äì182. [19] J. Bobadilla, R. Lara-Cabrera, A. Gonzalez-Prieto, F. Ortega, DeepFair: Deep learning for improving fairness in recommender systems., Int. J. Interact. Multimed. Artif. Intell. 6 (6) (2021) 86‚Äì95. [20] Y. Himeur, A. Alsalemi, A. Al-Kababji, F. Bensaali, A. Amira, C. Sardianos, G. Dimitrakopoulos, I. Varlamis, A survey of recommender systems for energy efficiency in buildings: Principles, challenges and prospects, Inf. Fusion 72 (2021) 1‚Äì21. [21] J. Bobadilla, J. Due√±as, A. Guti√©rrez, F. Ortega, Deep variational embedding representation on neural collaborative filtering recommender systems, Appl. Sci. 12 (9) (2022) 4168. [22] J. Bobadilla, √Å. Gonz√°lez-Prieto, F. Ortega, R. Lara-Cabrera, Deep learning approach to obtain collaborative filtering neighborhoods, Neural Comput. Appl. 34 (4) (2022) 2939‚Äì2951. [23] S. Zhang, L. Yao, A. Sun, Y. Tay, Deep learning based recommender system: A survey and new perspectives, ACM Comput. Surv. 52 (1) (2019) 1‚Äì38. [24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, Y. Bengio, Generative adversarial networks, Commun. ACM 63 (11) (2020) 139‚Äì144. [25] D. Sacharidis, Diversity and novelty in social-based collaborative filtering, in: Proceedings of the 27th ACM Conference on User Modeling, Adaptation and Personalization, 2019, pp. 139‚Äì143. [26] A. Gogna, A. Majumdar, DiABlO: Optimization based design for improving diversity in recommender system, Inform. Sci. 378 (2017) 59‚Äì74. [27] J. Bobadilla, A. Gutierrez, S. Alonso, √Å. Gonz√°lez-Prieto, Neural collaborative filtering classification model to obtain prediction reliabilities, Int. J. Interact. Multimed. Artif. Intell. 7 (4) (2022) 18‚Äì26. Knowledge-Based Systems 280 (2023) 111016 16 J. Bobadilla et al. [28] F. Pajuelo-Holguera, J.A. G√≥mez-Pulido, F. Ortega, Evaluating strategies for selecting test datasets in recommender systems, in: International Conference on Hybrid Artificial Intelligence Systems, Springer, 2019, pp. 243‚Äì253. [29] K.D. Bollacker, S. Lawrence, C.L. Giles, CiteSeer: An autonomous web agent for automatic retrieval and identification of interesting publications, in: Proceedings of the Second International Conference on Autonomous Agents, 1998, pp. 116‚Äì123. [30] W. Choochaiwattana, Usage of tagging for research paper recommendation, in: 2010 3rd International Conference on Advanced Computer Theory and Engineering, Vol. 2, ICACTE, IEEE, 2010, pp. V2‚Äì439. [31] J. Shokeen, C. Rana, Social recommender systems: techniques, domains, metrics, datasets and future scope, J. Intell. Inf. Syst. 54 (3) (2020) 633‚Äì667. [32] Y. Xing, I. Mohallick, J.A. Gulla, √ñ. √ñzg√∂bek, L. Zhang, An educational news dataset for recommender systems, in: Joint European Conference on Machine Learning and Knowledge Discovery in Databases, Springer, 2020, pp. 562‚Äì570. [33] F. Ortega, J. Bobadilla, A. Guti√©rrez, R. Hurtado, X. Li, Artificial intelligence scientific documentation dataset for recommender systems, IEEE Access 6 (2018) 48543‚Äì48555. [34] D. Liang, R.G. Krishnan, M.D. Hoffman, T. Jebara, Variational autoencoders for collaborative filtering, in: Proceedings of the 2018 World Wide Web Conference, 2018, pp. 689‚Äì698. [35] S. Zamany, D. Li, H. Fei, P. Li, Towards deeper understanding of variational auto-encoders for binary collaborative filtering, in: Proceedings of the 2022 ACM SIGIR International Conference on Theory of Information Retrieval, 2022, pp. 254‚Äì263. [36] M. Gao, J. Zhang, J. Yu, J. Li, J. Wen, Q. Xiong, Recommender systems based on generative adversarial networks: A problem-driven perspective, Inform. Sci. 546 (2021) 1166‚Äì1185. [37] Y. Deldjoo, T.D. Noia, F.A. Merra, A survey on adversarial recommender systems: from attack/defense strategies to generative adversarial networks, ACM Comput. Surv. 54 (2) (2021) 1‚Äì38. [38] D.-K. Chae, J.-S. Kang, S.-W. Kim, J.-T. Lee, Cfgan: A generic collaborative filtering framework based on generative adversarial networks, in: Proceedings of the 27th ACM International Conference on Information and Knowledge Management, 2018, pp. 137‚Äì146. [39] Z. Wang, M. Gao, X. Wang, J. Yu, J. Wen, Q. Xiong, A minimax game for generative and discriminative sample models for recommendation, in: Pacific- Asia Conference on Knowledge Discovery and Data Mining, Springer, 2019, pp. 420‚Äì431. [40] W. Zhao, B. Wang, J. Ye, Y. Gao, M. Yang, X. Chen, Plastic: Prioritize long and short-term information in top-n recommendation using adversarial training, in: Ijcai, 2018, pp. 3676‚Äì3682. [41] H. Bharadhwaj, H. Park, B.Y. Lim, Recgan: recurrent generative adversarial net- works for recommendation systems, in: Proceedings of the 12th ACM Conference on Recommender Systems, 2018, pp. 372‚Äì376. [42] G. Guo, H. Zhou, B. Chen, Z. Liu, X. Xu, X. Chen, Z. Dong, X. He, IPGAN: Generating informative item pairs by adversarial sampling, IEEE Trans. Neural Netw. Learn. Syst. (2020). [43] J. Zhao, H. Li, L. Qu, Q. Zhang, Q. Sun, H. Huo, M. Gong, DCFGAN: An adver- sarial deep reinforcement learning framework with improved negative sampling for session-based recommender systems, Inform. Sci. 596 (2022) 222‚Äì235. [44] J. Sun, B. Liu, H. Ren, W. Huang, NCGAN:: A neural adversarial collaborative filtering for recommender system, J. Intell. Fuzzy Systems 42 (4) (2022) 2915‚Äì2923. [45] Y. Lin, Z. Xie, B. Xu, K. Xu, H. Lin, Info-flow enhanced GANs for recommender, in: Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2021, pp. 1703‚Äì1707. [46] Q. Wang, Q. Huang, K. Ma, X. Zhang, A recommender system based on model regularization wasserstein generative adversarial network, in: 2021 IEEE International Conference on Systems, Man, and Cybernetics, SMC, IEEE, 2021, pp. 2043‚Äì2048. [47] J. Wen, X.-R. Zhu, C.-D. Wang, Z. Tian, A framework for personalized recom- mendation with conditional generative adversarial networks, Knowl. Inf. Syst. 64 (10) (2022) 2637‚Äì2660. [48] G. Deng, C. Han, D.S. Matteson, Extended missing data imputation via GANs for ranking applications, Data Min. Knowl. Discov. (2022) 1‚Äì23. [49] H. Chen, S. Wang, N. Jiang, Z. Li, N. Yan, L. Shi, Trust-aware generative adversarial network with recurrent neural network for recommender systems, Int. J. Intell. Syst. 36 (2) (2021) 778‚Äì795. [50] G. Van Houdt, C. Mosquera, G. N√°poles, A review on the long short-term memory model, Artif. Intell. Rev. 53 (2020) 5929‚Äì5955. [51] W. Shafqat, Y.-C. Byun, A hybrid GAN-based approach to solve imbalanced data problem in recommendation systems, IEEE Access 10 (2022) 11036‚Äì11047. [52] Z. Lin, A. Khetan, G. Fanti, S. Oh, Pacgan: the power of two samples in generative adversarial networks, in: Proceedings of the 32nd International Conference on Neural Information Processing Systems, 2018, pp. 1505‚Äì1514. [53] M. Mladenov, C.-w. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D. Tran, I. Vendrov, C. Boutilier, Demonstrating principled uncertainty modeling for recommender ecosystems with RecSim NG, in: Fourteenth ACM Conference on Recommender Systems, 2020, pp. 591‚Äì593. [54] J.-C. Shi, Y. Yu, Q. Da, S.-Y. Chen, A.-X. Zeng, Virtual-taobao: Virtualizing real- world online retail environment for reinforcement learning, in: Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33‚Äì01, 2019, pp. 4902‚Äì4909. [55] C.-N. Ziegler, S.M. McNee, J.A. Konstan, G. Lausen, Improving recommendation lists through topic diversification, in: Proceedings of the 14th International Conference on World Wide Web, 2005, pp. 22‚Äì32. [56] M. del Carmen Rodr√≠guez-Hern√°ndez, S. Ilarri, R. Hermoso, R. Trillo-Lado, DataGenCARS: A generator of synthetic data for the evaluation of context-aware recommendation systems, Pervasive Mob. Comput. 38 (2017) 516‚Äì541. [57] V. Provalov, E. Stavinova, P. Chunaev, SynEvaRec: A framework for evalu- ating recommender systems on synthetic data classes, in: 2021 International Conference on Data Mining Workshops, ICDMW, IEEE, 2021, pp. 55‚Äì64. [58] A. Cossu, A. Carta, V. Lomonaco, D. Bacciu, Continual learning for recurrent neural networks: an empirical evaluation, Neural Netw. 143 (2021) 607‚Äì627. [59] M. Ahmed, R. Seraj, S.M.S. Islam, The k-means algorithm: A comprehensive survey and performance evaluation, Electronics 9 (8) (2020) 1295. [60] F.M. Harper, J.A. Konstan, The movielens datasets: History and context, ACM Trans. Interact. Intell. Syst. (TIIS) 5 (4) (2015) 1‚Äì19. [61] F. Ortega, B. Zhu, J. Bobadilla, A. Hernando, CF4j: Collaborative filtering for java, Knowl.-Based Syst. 152 (2018) 94‚Äì99."
Deep variational models for collaborative filtering-based recommender systems.pdf,Deep variational models for collaborative filtering-based | recommender systems,Recommender systems  Collaborative Ô¨Åltering  Variational enrichment  Deep learning,"ORIGINAL ARTICLE Deep variational models for collaborative filtering-based recommender systems Jesu¬¥s Bobadilla 1,2 ‚Ä¢ Fernando Ortega 1,2 ‚Ä¢ Abraham Gutie¬¥rrez 1,2 ‚Ä¢ A ¬¥ ngel Gonza¬¥lez-Prieto 2,3,4 Received: 16 September 2022 / Accepted: 22 November 2022 / Published online: 9 December 2022  The Author(s) 2022 Abstract Deep learning provides accurate collaborative Ô¨Åltering models to improve recommender system results. Deep matrix factorization and their related collaborative neural networks are the state of the art in the Ô¨Åeld; nevertheless, both models lack the necessary stochasticity to create the robust, continuous, and structured latent spaces that variational autoencoders exhibit. On the other hand, data augmentation through variational autoencoder does not provide accurate results in the collaborative Ô¨Åltering Ô¨Åeld due to the high sparsity of recommender systems. Our proposed models apply the variational concept to inject stochasticity in the latent space of the deep architecture, introducing the variational technique in the neural collaborative Ô¨Åltering Ô¨Åeld. This method does not depend on the particular model used to generate the latent representation. In this way, this approach can be applied as a plugin to any current and future speciÔ¨Åc models. The proposed models have been tested using four representative open datasets, three different quality measures, and state-of-the-art baselines. The results show the superiority of the proposed approach in scenarios where the variational enrichment exceeds the injected noise effect. Additionally, a framework is provided to enable the reproducibility of the conducted experiments. Keywords Recommender systems  Collaborative Ô¨Åltering  Variational enrichment  Deep learning 1 Introduction Recommender Systems (RSs) are an artiÔ¨Åcial intelligence Ô¨Åeld that provides methods and models to predict and recommend items to users (e.g., Ô¨Ålms to persons, e-com- merce products to costumers, services to companies, Quality of Service (QoS) to Internet of Things (IoT) devices, etc.) [ 1 ]. Current popular RSs are Spotify, NetÔ¨Çix, TripAdvisor, Amazon, etc. RSs are usually categorized attending to their Ô¨Åltering strategy, mainly demo- graphic [ 2 ], content-based [ 3 ], context-aware [ 4 ], social [ 5 ], Collaborative Filtering (CF) [ 1 , 6 ] and Ô¨Åltering ensembles [ 7 , 8 ]. CF is the most accurate and widely used Ô¨Åltering approach to implement RSs. CF models have evolved from the K-Nearest Neighbors (KNN) algorithm to the Probabilistic Matrix Factorization (PMF) [ 9 ], the non- Negative Matrix Factorization (NMF) [ 10 ] and the Baye- sian non-Negative Matrix Factorization (BNMF) [ 11 ]. Currently, deep learning research approaches are growing in strength: they provide improvement in accuracy com- pared to the Machine Learning (ML)-based Matrix Fac- torization (MF) models [ 12 ]. Additionally, deep learning architectures are usually more Ô¨Çexible than the MF-based ones, introducing combined deep and shallow learn- ing [ 13 ], integrated content-based ensembles [ 14 ], gener- ative approaches [ 15 , 16 ], among others. Deep Matrix Factorization (DeepMF) [ 17 ] is a neural network model that implements the popular MF concept. DeepMF was designed to take as input a user-item matrix with explicit ratings and nonpreference implicit feedback, & A ¬¥ ngel Gonza¬¥lez-Prieto angelgonzalezprieto@ucm.es 1 Departamento de Sistemas Informa¬¥ticos, ETSI Sistemas Informa¬¥ticos, Universidad Polite¬¥cnica de Madrid, C. de Alan Turing, s/n, Madrid, 28031 Madrid, Spain 2 KNODIS Research Group, Universidad Polite¬¥cnica de Madrid, C. de Alan Turing, s/n, Madrid, 28031 Madrid, Spain 3 Departamento de A ¬¥ lgebra, Geometrƒ±¬¥a y Topologƒ±¬¥a, Universidad Complutense de Madrid, Plaza Ciencias 3, Madrid, 28040 Madrid, Spain 4 Instituto de Ciencias Matema¬¥ticas (CSIC-UAM-UCM- UC3M), C/ Nicola¬¥s Cabrera, 13-15, Madrid, 28049 Madrid, Spain Neural Computing and Applications (2023) 35:7817‚Äì7831 https://doi.org/10.1007/s00521-022-08088-2 (0123456789().,-volV) (0123456789(). ,- volV) although current implementations use two embedding layers whose inputs are, respectively, user and items. The experimental results evidence the DeepMF superiority over the traditional approaches based on ML-focused RS, par- ticularly the most used MF models: PMF, NMF, and BNMF. Currently, DeepMF is a popular model that is rapidly replacing the traditional MF models based on classical ML. Additionally, DeepMF has been used in the RS Ô¨Åeld to combine social behaviors (clicks, ratings,...) with images [ 18 ], and a social trust-aware RS has been implemented by using DeepMF to extract features from the user-item rating matrix for improving the initialization accuracy [ 19 ]. QoS predictions have also been addressed by using DeepMF [ 20 ]. To learn attribute representations, a DeepMF model has been used that creates a low-dimen- sional representation of a dataset that lends itself to a clustering interpretation [ 21 ]. Finally, the classical matrix completion task has been addressed by using the DeepMF approach [ 22 ]. The not so widely spread Neural Collaborative Filtering (NCF) model [ 13 ] may be seen as an augmented DeepMF model, where deeper layers are added to the ‚Äò Dot ‚Äô one. Additionally, the ‚Äò Dot ‚Äô layer can be replaced by a ‚Äò Concatenate ‚Äô layer. Figure 1 shows the explained concepts. NCF slightly outperforms the DeepMF accuracy results, but it increases the required runtime to train the model and to run the forward process: it is necessary to execute the ‚Äòextra‚Äô Multi-Layer Perceptron (MLP) on top of the ‚Äò Dot ‚Äô or ‚Äò Concatenate ‚Äô layers. Moreover, compared to DeepMF, the NCF architecture adds new hyper-parameters to set: mainly the number of hidden layers (depth) and their size (number of neurons in each layer) of the MLP architecture. The hypothesis of the paper is that we can improve the existing CF neural models by adding a variational stage that borrows its operative from the Variational Autoen- coders (VAE). VAEs not only improve latent factor-based models, but they also manage nonlinear probabilistic latent-variable models. While VAEs have been extensively used in the image-generative area, they have rarely been covered in the CF Ô¨Åeld. Autoencoders perform a nonlinear PCA, and VAEs improve their results by performing a nonlinear factor analysis. Unfortunately, regular autoen- coders do not ensure the regularity of the latent space; this is the reason why, in image processing, they do not perform Ô¨Åne producing new content from random encodings: Without explicit regularization, some combinations of the latent space are meaningless once decoded. The VAEs superiority comes from their ‚Äòvariational‚Äô behavior, which allows to make suitable regularizations such as in the statistic variational method. Using VAEs, inputs are encoded as distributions instead of single points, making it possible to naturally express latent space regularization. The CF improvement using VAEs is because the item and the user latent factor distributions are regularized in the training stage, ensuring that their latent spaces have good properties and conveniently generalize RS predictions. The VAEs regularization has two main properties: (1) com- pleteness: points sampled in the latent space give mean- ingful content once decoded, and (2) continuity: close points in the latent space provide similar contents when they are decoded. To accomplish these properties, usually regularization is done by enforcing distributions to be close to a centered and reduced standard normal distribution. Regularization involves a higher reconstruction error that can be balanced using the Kullback‚ÄìLeibler divergence. The use of VAE in the CF Ô¨Åeld provides a better Fig. 1 Deep Matrix Factorization (DeepMF) versus Neural Collaborative Filtering (NCF) 7818 Neural Computing and Applications (2023) 35:7817‚Äì7831 generalization; it not only can improve recommendations, but it also makes easier to use the latent codiÔ¨Åcations of items and users to make clustering, to explain recommen- dations, and to generate augmented datasets. The com- pleteness and continuity properties make possible these additional beneÔ¨Åts of the VAEs in the CF area. The rest of the paper has been structured as follows: In Sect. 2 , we describe the main ideas involved in our pro- posal, as well as its differences with the related work in variational CF-based recommender systems. In Sect. 3 , the proposed model is explained. Section 4 shows the experi- ments‚Äô design, results and their discussions. Finally, Sect. 5 contains the main conclusions of the paper and the future works. 2 Fundamentals and related work 2.1 VAEs as generative models Variational Autoencoders (VAEs) act as regular autoen- coders; they aim to compress the input raw values into a latent space representation by means of an encoder neural network, whereas the decoder neural network makes the opposite operation seeking to decompress from latent space to output raw values. The main difference between clas- sical autoencoders and VAEs is the latent space design, meaning, and operation. Classical autoencoders do not generate structured latent spaces, whereas VAEs introduce a statistical process that forces them to learn continuous and structured latent spaces. In this way, VAEs turn the samples into parameters of a statistical distribution, usually the means and variance of a Gaussian distribution. From the parameters in the multivariate distribution, we draw a random sample and a latent space sample is obtained for each training input. This operation procedure is represented in Fig. 2 . The stochasticity of the random sampling improves the robustness and forces the encoding of continuous and meaningful latent space representations, as it can be seen in Fig. 3 , where it is shown the VAE latent space represen- tation and its cumulative normal distribution. Due to their properties, VAEs have been used as gen- erative deep learning models in the image processing Ô¨Åeld. Reconstruction of a multispectral image has been per- formed by means of a VAE [ 23 ] that parameterizes the latent space of Gaussian distribution parameters. VAEs have been also used to create superresolution images as in [ 24 ], where a model is proposed to encode low-resolution images in a dense latent space vector that can be decoded for target high resolution image denoising. The blur image problem using VAE is tackled in [ 25 ] by adding a condi- tional sampling mechanism that narrows down the latent space, making it possible to reconstruct high resolution images. Moreover, in [ 26 ], the authors propose a Ô¨Çexible autoencoder model able to adapt to varying data patterns with time. By importing the VAE concept from image processing, several papers have used these models to improve RS results. For instance, denoising and variational autoencoders are tested in [ 27 ], where the authors reported the superiority of the VAE option against other models, or in [ 28 ], where variational autoencoders are combined with social information to improve the quality of the recommendations. 2.2 Our proposal: Deep variational models The aim of this paper is to propose a neural architecture that joins the best of the DeepMF and NCF models with the VAE concept. This novel models will be called, respec- tively, Variational Deep Matrix Factorization (VDeepMF) and Variational Neural Collaborative Filtering (VNCF). In contrast with the autoencoder and Generative Adversarial Network (GAN) approaches in the CF Ô¨Åeld [ 16 , 29 , 30 ], we shall not use the generative decoder stage and we maintain the regression output layer presented in the DeepMF and the NCF models. The main advantage in the use of the VAE operation is the robustness that it confers to the latent representation. This robustness can be seen by observing Fig. 3 . If we consider each dot drawn as a train sample representation in the latent space, then test samples are most likely to be correctly classiÔ¨Åed in the VAE model (right graph in Fig. 3 ) than being correctly classiÔ¨Åed in the regular autoencoder model (left graph in Fig. 3 ). In short, the variational approach stochastically ‚Äòspreads‚Äô the sam- ples in the latent space, improving the chances of classi- fying correctly the training samples. In our proposed RS CF scenario, we expect that rating values can be better predicted when a variational latent space has been learnt, because this space covers a wider, more robust, and more representative latent area. Whereas with a traditional autoencoders each sample would be coded as a value in the latent space (white circle in Fig. 4 ), the VAE encodes the parameters of a multivariate distri- bution (e.g., mean and variance of both the blue and the orange Gaussian distributions in Fig. 4 ). From the learnt distribution parameters, random sampling is carried out to generate stochastic latent space values (gray circles in Fig. 4 ). Each epoch in the learning process generates a new set of latent space values. Once the proposed model has been trained, when a h user, item i tuple is presented to the model, the obtained latent space value (green circle in Fig. 4 ) can be better predicted in the VAE scenario than in the regular autoencoder scenario: the random sampled values (gray circles) of the enriched latent space will help to associate the predicted sample (green circle) with their Neural Computing and Applications (2023) 35:7817‚Äì7831 7819 associated training samples (white circle), making the prediction process much more robust and accurate. From the above explanations, the VAE operation can be deÔ¨Åned following Fig. 2 in its ‚ÄòVariational layers‚Äô stage: Ô¨Årst, two dense layers code the normal distribution parameters that set the mean and variance of the latent factors. In the CF scenario, two dense layers are arranged to code the normal distribution parameters of the items, and two other different dense layers are used to code the normal distribution parameters of the users. This variational approach regularizes the latent factors and makes it pos- sible to reach the explained completeness and continuity goals. Once the distribution parameter layers are regular- ized, it is necessary to obtain a single latent factor point to code each user or item in the dataset; that is, for each user and item in the input of the model we need to combine its mean and variance. A normal random function is used to generate the latent factor point, coding the item (or the user) in the model input. Then, each latent factor point is obtained by combining: the normal random value, its item mean (or its user mean) and its item variance (or its user variance). This operation is usually performed using a neural ‚ÄòLambda‚Äô layer. Each Lambda layer result can be seen as a regularized version of the DeepMF nonvariational Fig. 2 Operation of a trained Variational Autoencoder (VAE) model. When a new sample is presented to the encoder stage (the handwritten digit ‚Äô2‚Äô in this example), the model produces in the latent space a probability distribution. Typically, this distribution belongs to a known family (a multivariate normal distribution in this example), so its shape is determined by some numerical parameters (mean and standard deviation in our case). With this information, the decoder stage generates an instance by sampling this distribution (getting a slightly different digit ‚Äô2‚Äô in this example). This introduces a stochastic component in the generation procedure that enriches the latent space and variability of the generative model Fig. 3 Representation of a VAE latent space for the MNIST dataset (left side) and its cumulative normal distribution (right side) 7820 Neural Computing and Applications (2023) 35:7817‚Äì7831 approach. Finally, we obtain the prediction of the rating of the user to the item by combining the ‚ÄòLambda‚Äô user and item factors using a dot product. In short, our variational approach incorporates the following substages: (1) Con- verting the input embedding factors to normal distribution values; and thus, making a regularization to generate continuous and complete latent factor codes, (2) Combin- ing the normal distribution latent factor codes to obtain single latent factor values, and (3) Predicting ratings by making the dot product of the regularized latent factor values. 2.3 VAEs for recommender systems Current CF-based variational autoencoders usually obtain raw augmented data. One strategy is to synthetize ratings from user to items or generated relevant versus not relevant votes from users to items [ 16 , 27 , 31 ], and another approach is to pre-train a VAE model to map data vectors into the latent space, an idea that has been intensively studied in several variants [ 32 ‚Äì 36 ]. In any case, these strategies force practitioners to sequentially run two separated models: the generative model (GAN or VAE) that provides augmented data, and the regression CF model that makes predictions and rec- ommendations. This approach presents three main draw- backs: (1) complexity, as two separate models are necessary, (2) large time consumption, and (3) sparsity management. As we will explain deeper in the following section, our proposed model does not generate raw aug- mented data. On the contrary, its innovation is based on the use of a single model to internally manage both augmen- tation and prediction aims. Particularly signiÔ¨Åcant is the way in which the proposed model addresses the sparsity problem: we do not make augmentation on the sparse raw data (ratings cast from users to item), but an internal ‚Äòaugmentation‚Äô process in the dense latent space of the model (Figs. 3 and 4 ). Each sample that is randomly gen- erated from the latent space feeds the model regression layers. Thereby, we propose a model that Ô¨Årst generates stochastic variational samples in a dense latent space, and then, these generated samples act as inputs of the regres- sion stage of the model. To test these ideas, the hypothesis considered in this paper is that the augmented samples will be more accurate and effective if they are generated in an inner and dense latent space rather than in a very sparse input space. It is important to realize that enriching the inner latent space can improve the recommendation results, but it also injects noise to the latent space that may potentially worsen the results. It is expected that the proposed approach will work better with poor latent spaces, whereas when it is applied to rich spaces, the spurious entropy added by the variational stage could worsen recommendations. Thus, medium-size CF datasets, or large and complex ones are better candi- dates to improve their results when the variational proposal is applied, whereas large datasets with predictable data distributions will probably not beneÔ¨Åt from the noise injection of the variational architecture. 3 Proposed model The proposed neural architecture will mix the VAE and the DeepMF (or the NCF) models. From the VAE, we take the encoder stage and its variational process, and from the DeepMF or the NCF model, we use its regression layers. This is an innovative approach in the RS Ô¨Åeld, since the VAE and GAN neural networks have only been used as a posteriori stage to make data augmentation, i.e., to obtain enriched input datasets to feed the CF DeepMF or NCF models. Hence, the traditional approach needs to separately train two models, Ô¨Årst the VAE and then the DeepMF/NCF networks. As discussed in Sect. 2.3 , these combined solu- tions present important disadvantages in terms of model complexity, time consumption and poor sparsity management. In sharp contrast, our proposed approach efÔ¨Åciently joins the VAE and the Deep CF regression concepts to obtain improved predictions with a single training process. In the learning stage, the training samples feed the model (left hand side of Fig. 5 ). Each training sample consists of the tuple h user, item, rating i (rating casted by the user to the item). In the DeepMF/NCF architecture, each user is represented by his/her vector of voted ratings, and each item is represented by its vector of received ratings. The model learns the ratings (third element in the tuples) casted by the users to the items (Ô¨Årst and second elements in the Fig. 4 Latent space representation of the proposed variational model. From the learnt means and variances of the multivariate Gaussian distribution, a random sampling process is run to spread the latent space sample values (gray circles) that will help to accurately predict the unknown sample rating values (green circle) Neural Computing and Applications (2023) 35:7817‚Äì7831 7821 tuples). In other words, the ratings are outputs of the neural network (right hand side of Fig. 5 ). Thanks to this architecture, the variational stage is nat- urally embedded into the model, so it can be Ô¨Çexibly used to inject variability into the samples. It is worth mentioning that this stage is also trained simultaneously to the pre- dictive part of the model, mutually inÔ¨Çuencing each other, which we expect will lead to better results than a simple separate learning. 3.1 Formalization of the model The architectural details of the proposed models are shown in Fig. 6 . For simplicity, only the Variational Deep Matrix Factorization (VDeepMF) architecture is shown in this Ô¨Ågure. The corresponding model for NCF, named Varia- tional Neural Collaborative Filtering (VNCF), is analogous to the VDeepMF one: it has the same ‚Äò Embedding ‚Äô and ‚Äò Variational ‚Äô layers and we should only replace the ‚Äò Dot ‚Äô layer of DeepMF by a ‚Äò Concatenate ‚Äô layer fol- lowed by a MLP. To Ô¨Åx the notation, let us suppose that our dataset contains U users and I items. In general, the aim of any deep learning model for CF-based prediction is to train a (stochastic) neural network that implements a function h : R U  R I ! R : This function h operates as follows. Let us codify the u -th user of the dataset (resp. the i -th item) using one-hot-en- coding as the u -th canonical basis vector e u (resp. the i -th canonical basis vector e i ). Then, we have that the outcome of the model is the following h √∞ e u ; e i √û 2 R ¬º Prediction of the score that the u 0th user would assign to the i 0th item : To train this function h , in the learning phase the neural network is fed with a set X ¬º h u ; i ; r i f g of training tuples h u ; i ; r i of a user u that rated item i with a score r . The function h is trained to minimize the error E √∞ h √û ¬º X h u ; i ; r i2 X d √∞ h √∞ e u ; e i √û ; r √û : √∞ 1 √û Here, d : R  R ! R is any metric on R . Typical choices are the so-called Mean Squared Error (MSE) and Mean Absolute Error (MAE), respectively, given by d MSE √∞ x ; y √û ¬º √∞ x  y √û 2 ; d MAE √∞ x ; y √û ¬º j x  y j : Our proposal for the VDeepMF consist on decomposing h has a combination of a ‚Äò Embedding ‚Äô, followed by a ‚Äò Variational ‚Äô stage and a Ô¨Ånal ‚Äò Dot ‚Äô layer, as shown in Fig. 6 ), that is h ¬º Dot  Variational  Embedding : Notice that, at the end of the day, h is a deep leaning model with novel customary layers designed for the RS problem. In this way, h can be trained to reduce the error E √∞ h √û of Eq. ( 1 ) with the standard Deep Learning (DL) methods, such as the backpropagation algorithm. 3.2 The embedding layer The Ô¨Årst ‚Äò Embedding ‚Äô layer (left hand side of Fig. 6 ) is borrowed from the Natural Language Processing (NLP) [ 13 ]. The idea is that this layers provides a fast translation of users and items into their respective Fig. 5 Proposed VDeepMF/ NCF approach. CF samples are encoded in the latent space by means of a variational process and then predictions are obtained by using a regression neural network 7822 Neural Computing and Applications (2023) 35:7817‚Äì7831 representations in the latent spaces. To be precise, this layer implements a linear map Embedding : R U  R I ! R L  R L ; that maps a pair √∞ e u ; e i √û into a pair of dense vectors √∞ v u ; w i √û 2 R L  R L that represents the u -th user and the i -th item, being L [ 0 the dimension of the representations. For our purpose, we implement the ‚Äò Embedding ‚Äô layer as a regular MLP dense layer, in sharp contrast with other approaches in NLP such as word2vec [ 37 , 38 ], GloVe [ 39 ] or ELMo [ 40 ], among others. The reason is that these later approaches seek to Ô¨Ånd an embedding preserving some metric information of the words, typically, the likelihood of Ô¨Ånding two words together or their semantic similarity. However, in our case, since we perform context-free CF prediction, no a priori information about the similarity between users or items is available. Indeed, this is precisely the ultimate goal of the RS: to Ô¨Ånd an appropriate repre- sentations of these users and items in the latent space. For this reason, we decided not to add any extra mechanism that could bias this training process, so the ‚Äò Embedding ‚Äô layer will act as a regular dense layer to be trained in parallel during the learning process. Finally, we would like to point out that, even though from a conceptual point of view the ‚Äò Embedding ‚Äô layer is just a dense layer, to save time and space, these ‚Äò Embedding ‚Äô layers are typically implemented through lookup tables. In this way, instead of feeding the network with the one-hot encoding of the user u (resp. the item i ), we input it via its ID as user (resp. as item). The lookup table efÔ¨Åciently recovers the u -th (resp. i -th) column of the embedding matrix that contains v u (resp. w i ) so that the translation can be conducted in a more efÔ¨Åcient way than with a standard MLP layer by exploiting the sparsity of the input. 3.3 The variational layer The variational process is carried out by the ‚Äò Varia- tional ‚Äô stage (labeled as ‚Äòvariational layers‚Äô at the middle of Fig. 6 ). This is the core of our proposed model. From the latent space representation √∞ v u ; w i √û 2 R L  R L of the u -th user and the i -th item, two separated dense layers return the mean and variance parameters of two Gaussian multivariate distribution. In this way, if Ô¨Åx a latent space dimension K [ 0, the Ô¨Årst part of this ‚Äò Variational ‚Äô stage (left part of the middle rectangle of Fig. 6 ) computes a map S √∞ v u ; w i √û ¬º √∞ l 1 √∞ v u √û ; r 2 1 √∞ v u √û ; l 2 √∞ w i √û ; r 2 √∞ w i √û√û 2 R 4 K : The outputs l 1 √∞ v u √û ; l 2 √∞ w i √û of S will be interpreted as the means of two Gaussian distributions to the user and the item, respectively, whereas r 2 1 √∞ v u √û ; r 2 √∞ w i √û will represent variance. The second part of the ‚Äò Variational ‚Äô stage (left right of the middle rectangle of Fig. 6 ) is ruled by a pair of random vectors √∞ P l 1 √∞ v u √û ; r 2 1 √∞ v u √û ; Q l 2 √∞ w i √û ; r 2 √∞ w i √û √û where P  N √∞ l 1 √∞ v u √û ; diag r 2 1 √∞ v u √û√û ; Q  N √∞ l 2 √∞ w u √û ; diag r 2 2 √∞ w i √û√û : Here, N √∞ l ; R √û denotes a K -dimensional multivariate nor- mal distribution of mean vector l and diagonal covariance matrix R , i.e., whose probability density function is Fig. 6 Proposed VDeepMF architecture. The NCF architecture will have identical ‚ÄòEmbedding‚Äô and ‚ÄòVariational‚Äô layers to the VDeepMF one; it will just replace the ‚ÄòDot‚Äô layer for a ‚ÄòConcatenate‚Äô layer, followed by an MLP Neural Computing and Applications (2023) 35:7817‚Äì7831 7823 f √∞ s √û ¬º 1 Ô¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨ÉÔ¨É √∞ 2 p √û K det R q exp  1 2 √∞ s  l √û t R  1 √∞ s  l √û   : Notice that, in our case, the covariance matrix is always diagonal. In this setting, the task of the ‚Äò Variational ‚Äô stage is just to sample P and Q . In this manner Variational √∞ v u ; w i √û ¬º √∞ p ; q √û 2 R K  R K ; where p is a sample of P ¬º P √∞ S √∞ v u ; w i √û√û  N √∞ l 1 √∞ v u √û ; diag r 2 1 √∞ v u √û√û and q is a sample of Q ¬º Q √∞ S √∞ v u ; w i √û√û  N √∞ l 2 √∞ w u √û ; diag r 2 2 √∞ w i √û√û . This pair represents the stochastic latent representations associated with √∞ v u ; w i √û . 3.4 The join layer This is the only layer that depends on the particular choice of the architecture. In the case of the Variational Deep Matrix Factorization (VDeepMF) architecture, this Ô¨Ånal layer is a ‚Äò Dot ‚Äô layer (labeled as ‚Äòregression layer‚Äô at right hand side of Fig. 6 ). It is just a linear layer that simply computes the dot product of the latent vectors p and q . Therefore Dot √∞ p ; q √û ¬º p  q : In the case of VNCF, this simple layer is replaced by a fully connected MLP H : R K ! R K ! R that extracts the nonlinear relations from p and q . Therefore, summarizing the process, the proposed VDeepMF model h computes h √∞ e u ; e i √û ¬º Dot  Variational  Embedding √∞ e u ; e i √û ¬º P l 1 √∞ v u √û ; r 2 1 √∞ v u √û  Q l 2 √∞ w i √û ; r 2 √∞ w i √û : Analogously, the VNCF model returns the proposed VDeepMF model h computes h √∞ e u ; e i √û ¬º H  Variational  Embedding √∞ e u ; e i √û ¬º H √∞ P l 1 √∞ v u √û ; r 2 1 √∞ v u √û ; Q l 2 √∞ w i √û ; r 2 √∞ w i √û √û : In both cases, h √∞ e u ; e i √û is a random variable that, when sampled, returns a natural number that should be inter- preted as the predicted rating by h for the user u regarding item i . 4 Empirical evaluation In this section, we describe the empirical experiments carried out to evaluate the performance of the variational approach in the DeepMF and NCF models. 4.1 Experimental setup The experimental evaluation has been performed over four different datasets to measure the performance of the pro- posed method over different environments. The selected datasets are: FilmTrust [ 41 ], an small dataset that contains the ratings of thousands of items to movies; MovieLens 1 M [ 42 ], the gold standard dataset in CF-based RS; MyAnimeList [ 43 ], a dataset extracted from Kaggle 1 that contains the ratings of thousands of users to anime comics; and NetÔ¨Çix [ 44 ], a popular dataset with hundred of millions ratings used in the NetÔ¨Çix Prize competition. Table 1 shows the main parameters of these datasets. The corpus of these datasets has been randomly splitted into training ratings (80% of the ratings) and test ratings (20% of the ratings). The evaluation of the proposed method has been ana- lyzed from three different points of view: the quality of the predictions [ 45 ], the quality of the recommendations [ 46 ], and the quality of the recommendation lists [ 47 ]. To measure the quality of the predictions, we have compared the real rating r u ; i of an user u to an item i of the test split R test with the predicted one, ^ r u ; i . These compar- isons have been carried out in three ways: using the MAE as in Eq. ( 2 ), using the MSE as in Eq. ( 3 ) and computing the proportion of the explained variance R 2 as in Eq. ( 4 ). Notice that, in Eq. ( 4 ),  r denotes the mean of the ratings contained in the test split. MAE ¬º 1 # R test X h u ; i i2 R test j r u ; i  ^ r u ; i j ; √∞ 2 √û MSE ¬º 1 # R test X h u ; i i2 R test r u ; i  ^ r u ; i   2 ; √∞ 3 √û R 2 ¬º 1  X h u ; i i2 R test r u ; i  ^ r u ; i   2 X h u ; i i2 R test r u ; i   r   2 : √∞ 4 √û To measure the quality of the recommendations, we have analyzed the impact of the top N recommended items to the user u , collected in the list T N u . Using precision Eq. ( 5 ), we measure the proportion of relevant recommendations (i.e., the user rated the item with a rated equal or greater than a threshold h ) among the top N . Here, U denotes the set of user in the test split. In a similar vein, using recall Eq. ( 6 ), we measure the proportion of the test items rated by the user u , R test u , that were relevant to him or her and were included into the recommended items T N u . For the con- ducted experiments, the used thresholds are h ¬º 3 for FilmTrust, h ¬º 4 for MovieLens and NetÔ¨Çix, and h ¬º 8 for MyAnimeList. These thresholds were chosen in agreement 1 www.kaggle.com . 7824 Neural Computing and Applications (2023) 35:7817‚Äì7831 with the results of [ 48 ], where it was shown that these values represent a fair trade-off between provided coverage of the dataset and prediction accuracy. Precision ¬º 1 # U X u 2 U f i 2 T N u j r u ; i  h g N ; √∞ 5 √û Recall ¬º 1 # U X u 2 U f i 2 T N u j r u ; i  h g f i 2 R test u j r u ; i  h g : √∞ 6 √û Additionally, we have measure the quality of the recom- mendations using the harmonic mean of the precision and the recall using F1 score Eq. ( 7 ). F1 ¬º 2  Precision  Recall Precision √æ Recall √∞ 7 √û However, evaluating the quality of recommendations based solely on user ratings provides a biased view of the rec- ommender‚Äôs performance. Therefore, we have also deter- mined the novelty Eq. ( 8 ) of the recommendations. Novelty [ 49 ] is calculated by assigning more weight to those items that have received fewer ratings. In other words, the nov- elty of an item is inversely proportional to the number of ratings received for an item ( # R i ) with respect to the total number of votes in the recommender system ( # R ). Novelty ¬º 1 # U X u 2 U P i 2 T N u  log 2 # R i # R   N : √∞ 8 √û Finally, to measure the quality of the recommendation lists, we use the normalized Discounted Cumulative Gain (nDCG). Suppose that the recommendation list of the user u , T N u , is sorted decreasingly so that the items predicted as more relevant are placed in the Ô¨Årst positions. Given i 2 T N u , let pos T N u √∞ i √û be the position of the item i in the recommendation list. Analogously, suppose that the real top N recommendations to user u , R N u , as sorted decreas- ingly and denote by pos R Nu √∞ i √û the position of the item i 2 R N u in the list. In this setting, the Discounted Cumulative Gain (DCG) and the Ideal Cumulative Gain (IDCG) of the user u 2 U are deÔ¨Åned as in Eq. ( 9 ). DCG u ¬º X i 2 T N u 2 r u ; i  1 log 2 pos T Nu √∞ i √û √æ 1   ; IDCG u ¬º X i 2 R N u 2 r u ; i  1 log 2 pos R Nu √∞ i √û √æ 1   : √∞ 9 √û In this way, nDCG is given by the mean of the ratio between DCG and IDCG as in Eq. ( 10 ). nDCG ¬º 1 # U X u 2 U DCG u IDCG u : √∞ 10 √û Due to the stochastic nature of the variational embedded space of the proposed method, the test predictions used to evaluate the proposed method have been computed as the average of the 10 predictions performed for each pair of user u and item i . Overall, the proposed variational architecture ade- quately improves simple models such as the DeepMF one, approaching their results to larger models such as the NCF. This tendency can be observed in both predictions and recommendation quality measures. Additionally, shorter running times are needed to train the proposed variational approach compared to baselines. This is the expected behavior in the hypothesis of the paper, but a remarkable constraint must be considered: the variational stage works particularly well when applied to not too large datasets, whereas using large datasets, the variational approach could not be necessary. The key idea is the ability of the Table 1 Main parameters of the datasets used in the experiments Dataset N users N items N ratings Scores Sparsity (%) FilmTrust 1508 2071 35,494 0.5 to 4.0 98.86 MovieLens 6040 3706 1,000,209 1 to 5 95.53 MyAnimeList 69,600 9927 6,337,234 1 to 10 99.08 NetÔ¨Çix 480,189 17,770 100,480,507 1 to 5 98.82 Table 2 Quality of the predictions FilmTrust MovieLens MyAnimeList NetÔ¨Çix (a) Mean Absolute Error. The lower the better. VDeepMF 0.6567. 0.6827 0.8722 0.7176 DeepMF 0.7957 0.6993 0.9044 0.6830 VNCF 0.6410 0.7263 0.9281 0.7474 NCF 0.6361 0.7021 0.8874 0.6903 (b) Mean Squared Error. The lower the better. VDeepMF 0.7324 0.7529 1.3453 0.8581 DeepMF 1.2046 0.7939 1.5017 0.7789 VNCF 0.6844 0.8179 1.4605 0.8952 NCF 0.6743 0.7908 1.3674. 0.7774 (c) R 2 score. The higher the better. VDeepMF 0.1438 0.3980 0.4549 0.2711 DeepMF - 0.4082 0.3652 0.3916 0.3384 VNCF 0.1999 0.3460 0.4083 0.2396 NCF 0.2118 0.3677 0.4460. 0.3397 The best results for each quality measure are highlighted in bold Neural Computing and Applications (2023) 35:7817‚Äì7831 7825 proposed model to deal with entropy: The variational stage increases entropy by generating stochastic latent factors and then enriching the latent space and making it more robust to the input sample variability. The intrinsic com- pleteness and continuity properties of the VAE are the foundations on which the variational approach gets robust, continuous, and structured latent spaces. These enriched spaces provide the improved results obtained in the experiments. 4.2 Experimental results Table 2 includes the quality of the predictions performed by the proposed model. Best values for each dataset are highlighted in bold. Table 2 a contains the MAE Eq. ( 2 ), Table 2 b contains the MSE Eq. ( 3 ), and Table 2 c contains the R 2 score Eq. ( 4 ). We can observe that the proposed variational approach improves the prediction capability of DeepMF in all datasets except of NetÔ¨Çix and reports worse predictions when it is applied to NCF. We justify these results by taking into account the fea- tures of the deep learning models used and the properties of each dataset. On the one hand, the larger the size of the dataset, the less necessary it is to enrich the votes with the proposed variational approach. In other words, when the dataset is small, the amount of Shannon entropy [ 50 ] that it contains might be quite limited. By using a variational method to generate new samples, we add some extra entropy that enriches the dataset, giving the chance to the regressive part of exploiting this extra data. However, large datasets usually present a large entropy in such a way that the regressive models can effectively extract very subtle information from them. In this setting, if we add a varia- tional stage, instead of adding new relevant variability to the dataset, we only add noise that muddies the underlying patterns. For this reason, the variational approach is of no beneÔ¨Åt in huge datasets like NetÔ¨Çix. On the other hand, the NCF model is more complex than the DeepMF one, so data enrichment has less impact for complex models that are able to Ô¨Ånd more sophisticated relationships between data than simpler models. In fact, Fig. 7 Quality of the recommendations measured by precision and recall. The higher the better 7826 Neural Computing and Applications (2023) 35:7817‚Äì7831 based on these results, we can assert that including the variational approach into a simple model such as DeepMF is equivalent to using a more complex model such as NCF. Furthermore, Figs. 7 and 8 show the quality of the recommendations using precision Eq. ( 5 ), recall Eq. ( 6 ) and F1 Eq. ( 7 ) quality measures. In FilmTrust (Figs. 7 a and 8 a), MovieLens (Figs. 7 b and 8 b) and MyAnimeList (Figs. 7 c and 8 c), we can observe that the proposed variational approach reports a beneÔ¨Åt for the DeepMF model and it worsens the results of the NCF model. In addition, VDeepMF model is the model that computes the best recommendations for these datasets. In contrast, in NetÔ¨Çix (Figs. 7 d and 8 d), the proposed variational approach does not improve the quality of the recommen- dations, with NCF being the model that provides the best recommendations for this dataset. These results are con- sistent with those analyzed when measuring the quality of the predictions. Consequently, it is evident that the pro- posed variational approach works adequately when the dataset is not too large and the model used is not too complex. Fig. 9 contains the quality of recommendations regard- ing novelty Eq. ( 8 ). It is observed that, when the variational stage is added to the DeepMF model, a signiÔ¨Åcant improvement of the novelty of the recommendations in small datasets is achieved. As the dataset becomes larger, the impact of the variational step is detrimental to the model. Thus, the variational stage has a positive impact on the FilmTrust (Fig. 9 a) and MovieLens (Fig. 9 b) datasets and a negative impact on the MyAnimeList (Fig. 9 c) and NetÔ¨Çix (Fig. 9 d) datasets. On the contrary, when a varia- tional stage is added to the NCF model, its impact on novelty is practically zero regardless of the dataset size. This experiment, like the previous ones, reafÔ¨Årms the conclusion that a variational step improves the results of simple models on small datasets. In addition, Fig. 10 contains the nDCG results. From it, we can observe the same trends as those shown in previous experiments: in FilmTrust (Fig. 10 a), the quality of the recommendation lists do not vary independently of whether the variational approach is used or not; in MovieLens (Fig. 10 b) and MyAnimeList (Fig. 10 c), the combination of the variational approach with simple modeling such as DeepMF, provides the best results; and in NetÔ¨Çix (Fig. 10 d), the variational approach signiÔ¨Åcantly worsens the quality of the recommendation lists. Fig. 8 Quality of the recommendations measured by F1. The higher the better Neural Computing and Applications (2023) 35:7817‚Äì7831 7827 5 Conclusions In the latest trends, accuracy of RSs is being improved by using deep learning models such as deep matrix factor- ization and neural collaborative Ô¨Åltering. However, these models do not incorporate stochasticity in their design, unlike variational autoencoders do. Variational random sampling has been used to create augmented input raw data in the collaborative Ô¨Åltering context, but the inherent col- laborative Ô¨Åltering data sparsity makes it difÔ¨Åcult to get accurate results. This paper applies the variational concept not to generate augmented sparse data, but to create aug- mented samples in the latent space codiÔ¨Åed at the dense inner layers of the proposed neural network. This is an innovative approach trying to combine the potential of the variational stochasticity with the augmentation concept. Augmented samples are generated in the dense latent space of the neural network model. In this way, we avoid the sparse scenario in the variational process. Observe that the proposed model in this paper also encodes the intrinsic locality of the users and items in the latent space. Recall that regular MF models capture the similarity of users and items in the latent space since pre- dictions are constructed via inner product, a continuous function. In the same spirit, our variational models also preserve this locality since the output is still computed through a continuous function: the feed-forward neural network, a much more complicated function, but eventu- ally continuous. Moreover, since the probability distribu- tions representing each user and item in the latent space depend on continuous parameters (the mean and standard deviation of a Gaussian distribution), small variations in these parameters, corresponding to similar items or users, are also encoded as almost equal distributions, and thus, their samples tend to be also close in the distributional sense. Thanks to these ideas, the results of the experimental analyses conducted in this paper show an important improvement when the proposed models are applied to middle-size representative collaborative Ô¨Åltering datasets, compared to the state-of-the-art baselines, testing both prediction and recommendation quality measures. In sharp contrast, testing on the huge NetÔ¨Çix dataset not only leads to no improvement, but the recommendation quality Fig. 9 Quality of the recommendations measured by novelty. The higher the better 7828 Neural Computing and Applications (2023) 35:7817‚Äì7831 actually gets worse. In this manner, increasing the Shannon entropy in rich latent spaces causes that the negative effect of the introduced noise exceeds its beneÔ¨Åt. Therefore, the proposed deep variational models should be applied to seek to a fair balance between their positive enrichment and their negative noise injection. To emphasize this idea, in Table 3, we show the total time and epochs required by each model to be Ô¨Åtted to each dataset using a Quadro RTX 8000 GPU. Best time for each dataset is in bold. We can observe that including a varia- tional layer to the model signiÔ¨Åcantly reduces the required time for Ô¨Åtting. Variational models are able to generate Shannon entropy that is transferred to the regression stage, leading to a more effective training that requires fewer epochs to be Ô¨Åtted. Therefore, the Ô¨Åtting time needed to reach acceptable results is substantially lower. The results presented in this work can be considered as generalizable, since they were analyzed in four represen- tative and open CF datasets. Researchers can reproduce our experiments and easily create their own models by using the provided framework referenced in Sect. 3 . The authors of this work are committed to reproducible science, so the code used in these experiments is publicly available. Among the most promising future works, we propose the following: (1) introducing the variational process in the alternative inner layers of the relevant architectures in the collaborative Ô¨Åltering area, (2) screening the learning evolution in the training process, since it is faster than the classical models but it also requires early stopping in the Fig. 10 Quality of the recommendations lists measured by NDCG. The higher the better Table 3 Fitting time using a Quadro RTX 8000 FilmTrust MovieLens MyAnimeList NetÔ¨Çix VDeepMF 61s (15 epochs) 601s (6 epochs) 7629s (9 epochs) 12655s (3 epochs). DeepMF 75s (25 epochs) 677s (10 epochs) 13217s (20 epochs) 15697s (4 epochs) VNCF 35s (7 epochs) 1030s (9 epochs) 9945s (9 epochs) 12650s (3 epochs) NCF 56s (15 epochs) 876s (10 epochs) 12111s (15 epochs) 16896s (4 epochs) Best Ô¨Åtting times for each datased in bold Neural Computing and Applications (2023) 35:7817‚Äì7831 7829 training stage, (3) providing further theoretical explana- tions of the properties of the CF datasets, in terms of Shannon entropy or other statistical features, that ensure a good performance of the proposed models, (4) applying probabilistic deep learning models in the CF Ô¨Åeld to cap- ture complex nonlinear stochastic relationships between random variables, and (5) testing the impact of the pro- posed concept when recommendations are made to groups of users. Acknowledgements A ¬¥ . G.-P. acknowledges the hospitality of the Department of Mathematics at Universidad Auto¬¥noma de Madrid where part of this work was developed. This work was partially supported by Ministerio de Ciencia e Innovacio¬¥n of Spain under the project PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de Madrid under Convenio Plurianual with the Universidad Polite¬¥cnica de Madrid in the actuation line of Programa de Excelencia para el Profesorado Universitario . The forth author has been partially sup- ported by the Madrid Government (Comunidad de Madrid ‚Äì Spain) under the Multiannual Agreement with the Universidad Complutense de Madrid in the line Research Incentive for Young PhDs, in the context of the V PRICIT (Regional Programme of Research and Technological Innovation) through the project PR27/21-029. Funding Open Access funding provided thanks to the CRUE-CSIC agreement with Springer Nature. Data availability The datasets analyzed during the current study are available in the repositories referred in the references [ 41 ‚Äì 44 ]. Declarations Conflict of interest The authors declare that they have no conflict of interest. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article‚Äôs Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article‚Äôs Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons. org/licenses/by/4.0/ . References 1. Beel J, Langer S, Genzmehr M, Gipp B, Breitinger C, Nu¬®rnberger A (2013) Research paper recommender system evaluation: a quantitative literature survey. In: Proceedings of the international workshop on reproducibility and replication in recommender systems evaluation, pp 15‚Äì22 2. Bobadilla J, Gonza¬¥lez-Prieto A ¬¥ , Ortega F, Lara-Cabrera R (2021) Deep learning feature selection to unhide demographic recom- mender systems factors. Neural Comput Appl 33(12):7291‚Äì7308 3. Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender systems leveraging multimedia content. ACM Comput Surveys (CSUR) 53(5):1‚Äì38 4. Kulkarni S, Rodd SF (2020) Context aware recommendation systems: a review of the state of the art techniques. Comput Sci Rev 37:100255 5. Shokeen J, Rana C (2020) A study on features of social recom- mender systems. Artif Intell Rev 53(2):965‚Äì988 6. Bobadilla J, Alonso S, Hernando A (2020) Deep learning archi- tecture for collaborative Ô¨Åltering recommender systems. Appl Sci 10(7):2441 7. Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of a recommender system with ensemble learning and graph embedding: a case on movielens. Multimed Tools Appl 80(5):7805‚Äì7832 8. C¬∏ ano E, Morisio M (2017) Hybrid recommender systems: a systematic literature review. Intell Data Anal 21(6):1487‚Äì1524 9. Mnih A, Salakhutdinov RR (2007) Probabilistic matrix factor- ization. Adv Neural Inf Process Syst 20:1257‚Äì1264 10. Fe¬¥votte C, Idier J (2011) Algorithms for nonnegative matrix factorization with the b -divergence. Neural Comput 23(9):2421‚Äì2456 11. Hernando A, Bobadilla J, Ortega F (2016) A non negative matrix factorization for collaborative Ô¨Åltering recommender systems based on a bayesian probabilistic model. Knowl-Based Syst 97:188‚Äì202 12. Rendle S, Krichene W, Zhang L, Anderson J (2020) Neural collaborative Ô¨Åltering vs. matrix factorization revisited. In: Fourteenth ACM conference on recommender systems, pp 240‚Äì248 13. He X, Liao L, Zhang H, Nie L, Hu X, Chua T-S (2017) Neural collaborative Ô¨Åltering. In: Proceedings of the 26th international conference on world wide web, pp 173‚Äì182 14. Narang S, Taneja N (2018) Deep content-collaborative recom- mender system (dccrs). In: 2018 international conference on advances in computing, communication control and networking (ICACCCN), pp 110‚Äì116. IEEE 15. Bobadilla J, Lara-Cabrera R, Gonza¬¥lez-Prieto A ¬¥ , Ortega F (2021) Deepfair: deep learning for improving fairness in recommender systems. Int J Interact Multimed Artif Intell 6(6):86‚Äì94 16. Gao M, Zhang J, Yu J, Li J, Wen J, Xiong Q (2021) Recom- mender systems based on generative adversarial networks: a problem-driven perspective. Inf Sci 546:1166‚Äì1185 17. Xue H-J, Dai X, Zhang J, Huang S, Chen J (2017) Deep matrix factorization models for recommender systems. In: IJCAI, Mel- bourne, Australia, vol 17, pp 3203‚Äì3209 18. Wen J, She J, Li X, Mao H (2018) Visual background recom- mendation for dance performances using deep matrix factoriza- tion. ACM Trans Multimed Comput Commun Appl (TOMM) 14(1):1‚Äì19 19. Wan L, Xia F, Kong X, Hsu C-H, Huang R, Ma J (2020) Deep matrix factorization for trust-aware recommendation in social networks. IEEE Trans Network Sci Eng 8(1):511‚Äì528 20. Zou G, Chen J, He Q, Li K-C, Zhang B, Gan Y (2020) Ndmf: Neighborhood-integrated deep matrix factorization for service qos prediction. IEEE Trans Netw Serv Manage 17(4):2717‚Äì2730 21. Trigeorgis G, Bousmalis K, Zafeiriou S, Schuller BW (2016) A deep matrix factorization method for learning attribute repre- sentations. IEEE Trans Pattern Anal Mach Intell 39(3):417‚Äì429 22. Fan J, Cheng J (2018) Matrix completion by deep matrix fac- torization. Neural Netw 98:34‚Äì41 23. Liu X, Gherbi A, Wei Z, Li W, Cheriet M (2020) Multispectral image reconstruction from color images using enhanced varia- tional autoencoder and generative adversarial network. IEEE Access 9:1666‚Äì1679 7830 Neural Computing and Applications (2023) 35:7817‚Äì7831 24. Liu Z-S, Siu W-C, Wang L-W, Li C-T, Cani M-P (2020) Unsupervised real image super-resolution via generative varia- tional autoencoder. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pp 442‚Äì443 25. Liu Z-S, Siu W-C, Chan Y-L (2020) Photo-realistic image super- resolution via variational autoencoders. IEEE Trans Circ Syst Video Technol 31(4):1351‚Äì1365 26. Zhang S-s, Liu J-w, Zuo X, Lu R-k, Lian S-m (2021) Online deep learning based on auto-encoder. Appl Intell 51(8):5420‚Äì5439 27. Liang D, Krishnan RG, Hoffman MD, Jebara T (2018) Varia- tional autoencoders for collaborative Ô¨Åltering. In: Proceedings of the 2018 world wide web conference, pp 689‚Äì698 28. Nisha C, Mohan A (2019) A social recommender system using deep architecture and network embedding. Appl Intell 49(5):1937‚Äì1953 29. Rama K, Kumar P, Bhasker B (2021) Deep autoencoders for feature learning with embeddings for recommendations: a novel recommender system solution. Neural Comput Appl 33(21):14167‚Äì14177 30. Tahmasebi H, Ravanmehr R, Mohamadrezaei R (2021) Social movie recommender system based on deep autoencoder network using twitter data. Neural Comput Appl 33(5):1607‚Äì1623 31. Li X, She J (2017) Collaborative variational autoencoder for recommender systems. In: Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pp 305‚Äì314 32. He M, Meng Q, Zhang S (2019) Collaborative additional varia- tional autoencoder for top-n recommender systems. IEEE Access 7:5707‚Äì5713 33. Nahta R, Meena YK, Gopalani D, Chauhan GS (2021) Two-step hybrid collaborative Ô¨Åltering using deep variational bayesian autoencoders. Inf Sci 562:136‚Äì154 34. Shenbin I, Alekseev A, Tutubalina E, Malykh V, Nikolenko SI (2020) Recvae: a new variational autoencoder for top-n recom- mendations with implicit feedback. In: Proceedings of the 13th international conference on web search and data mining, pp. 528‚Äì536 35. Wang K, Xu L, Huang L, Wang C-D, Lai J-H (2019) Sddrs: stacked discriminative denoising auto-encoder based recom- mender system. Cogn Syst Res 55:164‚Äì174 36. Liu Y, Wang S, Khan MS, He J (2018) A novel deep hybrid recommender system based on auto-encoder with neural collab- orative Ô¨Åltering. Big Data Mining Anal 1(3):211‚Äì221 37. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013) Distributed representations of words and phrases and their com- positionality. Adv Neural Inform Process Syst 26 38. Mikolov T, Chen K, Corrado G, Dean J (2013) EfÔ¨Åcient esti- mation of word representations in vector space. arXiv preprint arXiv:1301.3781 39. Pennington J, Socher R, Manning CD (2014) Glove: Global vectors for word representation. In: Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pp 1532‚Äì1543 40. Peters M, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, Zettlemoyer L (1802) Deep contextualized word representations. 2018. arXiv preprint arXiv:1802.05365 41. Guo G, Zhang J, Yorke-Smith N (2013) A novel bayesian simi- larity measure for recommender systems. In: Proceedings of the 23rd international joint conference on artiÔ¨Åcial intelligence (IJCAI), pp 2619‚Äì2625 42. Harper FM, Konstan JA (2015) The movielens datasets: history and context. Acm Trans Interact Intell Syst (tiis) 5(4):1‚Äì19 43. Azathoth: MyAnimeList Dataset. https://www.kaggle.com/aza thoth42/myanimelist . [Online; accessed 06-July-2021] (2018) 44. Bennett J, Lanning S et al (2007) The netÔ¨Çix prize. In: Pro- ceedings of KDD Cup and Workshop, New York, NY, USA, vol 2007, p 35. 45. Bobadilla J, Hernando A, Ortega F, Bernal J (2011) A framework for collaborative Ô¨Åltering recommender systems. Expert Syst Appl 38(12):14609‚Äì14623 46. Herlocker J-L, Konstan J-A, Terveen L-G, Riedl J-T (2004) Evaluating collaborative Ô¨Åltering recommender systems. ACM Trans Inf Syst 22(1):5‚Äì53 47. Gunawardana A, Shani G (2015) Evaluating recommender sys- tems. Handbook, Boston, MA 48. Ortega F, Lara-Cabrera R, Gonza¬¥lez-Prieto A ¬¥ , Bobadilla J (2021) Providing reliability in recommender systems through bernoulli matrix factorization. Inf Sci 553:110‚Äì128 49. Castells P, Vargas S, Wang J (2011) Novelty and diversity metrics for recommender systems: choice, discovery and rele- vance. In: Proceedings of the 33rd European conference on information retrieval (ECIR‚Äô11) 50. Shannon CE, Weaver W (1949) The mathematical theory of communication. University of Illinois Press, Urbana Publisher‚Äôs Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afÔ¨Åliations. Neural Computing and Applications (2023) 35:7817‚Äì7831 7831"
Neural group recommendation based on a probabilistic semantic aggregation.pdf,Neural group recommendation based on a probabilistic semantic | aggregation,Group recommender system  Collaborative Ô¨Åltering  Aggregation models  Deep learning,"ORIGINAL ARTICLE Neural group recommendation based on a probabilistic semantic aggregation Jorge DuenÀúas-Lerƒ±¬¥n 2,3 ‚Ä¢ Rau¬¥l Lara-Cabrera 1,2 ‚Ä¢ Fernando Ortega 1,2 ‚Ä¢ Jesu¬¥s Bobadilla 1,2 Received: 28 July 2022 / Accepted: 13 February 2023 / Published online: 22 March 2023  The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023 Abstract Recommendation to groups of users is a challenging subÔ¨Åeld of recommendation systems. Its key concept is how and where to make the aggregation of each set of user information into an individual entity, such as a ranked recommendation list, a virtual user, or a multi-hot input vector encoding. This paper proposes an innovative strategy where aggregation is made in the multi-hot vector that feeds the neural network model. The aggregation provides a probabilistic semantic, and the resulting input vectors feed a model that is able to conveniently generalize the group recommendation from the individual predictions. Furthermore, using the proposed architecture, group recommendations can be obtained by simply feedforwarding the pre-trained model with individual ratings; that is, without the need to obtain datasets containing group of user information, and without the need of running two separate trainings (individual and group). This approach also avoids maintaining two different models to support both individual and group learning. Experiments have tested the proposed architecture using three representative collaborative Ô¨Åltering datasets and a series of baselines; results show suitable accuracy improvements compared to the state of the art. Keywords Group recommender system  Collaborative Ô¨Åltering  Aggregation models  Deep learning 1 Introduction Personalization is one of the Ô¨Åelds of ArtiÔ¨Åcial Intelligence (AI) that has a greater impact on the lives of individuals. We can Ô¨Ånd a multitude of services that provide us with a personalized choice of news, videos, songs, restaurants, clothes, travels, etc. The most relevant tech companies make extensive use of personalization services: Amazon, NetÔ¨Çix, Spotify, TripAdvisor, Google, TikTok, etc. These companies generate their personalized recommendations using Recommender System (RS) [ 1 ] applications. Rec- ommender System (RS) provides to their users personal- ized products or services (items) by Ô¨Åltering the most relevant information regarding the logs of items consumed by the users, the time and place that took place, as well as the existing information about users, their social networks, and the content of items (texts, pictures, videos, etc.). We can classify Recommender System (RS) attending to their Ô¨Åltering strategy as demographic [ 2 ], content-based [ 3 ], context-aware [ 4 ], social [ 5 ], Collaborative Filtering (CF) [ 6 , 7 ] and Ô¨Åltering ensembles [ 8 , 9 ]. Currently, the Matrix Factorization (MF) [ 10 ] machine learning model is used to obtain accurate and fast recommendations between input data (votes). Matrix Factorization (MF) translates the very sparse and huge matrix of discrete votes (from users to items) into two dense and relatively small matrices of real values. One of the matrices contains the set of short and dense vectors representing users, whereas the second matrix vectors represent items. Each vector element (real Jorge DuenÀúas-Lerƒ±¬¥n, Rau¬¥l Lara-Cabrera, Fernando Ortega and Jesu¬¥s Bobadilla have contributed equally to this work. & Jorge DuenÀúas-Lerƒ±¬¥n jorgedl@alumnos.upm.es Rau¬¥l Lara-Cabrera raul.lara@upm.es Fernando Ortega fernando.ortega@upm.es Jesu¬¥s Bobadilla jesus.bobadilla@upm.es 1 Departamento de Sistemas Informa¬¥ticos, Universidad Polite¬¥cnica de Madrid, Alan Turing s/n, 28031 Madrid, Spain 2 KNODIS Research Group, Universidad Polite¬¥cnica de Madrid, 28031 Madrid, Spain 3 Universidad Polite¬¥cnica de Madrid, Madrid, Spain Neural Computing and Applications (2023) 35:14081‚Äì14092 https://doi.org/10.1007/s00521-023-08410-6 (0123456789().,-volV) (0123456789(). ,- volV) value) is called the ‚Äò hidden factor value ‚Äô, since it represents some complex and unknown relationship between the input data (votes). However, Matrix Factorization (MF) machine learning models are fast and accurate, and they also present a remarkable drawback: They cannot detect, in their hidden factors, the complex nonlinear relationships between the input data. Neural Network (NN) can solve this problem through their nonlinear activation functions. Neural Net- work (NN)-based Recommender System (RS) [ 2 , 11 ] makes a compression of information by coding the patterns of the rating matrix in their embeddings and hidden lay- ers [ 12 ]. These embeddings play the role of the Matrix Factorization (MF) hidden factors, enriching the result by incorporating non-linear relations. The most well-known Neural Network (NN) base Recommender System (RS) approaches are Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) [ 13 ]. Group Recommendation (GR) [ 7 , 14 ] is a subÔ¨Åeld of the Recommender System (RS) area where recommendations are made to sets of users instead of to individual users (e.g., to recommend a movie to a group of three friends). As in the regular Recommender System (RS), the goal is to make accurate recommendations to the group. In this case, sev- eral policies can be followed; the most popular are (a) to minimize the mean accuracy error: to recommend the items that, on average, most like to all the group members, and (b) to minimize the maximum accuracy error: to recom- mend the items that does not excessively dislike to any of the group members. It is important to state that there are not open datasets containing group information to be used by group recommendation models; for this reason, gener- ally randomly generated groups are used for training and testing research models. Regardless of the machine learning approach used to implement Group Recommendation (GR) Recommender System (RS), the most notable design concept is to estab- lish where to locate the aggregation stage to convert indi- vidual information to group information. The general rule is the sooner the aggregation stage, the better the perfor- mance of Group Recommendation (GR) [ 14 ]. There are three different locations where group information can be aggregated into a uniÔ¨Åed group entity: (a) before the model, (b) in the model, and (c) after the model. The most intuitive approach is to combine individual recommenda- tions into a uniÔ¨Åed group recommendation (option c) [ 15 ]. This approach is known as Individual Preference Aggre- gation (IPA) and requires processing several individual recommendations followed by rank aggregation. However, the process is slow and not particularly accurate. On the other hand, to consider the entire group for the recom- mendation, we should work before or inside the model (options a or b). These approaches are known as Group Preference Aggregation (GPA). Aggregating group infor- mation before the model requires working with the user- item interaction matrix in a higher-dimensional space, which can lead to misinformation problems. To aggregate group information in the model, we need to work with the user‚Äòs hidden vector in the low-dimensional space. Aggregating several hidden vectors from individual users into a uniÔ¨Åed virtual user hidden vector [ 16 ] avoids compute the model predictions several times and makes the rank aggregation stage unnecessary. In addition, it takes advantage of operating with condensed information com- ing from the Matrix Factorization (MF) compression of information: the virtual user can be obtained simply by averaging the representative short and dense vectors of the users group; this is efÔ¨Åcient and accurate. An interesting question is can Neural Network (NN) operate the same way that Matrix Factorization (MF) does to obtain virtual users and generate recommendations? First, note that many Neural Network (NN)-based Recommender System (RS) models compress the user embedding in a different latent space than the item embedding, and it can be a problem; then, the Neural Network (NN) nonlinear ensemble repre- sentations are more complex than the Matrix Factorization (MF) hidden factor representations; consequently, simply averaging the ensembles of the users in the group does not automatically ensure a representative virtual user embed- ding. Furthermore, model-based aggregations (option ‚Äòb‚Äô in the previous paragraph) are model dependent, and then, it is necessary to design and test different solutions for dif- ferent Neural Network (NN)-based Recommender System (RS) models, whereas the Neural Network (NN) latent spaces are the state of the art to catch users and items relations, some other machine learning approaches have been designed, such as the use of the random walk with restart method [ 17 ] providing a framework to relate users, items, and groups, and to exploit the item content and the proÔ¨Åles of the users. A three-stage method [ 18 ] is proposed to increase the precision and fairness of Group Recom- mendation (GR), where binary Matrix Factorization (MF), graphs and the dynamic consensus model are processed sequentially. Some relevant and current GR research aims to make use of the concept of member preference (inÔ¨Çu- ence or expertise) concept, based on similarity and trust. The key idea is to detect the group leaders as group members that are trusted more than others and have more inÔ¨Çuence than others. In [ 19 ], fuzzy clustering and an implicit trust metric are combined to Ô¨Ånd neighborhoods. Group Recommendation (GR) based on an average strategy applied to user preference differences [ 20 ] has been com- bined with trusted social networks to correct recommen- dations. An aggregation approach for GR mimics crowd- sourcing concepts to estimate the level of expertise of group members [ 21 ]; it is implemented using parameters of 14082 Neural Computing and Applications (2023) 35:14081‚Äì14092 sensitivity and speciÔ¨Åcity. The impact of social factors on a Group Recommendation (GR) computational model is evaluated in [ 22 ], using the expertise factor, the inÔ¨Çuences of personality, preference similarities, and interpersonal relationships. In this paper, we present how we can generate Group Recommendation (GR) using Neural Network (NN)-based Recommender System (RS) by training the model using raw Collaborative Filtering (CF) data (i.e., the ratings of individual users to the items without any additional infor- mation). The Group Recommendation (GR) have been tested using the two most popular implementations of Neural Network (NN)-based Recommender System (RS): Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP). As stated previously, to make recom- mendations to groups using Neural Network (NN)-based Recommender System (RS), information of the individual users must be aggregated. The chosen information aggre- gation design is to merge the users of the group in the input vector that feeds the user embedding of the Neural Net- work (NN). This aggregation design is not novel, since it has been used by [ 23 ] applied to a Multi-Layer Perceptron (MLP) architecture. However, our approach combines several innovative aspects in comparison with the state of the art. On the one hand, the aggregation of the users in the group is a probabilistic function rather than a simple multi- hot encoding [ 23 ]; this better captures the relative impor- tance of users in the input vector that feeds the Neural Network (NN), moreover: This aggregation approach serves as front-end for any Neural Network (NN) Group Recommendation (GR) model. On the other hand, we propose the use of a simple Recommender System (RS) Neural Network (NN) model (Generalized Matrix Factor- ization (GMF)) instead of the deepest Multi-Layer Per- ceptron (MLP) one [ 23 ]; the hypothesis is that complex models overÔ¨Åt Group Recommendation (GR) scenarios, since they are designed to accurately predict individual predictions, whereas Group Recommendation (GR) must satisfy an average of the tastes in the group of users, that is, Group Recommendation (GR) should be designed to gen- eralize the set of individual tastes in the group. Further- more, the proposed architecture just needs a single training to provide both individual recommendations and group recommendations; particularly, the model is trained by only using individual recommendations (as in regular Recommender System (RS)). Once the model is trained to return individual predictions, we can Ô¨Åll the input vector by aggregating all the users in the group, then feedforward the trained model and Ô¨Ånally obtain the recommendation for the group of users. Anyway, the impact of these innovative aspects can be evaluated in Sect. 3 , where we empirically compare the proposed aggregation designs with respect to the main baseline [ 23 ] In summary, the Group Recommendation (GR) state of the art presents the following drawbacks: (a) Some research relies on additional data to the Collaborative Fil- tering (CF) ratings, such as trust or reputation information that is not available on the majority of datasets, (b) differ- ent proposals make the aggregation of individual users before (Individual Preference Aggregation (IPA)) or after (Ranking) the model, making it impossible to beneÔ¨Åt from the machine learning model inner representations (Group Preference Aggregation (GPA)), and (c) The proposed GR neural model solutions tend to apply architectures designed to make individual recommendations, rather than group ones; this leads to the model overÔ¨Åtting and to a low scalability referred to the number of users in a group. To Ô¨Åll the gap, our proposed model: (a) Acts exclusively on Collaborative Filtering (CF) ratings, (b) Makes user aggregation in the model, and (c) Its model depth and design enable adequate learning generalizations. Addi- tionally, the provided experiments test the proposed model according to different aggregation strategies to set the group labels used in the learning stage. In contrast, a notable limitation of our architecture and the experiments is the lack of testing on particularly demanding scenarios such as cold start in groups users, extremely sparse data sets, impact of popular item bias, and fear Group Recom- mendation (GR). The rest of the paper is structured as follows: Sect. 2 introduces the tested models and aggregation functions; Sect. 3 describes the experiment design, the selected quality measures, the chosen datasets and shows the results obtained; Sect. 4 provides their explanations; and Sect. 5 highlights the main conclusions of the article and the suggested future work. 2 Proposed model In Collaborative Filtering (CF), interactions (purchase, viewing, rating, etc.) between users and items are stored in a sparse matrix since it is common for users to interact only with a small proportion of the available items and, in the same way, only a small percentage of existing users interact with the items. The sparsity levels of this matrix are around 95‚Äì98% as shown in Table 2 . To handle this sparsity, current Collaborative Filtering (CF) models based on Neural Network (NN) [ 13 ] work with a projection of users and items into a low-dimensional latent space using embedding layers. Embedding layers are a very popular type of layer used in Neural Network (NN) that receive as input any entity and return a vector with a low-dimensional representation of the entity in a latent space. These vectors are commonly named latent factors . In order to transform the entity into its low-dimensional representation, the Neural Computing and Applications (2023) 35:14081‚Äì14092 14083 embedding layer Ô¨Årst transforms the entity into a one hot encoding representation (typically using a hash function). Figure 1 summarizes this process. In the context of Collaborative Filtering (CF), two embedding layers are required: one for the users and the other for the items. Later, both embedding layers are combined using a Neural Network (NN) architecture (see Fig. 2 ). For example, the models Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) use a dot layer and a concatenate layer followed by some fully connected dense layers as architectures, respectively. Formally, we deÔ¨Åne a Neural Network (NN) model U that predicts the rating that a user u will give to an item i ( ^ r u ; i ) combining the latent factors provided by the embed- ding layer (Emb L ) of the user u ( l u ~ ) and the item i ( l i ~ ): Emb L √∞ u √û ¬º l u ~ Emb L √∞ i √û ¬º l i ~ ; U √∞ l ~ u ; l ~ i √û ¬º ^ r u ; i √∞ 1 √û As stated in Sect. 1 , when working with Group Recom- mendation (GR), a straightforward strategy is Individual Preference Aggregation (IPA) [ 24 ]. This strategy makes a prediction for each member of the group and then performs an aggregation. This strategy does not treat the group as a whole. If we have a group of users G ¬º f u 1 ; u 2 ; :::; u n g , the prediction of the rating of this group G to an item i ( ^ r G ; i ) is computed as the average value of the individual predictions: ^ r G ; i ¬º 1 k G k X u 2 G ^ r u ; i ¬º 1 k G k X u 2 G U √∞ l u ~ ; l i ~ √û √∞ 2 √û On the other hand, the Group Preference Aggregation (GPA) strategies take into account the group as a whole. It should be noted that the order of users within the group and the length of it should not affect the aggregation; thus, the aggregations should meet the constraints of permutation invariant and Ô¨Åxed result length [ 23 ]. Our goal with the Group Preference Aggregation (GPA) strategy is to be able to obtain a prediction ^ r G ; i with a single forward propagation and to treat the group as a whole entity. We can achieve this by aggregating the latent factors of each user that belongs to the group to obtain the latent factor of the group l ~ G . Once the latent factors of the group are aggregated, the model U can be used to compute the predictions: Emb L √∞ G √û ¬º l ~ G ^ r Gi ¬º U √∞ l ~ G ; l ~ i √û √∞ 3 √û The aggregation of group latent factors in embedding layers can be achieved by modifying the input of the Neural Network (NN). As mentioned previously, embed- ding layers have as input a one hot representation of the entities. This approach is adequate when performing indi- vidual predictions, however, for group recommendations, we need to apply a multi-hot representation to the users‚Äô embedding layer, i.e., we encode the group by setting multiple inputs of the user embedding layer (the inputs related with the users that belong to the group) to a value higher than 0. This encoding allows us to take into account all group users at the same time for the extraction of latent factors of the group l ~ G . The simplest aggregation, which is used by the DeepGroup model [ 23 ], is to use as input for embedding a constant value proportional to the size of the group. We deÔ¨Åne the input of the user‚Äôs embedding layer for the user u as EmbeddingInput Average √∞ u √û ¬º 1 k G k if u 2 G 0 if u 62 G 8 < : √∞ 4 √û We call this aggregation ‚Äò Average ‚Äô since the embedding layer will generate the group latent factor equal to the average of the latent factors of all users in the group. Recommender System (RS) can give better predictions the more information they have about users, so to take advantage of this fact, we have tested the ‚Äò Expertise ‚Äô aggregation in which we give a weight to the users pro- portional to the number of votes they have entered into the system. Let k R u k the number of ratings of the user u , the input of the users‚Äô embedding layer for the user u is deÔ¨Åned as Fig. 1 Embedding layer schema Fig. 2 Collaborative Ô¨Åltering-based neural network model 14084 Neural Computing and Applications (2023) 35:14081‚Äì14092 EmbeddingInput Expertise √∞ u √û ¬º k R u k P g 2 G k R g k if u 2 G 0 if u 62 G 8 < : √∞ 5 √û In addition to the ‚Äò Expertise ‚Äô aggregation, we also pro- posed the ‚Äò Softmax aggregation as a smooth version of the ‚Äò Expertise ‚Äô aggregation. In this case, the input of the users‚Äô embedding layer for the user u is deÔ¨Åned as EmbeddingInput Softmax √∞ u √û ¬º e k R u k P g 2 G e k R g k if u 2 G 0 if u 62 G 8 > < > : √∞ 6 √û In Fig. 3 , we can see where the equations Ô¨Åt in the group recommendation process. The Ô¨Årst step is to generate the multi-hot vector with some of the described aggregation (Eqs. 4 , 5 , 6 ). This vector (multi-hot representation of the group) is fed into the embedding layer to obtain a vector of the latent factors of the groups l ~ G (Eq. 3 ). Once the latent Fig. 3 Graphical representation of the proposed model Table 1 Complete aggregation example (a) Rating count. User . u 13 . u 24 . u 30 . u 42 . #Rating 2 5 6 3 (b) Input values to the users‚Äô embedding layer. Strategy/ User . u 13 . u 24 . u 30 . u 42 . Average 0,25 0,25 0,25 0,25 Expertise 0,13 0,31 0,38 0,19 Softmax 0,01 0,26 0,70 0,03 Users‚Äô latent factors assuming a latent space of size 3. User/factor l 1 l 2 l 3 u 13 0,1 0,6 0,3 u 24 0,7 0,2 0,9 u 30 0,8 0,4 0,1 u 42 0,5 0,7 0,8 Group latent factors using different aggregations Agg factor l G 1 l G 2 l G 3 Average 0,525 0,475 0,525 Expertise 0,629 0,425 0,508 Softmax 0,758 0,359 0,331 Neural Computing and Applications (2023) 35:14081‚Äì14092 14085 factors of the group and the item are obtained, they are used to feed the model U (Generalized Matrix Factoriza- tion (GMF) or Multi-Layer Perceptron (MLP)) and produce the rating prediction for the group G on the item i (Eq. 2 ). In Table 1, we can Ô¨Ånd an example with some users (13, 24, 30 and 42) with different rating counts (Table 1 a), their input values to the users‚Äô embedding layer in a multi-hot fashion (Table 1 b), their individual latent factors (Table 1 c), and the Ô¨Ånal group latent factors with different aggregations (Table 1 d). 3 Experimental evaluation In this section, we show the experiments carried out to validate the aggregation proposed in this manuscript. As previously stated, the experiments have been performed using the most popular Neural Network (NN)-based Rec- ommender System (RS) architectures: Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP). We have chosen these two architectures because they are the best known and offer the best results for individual predictions. However, the aggregation strategies proposed can be applied to any Neural Network (NN) architecture based on embedding layers. The choice of datasets has been made considering that (a) there are no open datasets containing information on group voting; and (b) Generalized Matrix Factorization (GMF) and Multi-Layer Perceptron (MLP) models should be trained using individual voting, since the proposed aggregations allow computing predictions for groups on already trained models. For these reasons, we have chosen the following gold standard datasets in the Ô¨Åeld of Rec- ommender System (RS): MovieLens1M [ 25 ], the most popular dataset in the research of Recommender System (RS); FilmTrust [ 26 ], a dataset smaller than MovieLens1M to measure the performance of the aggregation in datasets with a lower number of users, items, and ratings; and MyAnimeList , a dataset with a range of votes higher than the MovieLens1M . Other popular datasets such as NetÔ¨Çix Prize or MovieLens10M have not been selected due to the high computational time required to train and test the models. The main parameters of the selected datasets can be found in Table 2 . The generation of synthetic groups has been carried out in such a way that all groups have voted at least Ô¨Åve items in test. In this way, it is possible to evaluate both the quality of the predictions and the quality of the recom- mendations to the groups as detailed below. Groups of different sizes (from 2 to 10 users) have been generated. For each group size, 10000 synthetic groups have been generated. The generation of a group has been carried out following the following algorithm: 1. DeÔ¨Åne the size of the group S . 2. Random select 5 items rated in test by at least S users. 3. Find all users who have rated the 5 items selected in 2. 4. If we found fewer than S users, go back to 2. Otherwise, random select S users and create a group. To measure the quality of the predictions for the group, we have calculated the mean absolute error MAE ¬º 1 # groups X G 1 k G k  k I G k X u 2 G X i 2 I G j ^ r G ; i  r u ; i j ; √∞ 7 √û the mean squared error MSE ¬º 1 # groups X G 1 k G k 1 k G k  k I G k X u 2 G X i 2 I G ^ r G ; i  r u ; i   2 ; √∞ 8 √û and mean maximum group error MAX ¬º 1 # groups X G max u 2 G max i 2 I G j ^ r G ; i  r u ; i j ; √∞ 9 √û where I G denotes the items rated by the group G To measure the quality of the recommendations for the group, we have calculated the Normalized Discounted Cumulative Gain (NDCG) score NDCG @ N ¬º 1 # groups X G DCG G @ N IDCG G @ N ; √∞ 10 √û DCG G @ N ¬º X i 2 X N G  r G ; i log 2 √∞ pos G √∞ i √û √æ 1 √û ; √∞ 11 √û IDCG G @ N ¬º X i 2 T N G  r G ; i log 2 √∞ ipos G √∞ i √û √æ 1 √û ; √∞ 12 √û where N is the number of items recommended to the group (in our experiments N ¬º 5 according to the generation of synthetic groups), X N G is the set of N items recommended to the group G , pos G √∞ i √û is the position of the item i in the group‚Äôs G recommendation list, T N G is the set of the top N items for the group G , ipos G √∞ i √û is the ideal rank of the item Table 2 Main parameters of the datasets used in the experiments Dataset #users #items #ratings Scores Sparsity Movie lens1M 6040 3706 911,031 1‚Äì5 95.94 Filmtrust 1508 2071 35,497 0‚Äì5 87.98 My anime list 19,179 2692 548,967 1‚Äì10 98.94 14086 Neural Computing and Applications (2023) 35:14081‚Äì14092 Table 3 Mean absolute error Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.74205 (0.409) 0.76075 (0.341) 0.76893 (0.299) 0.77009 (0.271) 0.77745 (0.249) 0.77659 (0.234) 0.77681 (0.221) 0.77599 (0.212) 0.77558 (0.201) GMF Expertise 0.74393 (0.41) 0.76207 (0.341) 0.77018 (0.299) 0.77155 (0.27) 0.779 (0.249) 0.77782 (0.234) 0.77834 (0.221) 0.77729 (0.211) 0.77685 (0.201) GMF Softmax 0.74246 (0.409) 0.7608 (0.341) 0.76891 (0.299) 0.77012 (0.27) 0.77751 (0.249) 0.7766 (0.234) 0.77687 (0.221) 0.77602 (0.212) 0.77562 (0.201) MLP IPA 0.74956 (0.444) 0.77342 (0.361) 0.78055 (0.313) 0.78211 (0.28) 0.78853 (0.258) 0.78633 (0.241) 0.78678 (0.228) 0.78591 (0.219) 0.78509 (0.207) MLP Avg DeepGroup 0.75537 (0.275) MLP Expertise 0.72596 (0.486) 0.74432 (0.405) 0.75031 (0.355) 0.75132 (0.32) 0.75787 (0.295) 0.75607 (0.276) 0.75709 (0.261) 0.75481 (0.25) 0.755 (0.238) MLP Softmax 0.72407 (0.485) 0.74297 (0.404) 0.74925 (0.355) 0.75019 (0.32) 0.75669 (0.295) 0.7556 (0.261) 0.75389 (0.25) 0.75361 (0.238) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.61552 (0.451) 0.71033 (0.32) 0.73011 (0.28) 0.73419 (0.252) 0.73606 (0.232) 0.73832 (0.217) 0.74045 (0.203) 0.74298 (0.193) 0.74212 (0.185) GMF Expertise 0.6149 (0.444) 0.71239 (0.319) 0.73144 (0.281) 0.73583 (0.252) 0.73742 (0.232) 0.7396 (0.217) 0.74166 (0.203) 0.74418 (0.193) 0.74336 (0.184) GMF Softmax 0.61512 (0.448) 0.71088 (0.319) 0.73035 (0.28) 0.73445 (0.252) 0.73624 (0.232) 0.73847 (0.217) 0.74057 (0.203) 0.74309 (0.193) 0.74222 (0.185) MLP IPA 0.58165 (0.442) 0.70368 (0.325) 0.72062 (0.281) 0.7252 (0.252) 0.72788 (0.232) 0.73073 (0.217) 0.73353 (0.203) 0.73623 (0.193) 0.73525 (0.185) MLP Avg DeepGroup 0.58199 (0.447) MLP Expertise 0.70449 (0.321) 0.71891 (0.276) 0.72315 (0.247) 0.72581 (0.228) 0.72865 (0.213) 0.73127 (0.2) 0.73429 (0.191) 0.73336 (0.183) MLP Softmax 0.58063 (0.45) 0.70226 (0.319) 0.71734 (0.275) 0.72153 (0.247) 0.72443 (0.228) 0.7272 (0.214) 0.72993 (0.2) 0.73295 (0.191) 0.73202 (0.183) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.95819 (0.477) 0.97595 (0.417) 0.99149 (0.38) 1.0017 (0.346) 1.01404 (0.329) 1.01942 (0.312) 1.022 (0.297) 1.02445 (0.282) GMF Expertise 0.93675 (0.572) 0.96424 (0.48) 0.98092 (0.419) 0.99603 (0.382) 1.00649 (0.349) 1.01805 (0.331) 1.02352 (0.314) 1.0258 (0.3) 1.0284 (0.284) GMF Softmax 0.93219 (0.568) 0.95848 (0.477) 0.97559 (0.417) 0.99104 (0.38) 1.00133 (0.346) 1.01363 (0.329) 1.0191 (0.312) 1.02173 (0.297) 1.02422 (0.282) MLP IPA 0.95479 (0.585) 0.98155 (0.484) 0.99709 (0.42) 1.00977 (0.381) 1.01745 (0.347) 1.02794 (0.329) 1.03188 (0.311) 1.03335 (0.298) 1.03451 (0.282) MLP Avg DeepGroup 0.93161 (0.618) 0.95609 (0.515) 0.98456 (0.408) 0.99331 (0.371) 1.0052 (0.351) 1.00857 (0.332) 1.01039 (0.317) 1.01233 (0.3) MLP Expertise 0.93803 (0.622) 0.96132 (0.517) 0.97587 (0.453) 0.98923 (0.409) 0.99851 (0.373) 1.00879 (0.353) 1.01258 (0.334) 1.01493 (0.319) 1.01599 (0.302) MLP Softmax 0.93351 (0.619) 0.97007 (0.451) Neural Computing and Applications (2023) 35:14081‚Äì14092 14087 Table 4 Mean squared error Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.92437 (0.676) 0.92648 (0.609) 0.94191 (0.567) 0.93896 (0.53) 0.9376 (0.5) 0.93832 (0.48) 0.93571 (0.454) GMF Expertise 0.88044 (0.918) 0.91329 (0.77) 0.92754 (0.678) 0.92979 (0.61) 0.94504 (0.568) 0.94135 (0.531) 0.93994 (0.501) 0.94037 (0.481) 0.93737 (0.454) GMF Softmax 0.87614 (0.914) 0.90952 (0.767) MLP IPA 0.94301 (0.982) 0.96557 (0.814) 0.96809 (0.712) 0.96499 (0.635) 0.97727 (0.591) 0.96889 (0.55) 0.96634 (0.518) 0.96648 (0.498) 0.96187 (0.47) MLP Avg DeepGroup 0.99415 (1.037) 1.03182 (0.875) 1.04278 (0.779) 1.04242 (0.695) 1.05597 (0.651) 1.05056 (0.605) 1.04927 (0.574) 1.04836 (0.552) 1.04669 (0.524) MLP Expertise 0.9986 (1.038) 1.03522 (0.88) 1.04511 (0.78) 1.04435 (0.696) 1.05806 (0.651) 1.05121 (0.604) 1.05084 (0.575) 1.0491 (0.553) 1.04787 (0.523) MLP Softmax 0.99487 (1.035) 1.03145 (0.874) 1.04258 (0.779) 1.04235 (0.694) 1.05605 (0.651) 1.05034 (0.604) 1.04925 (0.574) 1.04822 (0.552) 1.04659 (0.524) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.67941 (1.077) 0.7889 (0.688) 0.82014 (0.608) 0.82302 (0.544) 0.82574 (0.502) 0.83038 (0.469) 0.8324 (0.44) 0.8375 (0.42) 0.83544 (0.401) GMF Expertise 0.67314 (1.05) 0.79105 (0.686) 0.82229 (0.608) 0.82546 (0.543) 0.82758 (0.501) 0.83205 (0.468) 0.83382 (0.439) 0.83875 (0.418) 0.83673 (0.399) GMF Softmax 0.67596 (1.063) 0.7892 (0.687) 0.82043 (0.608) 0.82333 (0.544) 0.82592 (0.502) 0.83053 (0.469) 0.83251 (0.44) 0.83758 (0.42) 0.83552 (0.401) MLP IPA 0.77197 (0.683) 0.79514 (0.592) 0.7988 (0.528) 0.80315 (0.487) 0.80807 (0.454) 0.81084 (0.427) 0.81611 (0.406) 0.81408 (0.388) MLP Avg DeepGroup 0.61859 (1.009) MLP Expertise 0.62484 (0.979) 0.7691 (0.68) 0.78956 (0.586) 0.79332 (0.522) 0.79799 (0.483) 0.80305 (0.45) 0.80595 (0.423) 0.81166 (0.403) 0.80977 (0.385) MLP Softmax 0.62106 (0.992) 0.7649 (0.675) 0.78709 (0.585) 0.79116 (0.523) 0.79625 (0.484) 0.80142 (0.452) 0.80456 (0.424) 0.81033 (0.404) 0.8085 (0.387) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg GMF Expertise 1.49874 (1.963) 1.57277 (1.689) 1.61625 (1.463) 1.66444 (1.351) 1.69199 (1.239) 1.73293 (1.202) 1.74812 (1.133) 1.75649 (1.087) 1.76487 (1.035) GMF Softmax 1.47707 (1.932) 1.54383 (1.654) 1.58617 (1.433) 1.63575 (1.326) 1.66342 (1.214) 1.70677 (1.179) 1.72239 (1.112) 1.73223 (1.066) 1.74092 (1.016) MLP IPA 1.56623 (1.959) 1.61737 (1.655) 1.64909 (1.431) 1.69132 (1.32) 1.71029 (1.209) 1.74683 (1.169) 1.75711 (1.105) 1.76368 (1.058) 1.76877 (1.008) MLP Avg DeepGroup 1.61669 (2.032) 1.68103 (1.718) 1.71103 (1.492) 1.75681 (1.368) 1.77828 (1.254) 1.81759 (1.217) 1.82764 (1.148) 1.83403 (1.098) 1.84177 (1.049) MLP Expertise 1.64412 (2.059) 1.70965 (1.747) 1.74467 (1.517) 1.78446 (1.386) 1.80671 (1.276) 1.84221 (1.232) 1.85095 (1.163) 1.85803 (1.116) 1.86351 (1.063) MLP Softmax 1.62458 (2.038) 1.68445 (1.723) 1.7153 (1.496) 1.75838 (1.368) 1.78024 (1.256) 1.81848 (1.218) 1.82855 (1.149) 1.83446 (1.099) 1.84249 (1.049) 14088 Neural Computing and Applications (2023) 35:14081‚Äì14092 Table 5 Mean max error Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 1.35658 (0.588) 1.59908 (0.577) 1.64979 (0.573) 1.69807 (0.576) 1.73598 (0.571) GMF Expertise 1.02474 (0.602) 1.22441 (0.599) 1.35928 (0.59) 1.4551 (0.584) 1.54414 (0.579) 1.60112 (0.577) 1.65118 (0.574) 1.69926 (0.576) 1.7366 (0.571) GMF Softmax 1.02191 (0.6) 1.22143 (0.598) 1.45202 (0.583) 1.54118 (0.578) MLP IPA 1.04098 (0.642) 1.25715 (0.614) 1.38214 (0.602) 1.47972 (0.591) 1.56516 (0.586) 1.62067 (0.581) 1.66905 (0.576) 1.71687 (0.58) 1.7528 (0.573) MLP Avg DeepGroup 1.07144 (0.666) 1.28743 (0.643) 1.41937 (0.635) 1.51234 (0.633) 1.59812 (0.638) 1.65421 (0.642) 1.70673 (0.641) 1.75552 (0.647) 1.79703 (0.645) MLP Expertise 1.07476 (0.667) 1.28932 (0.644) 1.42078 (0.635) 1.51325 (0.634) 1.59883 (0.638) 1.65371 (0.64) 1.70637 (0.64) 1.75428 (0.645) 1.79566 (0.643) MLP Softmax 1.07226 (0.666) 1.28716 (0.643) 1.41906 (0.635) 1.51227 (0.633) 1.59788 (0.638) 1.65391 (0.641) 1.7066 (0.641) 1.75515 (0.646) 1.79669 (0.645) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.85192 (0.555) 1.13757 (0.573) 1.27696 (0.573) 1.36783 (0.57) 1.44244 (0.571) 1.51262 (0.575) 1.56818 (0.573) 1.62204 (0.571) 1.6636 (0.567) GMF Expertise 0.85171 (0.549) 1.13845 (0.571) 1.27815 (0.571) 1.36895 (0.567) 1.4425 (0.569) 1.51199 (0.573) 1.56717 (0.57) 1.62058 (0.569) 1.66146 (0.565) GMF Softmax 0.85147 (0.552) 1.13761 (0.572) 1.27708 (0.572) 1.3679 (0.569) 1.44234 (0.571) 1.51245 (0.575) 1.56799 (0.572) 1.62184 (0.571) 1.66335 (0.567) MLP IPA 1.11519 (0.564) 1.24935 (0.56) 1.33899 (0.555) 1.41348 (0.556) 1.48001 (0.557) 1.5341 (0.554) 1.58622 (0.552) 1.62791 (0.549) MLP Avg DeepGroup 0.79205 (0.553) 1.33662 (0.555) 1.4118 (0.557) 1.47902 (0.559) 1.53438 (0.556) 1.58697 (0.555) 1.62916 (0.55) MLP Expertise 0.79191 (0.56) 1.11441 (0.556) 1.24814 (0.555) 1.33715 (0.551) MLP Softmax 0.79235 (0.555) 1.11261 (0.556) 1.24698 (0.556) 1.41161 (0.556) 1.47872 (0.558) 1.53406 (0.555) 1.58662 (0.554) 1.62879 (0.55) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg GMF Expertise 1.31784 (0.858) 1.60964 (0.883) 1.8266 (0.882) 1.99902 (0.894) 2.13631 (0.891) 2.25748 (0.91) 2.34706 (0.908) 2.42357 (0.922) 2.49875 (0.932) GMF Softmax 1.30626 (0.847) 1.59061 (0.869) 1.80139 (0.867) 1.97309 (0.879) 2.10896 (0.878) 2.23046 (0.898) 2.31946 (0.896) 2.39701 (0.911) 2.47222 (0.92) MLP IPA 1.34151 (0.874) 1.63805 (0.873) 1.84317 (0.862) 2.01188 (0.868) 2.1406 (0.863) 2.25649 (0.876) 2.33763 (0.875) 2.4112 (0.889) 2.4836 (0.896) MLP Avg DeepGroup 1.36617 (0.893) 1.66385 (0.902) 1.8675 (0.896) 2.03818 (0.907) 2.17095 (0.906) 2.28936 (0.924) 2.37588 (0.921) 2.4495 (0.934) 2.52558 (0.943) MLP Expertise 1.38038 (0.9) 1.68165 (0.913) 1.89237 (0.908) 2.0611 (0.918) 2.1951 (0.914) 2.31311 (0.929) 2.39796 (0.923) 2.47081 (0.935) 2.54585 (0.943) MLP Softmax 1.37076 (0.894) 1.66676 (0.904) 1.87147 (0.898) 2.04076 (0.908) 2.17377 (0.907) 2.29111 (0.924) 2.37734 (0.921) 2.45073 (0.934) 2.52696 (0.943) Neural Computing and Applications (2023) 35:14081‚Äì14092 14089 i for the group G , and  r G ; i is the average rating of the users belonging to the group G for the item i . We can see the results of the experiment executed with these scores in Table 3 (MAE), Table 4 (MSE), Table 5 (Max), and Table 6 (NDCG). The cells with the best results Table 6 Discounted cumulative gain Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.9886 (0.015) 0.99178 (0.011) 0.99286 (0.01) 0.99358 (0.009) GMF Expertise 0.97995 (0.024) 0.98528 (0.019) 0.98853 (0.015) 0.99058 (0.012) 0.99171 (0.011) 0.99282 (0.01) 0.99351 (0.009) 0.99411 (0.008) 0.99454 (0.008) GMF Softmax 0.98006 (0.024) 0.98536 (0.019) 0.99064 (0.012) 0.99415 (0.008) 0.99456 (0.008) MLP IPA 0.97854 (0.026) 0.98342 (0.02) 0.98689 (0.016) 0.98906 (0.014) 0.99046 (0.012) 0.99178 (0.011) 0.99251 (0.01) 0.99303 (0.009) 0.99358 (0.009) MLP Avg DeepGroup 0.97778 (0.026) 0.98247 (0.021) 0.98556 (0.017) 0.98762 (0.015) 0.98894 (0.014) 0.98999 (0.013) 0.99064 (0.012) 0.99109 (0.011) 0.99152 (0.011) MLP Expertise 0.97777 (0.026) 0.98251 (0.021) 0.9855 (0.017) 0.98763 (0.015) 0.98894 (0.014) 0.99004 (0.013) 0.99071 (0.012) 0.99116 (0.011) 0.9916 (0.011) MLP Softmax 0.97784 (0.026) 0.98248 (0.021) 0.98554 (0.017) 0.98762 (0.015) 0.98895 (0.014) 0.98998 (0.013) 0.99069 (0.012) 0.9911 (0.011) 0.99153 (0.011) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.96788 (0.028) 0.97795 (0.022) 0.9815 (0.019) 0.98393 (0.016) 0.98596 (0.014) 0.9876 (0.013) 0.98869 (0.011) 0.98967 (0.011) 0.99038 (0.01) GMF Expertise 0.96795 (0.028) 0.97794 (0.022) 0.9815 (0.019) 0.98392 (0.016) 0.98593 (0.014) 0.9876 (0.013) 0.98868 (0.012) 0.98965 (0.011) 0.99038 (0.01) GMF Softmax 0.96795 (0.028) 0.97796 (0.022) 0.9815 (0.019) 0.98394 (0.016) 0.98596 (0.014) 0.9876 (0.013) 0.98868 (0.012) 0.98967 (0.011) 0.99038 (0.01) MLP IPA 0.9788 (0.023) 0.98249 (0.019) 0.98523 (0.016) 0.98704 (0.015) 0.98994 (0.011) 0.99165 (0.01) MLP Avg DeepGroup 0.96897 (0.027) 0.9789 (0.022) 0.98275 (0.018) 0.98527 (0.016) 0.98895 (0.012) 0.99104 (0.01) 0.99166 (0.009) MLP Expertise 0.9694 (0.027) 0.98276 (0.018) 0.98724 (0.014) 0.98893 (0.012) 0.98993 (0.011) 0.99106 (0.01) 0.99166 (0.009) MLP Softmax 0.9693 (0.027) 0.97891 (0.022) 0.98527 (0.016) 0.98728 (0.014) 0.98895 (0.012) 0.98998 (0.011) 0.99106 (0.01) Model \ Group Size 2 3 4 5 6 7 8 9 10 GMF IPA GMF Avg 0.98898 (0.014) 0.9949 (0.007) 0.99571 (0.006) 0.99662 (0.005) 0.99679 (0.005) 0.99714 (0.004) GMF Expertise 0.98893 (0.014) 0.99209 (0.01) 0.99383 (0.008) 0.99479 (0.007) 0.99566 (0.006) 0.99609 (0.005) 0.99658 (0.005) 0.99674 (0.005) 0.99708 (0.004) GMF Softmax 0.99217 (0.01) 0.99399 (0.008) 0.99615 (0.005) MLP IPA 0.98723 (0.015) 0.99062 (0.011) 0.99255 (0.009) 0.9936 (0.008) 0.99458 (0.007) 0.99507 (0.006) 0.99573 (0.006) 0.99591 (0.005) 0.99632 (0.005) MLP Avg DeepGroup 0.9868 (0.016) 0.99023 (0.012) 0.99212 (0.01) 0.99307 (0.008) 0.99415 (0.007) 0.99459 (0.007) 0.99519 (0.006) 0.99537 (0.006) 0.99565 (0.005) MLP Expertise 0.9867 (0.016) 0.99021 (0.012) 0.9921 (0.01) 0.99312 (0.008) 0.99414 (0.007) 0.99458 (0.007) 0.99521 (0.006) 0.99542 (0.006) 0.9957 (0.005) MLP Softmax 0.98684 (0.016) 0.99028 (0.012) 0.99211 (0.01) 0.99308 (0.008) 0.99416 (0.007) 0.9946 (0.007) 0.99521 (0.006) 0.99539 (0.006) 0.99566 (0.005) 14090 Neural Computing and Applications (2023) 35:14081‚Äì14092 have been highlighted (gray background and bold), while the standard deviation of each metric is in parentheses. All results are analyzed in Sect. 4 . All experiments have been run using an NVIDIA Quadro RTX 8000 GPU with 48 GB GDDR6 of memory, 4,608 NVIDIA Tensor Cores and a performance of 16.3 TFLOPS. We are committed to reproducible science, so the source code of all experiments with the values of the parameters used and their random seeds have been shared on GitHub. 1 4 Discussion The main goal of this research is to evaluate different aggregation techniques to make recommendations to groups. As shown in Sect. 3 , we can see different trends according to (a) the models used; (b) the way group information is aggregated; (c) the datasets on which they act; and (d) the size of the groups. Focusing on the models, we can see how Multi-Layer Perceptron (MLP), which has several hidden layers, obtains a lower MAE; however, Generalized Matrix Factorization (GMF), a simpler model, obtains a lower MSE. Although the Multi-Layer Perceptron (MLP) model has great power in these types of problem, it seems to overÔ¨Åt, generating very good recommendations for some users in the group but bad ones for the rest, hence achieving higher MSE values. On the other hand, the Generalized Matrix Fac- torization (GMF) model can obtain smaller maximum errors in each group, which means that no user in the group is badly affected by the recommendation. In the results, we can also observe how the models with higher maximum errors lead to a poorer order of items according to user preferences and obtain worse performance in the Normal- izaed Discounted Cumulative Gain (NDCG) metric. Looking at the aggregation of users, we can see that the best performing user aggregation is the average, followed by a very similar performance by the Softmax. However, the use of expert user weighting without softmax produces worse results. Based on the results, we can observe that in models that do not use a deep architecture, with several hidden layers, the Individual Preference Aggregation (IPA) and Group Preference Aggregation (GPA) strategies pro- duce similar results when the aggregation function is a linear transformation of latent factors (Generalized Matrix Factorization (GMF)). However, we can see how the nonlinearity of Multi-Layer Perceptron (MLP) produces different results between both two strategies. Regarding the different datasets, we can see that there is a clear trend in the models that achieve the best results in complex datasets with a large number of users, items, and votes, such as Movilens or MyAnimeList, while in the FilmTrust dataset, with a smaller number of votes, there is no clear trend. In terms of group size, as more users have the group, the probability of Ô¨Ånding discrepancies between user prefer- ences increases. Therefore, we can see how a larger group size leads to higher values in all error metrics. 5 Conclusions and future work With the irruption of Neural Network (NN) in the world of Collaborative Filtering (CF), the possibilities of their ability to Ô¨Ånd nonlinear patterns within user preferences to generate better predictions are opening up. To use these systems to generate a recommendation for a group of users, we need to aggregate their preferences. As we have seen in this research, there are several key points at which aggre- gation can be performed. Group Preference Aggregation (GPA) strategies do the aggregation before or inside the model, so they have the advantage of taking into account the preferences of the entire group and that a single feed- forward step generates the prediction. In contrast, the Individual Preference Aggregation (IPA) strategy, requires multiple predictions for each user and performs the aggregations after the model. In this study, we have tested how different approaches to perform Group Preference Aggregation (GPA) work in different datasets comparing different metrics. As future work, there are two key factors to consider. First, in this research, the researchers have designed user aggregation techniques presented to the models; in future work, these functions will be explored by different machine learning models. The second key point is that in this work, models perform a knowledge transfer from the model trained for individuals to make group predictions; it is shown that although the models have high performance (MAE improvement), they tend to overÔ¨Åt when working in groups (larger errors in group prediction leading to worse MSE). To solve this problem, future work will try to per- form a specialization training stage for groups after indi- vidual training. Funding This work has been co-funded by the Ministerio de Ciencia e Innovacio¬¥n of Spain and the European Regional Development Fund (FEDER) under grants PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de Madrid under Convenio Plurianual with the Uni- versidad Polite¬¥cnica de Madrid in the actuation line of Programa de Excelencia para el Profesorado Universitario . Data availability The MovieLens1M , FilmTrust and MyAni- meList dataset along with the source code of the experiments that support the Ô¨Åndings of this study is available in neural-cf-for- 1 https://github.com/KNODIS-Research-Group/neural-cf-for-groups Neural Computing and Applications (2023) 35:14081‚Äì14092 14091 groups GitHub‚Äôs repository [ https://github.com/KNODIS- Research-Group/neural-cf-for-groups ]. Declarations Conflict of interest The authors of this paper declare that they have no conflict of interest. References 1. Batmaz Z, Yurekli A, Bilge A, Kaleli C (2019) A review on deep learning for recommender systems: challenges and remedies. Artif Intell Rev 52(1):1‚Äì37. https://doi.org/10.1007/s10462-018- 9654-y 2. Bobadilla J, Gonza¬¥lez-Prieto A ¬¥ , Ortega F, Lara-Cabrera R (2021) Deep learning feature selection to unhide demographic recom- mender systems factors. Neural Comput Appl 33(12):7291‚Äì7308. https://doi.org/10.1007/s00521-020-05494-2 3. Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender systems leveraging multimedia content. ACM Comput Surv 53(5):1‚Äì38. https://doi.org/10.1145/3407190 4. Kulkarni S, Rodd SF (2020) Context aware recommendation systems: a review of the state of the art techniques. Comput Sci Rev 37:100255. https://doi.org/10.1016/j.cosrev.2020.100255 5. Shokeen J, Rana C (2020) A study on features of social recom- mender systems. Artif Intell Rev 53(2):965‚Äì988. https://doi.org/ 10.1007/s10462-019-09684-w 6. Bobadilla J, Alonso S, Hernando A (2020) Deep learning archi- tecture for collaborative Ô¨Åltering recommender systems. Appl Sci 10(7):2441. https://doi.org/10.3390/app10072441 7. Dara S, Chowdary CR, Kumar C (2020) A survey on group recommender systems. J Intell Inf Syst 54(2):271‚Äì295. https:// doi.org/10.1007/s10844-018-0542-3 8. Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of a recommender system with ensemble learning and graph embedding: a case on movielens. Multimed Tools Appl 80(5):7805‚Äì7832. https://doi.org/10.1007/s11042-020-09949-5 9. C¬∏ ano E, Morisio M (2017) Hybrid recommender systems: a systematic literature review. Intell Data Anal 21(6):1487‚Äì1524. https://doi.org/10.3233/IDA-163209 10. Salakhutdinov R, Mnih A (2007) Probabilistic matrix factoriza- tion. In: Proceedings of the 20th international conference on neural information processing systems. NIPS‚Äô07, pp. 1257‚Äì1264. Curran Associates Inc., Red Hook, NY, USA. https://doi.org/10. 5555/2981562.2981720 11. Bobadilla J, Gonza¬¥lez-Prieto A ¬¥ , Ortega F, Lara-Cabrera R (2022) Deep learning approach to obtain collaborative Ô¨Åltering neigh- borhoods. Neural Comput Appl 34(4):2939‚Äì2951. https://doi.org/ 10.1007/s00521-021-06493-7 12. Huang T, Zhang D-f, Bi L (2020) Neural embedding collabora- tive Ô¨Åltering for recommender systems. Neural Comput Appl 32:1‚Äì15. https://doi.org/10.1007/s00521-020-04920-9 13. He X, Liao L, Zhang H, Nie L, Hu X, Chua T-S (2017) Neural collaborative Ô¨Åltering. In: Proceedings of the 26th international conference on world wide web. WWW ‚Äô17, pp. 173‚Äì182. Inter- national World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE. https://doi.org/10.1145/ 3038912.3052569 14. Ortega F, Bobadilla J, Hernando A, Gutie¬¥Rrez A (2013) Incor- porating group recommendations to recommender systems: alternatives and performance. Inf Process Manage 49(4):895‚Äì901. https://doi.org/10.1016/j.ipm.2013.02.003 15. Baltrunas L, Makcinskas T, Ricci F (2010) Group recommen- dations with rank aggregation and collaborative Ô¨Åltering. In: Proceedings of the fourth acm conference on recommender sys- tems. RecSys ‚Äô10, pp. 119‚Äì126. Association for Computing Machinery, New York, NY, USA. https://doi.org/10.1145/ 1864708.1864733 16. Ortega F, Hernando A, Bobadilla J, Kang JH (2016) Recom- mending items to group of users using matrix factorization based collaborative Ô¨Åltering. Inf Sci 345:313‚Äì324. https://doi.org/10. 1016/j.ins.2016.01.083 17. Feng S, Zhang H, Wang L, Liu L, Xu Y (2019) Detecting the latent associations hidden in multi-source information for better group recommendation. Know-Based Syst 171:56‚Äì68. https://doi. org/10.1016/j.knosys.2019.02.002 18. Abolghasemi R, Engelstad P, Herrera-Viedma E, Yazidi A (2022) A personality-aware group recommendation system based on pairwise preferences. Inf Sci 595:1‚Äì17. https://doi.org/10.1016/j. ins.2022.02.033 19. Barzegar Nozari R, Koohi H (2020) A novel group recommender system based on members‚Äô inÔ¨Çuence and leader impact. Know- Based Syst 205:106296. https://doi.org/10.1016/j.knosys.2020. 106296 20. Wang X, Su L, Zhou Q, Wu L, Zhang Y (2020) Group recom- mender systems based on members‚Äô preference for trusted social networks. Sec Commun Netw 2020:1‚Äì11. https://doi.org/10. 1155/2020/1924140 21. Ismailoglu F (2022) Aggregating user preferences in group rec- ommender systems: a crowdsourcing approach. Decis Support Syst 152:113663. https://doi.org/10.1016/j.dss.2021.113663 22. Guo J, Zhu Y, Li A, Wang Q, Han W (2016) A social inÔ¨Çuence approach for group user modeling in group recommendation systems. IEEE Intell Syst 31(5):40‚Äì48. https://doi.org/10.1109/ MIS.2016.28 23. Sajjadi Ghaemmaghami S, Salehi-Abari A (2021) DeepGroup: group recommendation with implicit feedback. Association for Computing Machinery, New York, pp 3408‚Äì3412 24. Hu L, Cao J, Xu G, Cao L, Gu Z, Cao W (2014) Deep modeling of group preferences for group-based recommendation. In: Pro- ceedings of the twenty-eighth AAAI conference on artiÔ¨Åcial intelligence. AAAI‚Äô14, pp. 1861‚Äì1867. AAAI Press, Palo Alto, California. https://doi.org/10.1609/aaai.v28i1.9007 25. Harper FM, Konstan JA (2015) The movielens datasets: history and context. ACM Trans Interact Intell Syst 5(4):1‚Äì19. https:// doi.org/10.1145/2827872 26. Guo G, Zhang J, Yorke-Smith N (2013) A novel bayesian simi- larity measure for recommender systems. In: Proceedings of the twenty-third international joint conference on artiÔ¨Åcial intelli- gence. IJCAI ‚Äô13, pp. 2619‚Äì2625. AAAI Press, Menlo Park, California. https://doi.org/10.5555/2540128.2540506 Publisher‚Äôs Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional afÔ¨Åliations. Springer Nature or its licensor (e.g. a society or other partner) holds exclusive rights to this article under a publishing agreement with the author(s) or other rightsholder(s); author self-archiving of the accepted manuscript version of this article is solely governed by the terms of such publishing agreement and applicable law. 14092 Neural Computing and Applications (2023) 35:14081‚Äì14092"

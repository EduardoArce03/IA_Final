Documento,Texto_Original
Wasserstein GAN based architecture to generate collaborative filtering synthetic datasets.pdf,"https://doi.org/10.1007/s10489-024-05313-4
Wasserstein GAN‑based architecture to generate collaborative 
filtering synthetic datasets
Jesús Bobadilla1,2   · Abraham Gutiérrez1,2 
Accepted: 1 February 2024 
© The Author(s) 2024
Abstract
Currently, generative applications are reshaping different fields, such as art, computer vision, speech processing, and natural 
language. The computer science personalization area is increasingly relevant since large companies such as Spotify, Netflix, 
TripAdvisor, Amazon, and Google use recommender systems. Then, it is rational to expect that generative learning will 
increasingly be used to improve current recommender systems. In this paper, a method is proposed to generate synthetic 
recommender system datasets that can be used to test the recommendation performance and accuracy of a company on dif-
ferent simulated scenarios, such as large increases in their dataset sizes, number of users, or number of items. Specifically, 
an improvement in the state-of-the-art method is proposed by applying the Wasserstein concept to the generative adversarial 
network for recommender systems (GANRS) seminal method to generate synthetic datasets. The results show that our pro-
posed method reduces the mode collapse, increases the sizes of the synthetic datasets, improves their ratings distributions, 
and maintains the potential to choose the desired number of users, number of items, and starting size of the dataset. Both 
the baseline GANRS and the proposed Wasserstein-based WGANRS deep learning architectures generate fake profiles from 
dense, short, and continuous embeddings in the latent space instead of the sparse, large, and discrete raw samples that previ-
ous GAN models used as a source. To enable reproducibility, the Python and Keras codes are provided in open repositories 
along with the synthetic datasets generated to test the proposed architecture (https://​github.​com/​jesus​bobad​illa/​ganrs.​git).
Keywords  WGANRS · Generative Adversarial Networks · Recommender Systems · Wasserstein distance · Synthetic 
datasets · Collaborative Filtering
1  Introduction
Recommender systems (RSs) are used to provide personali-
zation facilities to users of internet services. Large compa-
nies that use RSs are Spotify, TripAdvisor, Netflix, Google 
Music, etc. RSs are becoming increasingly important due 
to its capacity to provide both accurate recommendations 
and recommendations designed to retain people using the 
service. Recommendations are provided by suggesting the 
products or services that have a higher probability of being 
liked by the user.
Consequently, it is necessary to filter the available 
items (products or services) in the RS. For this reason, 
RSs are usually classified according to their filtering 
approach. Social [1, 2], content-based [3], demographic 
[4, 5], context-aware [6], collaborative filtering (CF) [7] 
and their ensembles [8] are the most commonly used 
strategies. Social filtering recommends to the active users 
items that their followed, group of friends, contacts, etc., 
like. Content-based recommendations include items with 
similar content to those the active user liked. It is usual to 
compare descriptions or even item images. Demographic 
filtering selects users having demographic features such 
as those of the active user (similar age, same sex, same 
zip code or near zip code, etc.) and then extracts those 
item preferences. Context-aware filtering usually relies 
on geographic information, such as GPS coordinates. 
The most accurate and relevant filtering strategy is the 
 *	 Jesús Bobadilla 
	
jesus.bobadilla@upm.es
	
Abraham Gutiérrez 
	
abraham.gutierrez@upm.es
1	
Universidad Politécnica de Madrid, ETSISI, Ctra. de 
Valencia Km. 7, Madrid, Spain
2	
Technical University of Madrid, ETSISI, Ctra. de Valencia 
Km. 7, Madrid, Spain
/ Published online: 17 February 2024
Applied Intelligence (2024) 54:2472–2490

collaborative strategy. In this strategy, recommendations 
are based on the preferences of the most similar users. The 
machine learning method that best fits the CF concept is 
the K-nearest neighbours algorithm (KNN) [9]. It is simple 
and directly implements the CF concept, where the neigh-
bours are the most similar users to the active users. The 
main drawbacks of the KNN are that it is a memory-based 
method, it runs slowly, and it is not sufficiently accurate. 
The model-based matrix factorization (MF) [10] solves 
the KNN limitations. Moreover, it contains two vectors of 
hidden factors. The first vector is used to code (compress) 
the relevant information of users, whereas the second vec-
tor is used to code the relevant information of items. Both 
vectors belong to the same latent space, and they are com-
bined using a dot product. Additionally, the hidden factors 
are optimized by minimizing the prediction errors. Non-
negative matrix factorization (NMF) [11] ensures that the 
hidden factors are non-negative to enable some semantic 
interpretations of predictions.
Deep learning [12] can currently be implemented to 
obtain improved MF results. The simplest deep learn-
ing architecture is similar to that of MF. In this method, 
the hidden factors of the user are replaced by neural user 
embedding, and analogously, the hidden factors of items 
are replaced by neural item embedding. This model is 
called deep matrix factorization (DeepMF) [13], and it is 
better than MF due to the ability of its neural networks to 
remove complex non-linear patterns in raw data. DeepMF 
combines the embeddings using a dot layer. An improved 
DeepMF model is the variational deep matrix factorization 
(VDeepMF) [14], where an intermediate layer codes the 
parameters of a chosen distribution (usually Gaussian), 
and from it, a stochastic-based sampling process spreads 
samples in the latent space. The DeepMF (or VDeepMF) 
dot layer can be improved by replacing it with a multilayer 
perceptron (MLP) that combines the hidden factors of 
users and item embeddings and generates a manifold. This 
approach is called neural collaborative filtering (NCF) 
[15]. Our proposed architecture combines a DeepMF 
model and a Wasserstein generative adversarial network 
(GAN). GAN [16] networks can generate fake samples fol-
lowing the distribution of a source set of real data samples. 
The most common use of GAN networks is to generate 
realistic fake faces from a dataset of real human faces. 
Similarly, our objective is to generate synthetic (fake) CF 
samples from an existing dataset of CF samples, such as 
MovieLens [17]. Then, by collecting many fake samples, 
a synthetic CF dataset can be created.
Generating synthetic CF datasets makes it possible to 
simulate the stress situation in the RS, as it can be gener-
ated ‘families’ of datasets where gradually some of the 
parameters can be selected. For example, we can gener-
ate a family of CF datasets where the number of users 
grows from several thousands to millions, and then test 
in advance the performance of our system in different 
scenarios where the number of users gradually, or sud-
denly, grows (e.g. due to a marketing campaign or an influ-
encer action). This simulation can avoid system failures 
in extreme situations. Similarly, a family of datasets can 
be generated with a growing number of items. It leads 
to more sparse scenarios where the RS accuracy could 
decrease. This type of simulation gives us the conveni-
ence of incorporating many products or services in a short 
period of time. Additionally, the generation of synthetic 
datasets makes possible that researchers test their machine 
learning models in bounded scenarios, difficult to find in 
real datasets, such as increasingly sparse data matrices, 
different cold start situations, or extreme pattern variations 
in the user profiles.
The state-of-the-art methods in CF generation include 
statistical methods that are not able to adequately determine 
the patterns of complex datasets. Therefore, adversarial 
approaches [18], and GAN-based approaches have been 
proposed. Preventing shilling attacks is a relevant objective 
in the RS field, and some GAN-based approaches act as a 
defence against them [19]. Data augmentation is an obvi-
ous field where GANs can be applied. Purchase profiles are 
used in the collaborative filtering generative adversarial net-
work model (CFGAN) [20] model to increase the number of 
training samples in a dataset of commercial products. The 
identity-preservation generative adversarial network model 
(IPGAN) [21] incorporates negative sampling information to 
improve accuracy results. It allows two separate generative 
models to be incorporated, with one method managing posi-
tive data and the other method processing negative samples. 
Session information is used in the deep collaborative filter-
ing generative adversarial network (DCFGAN) model [22] 
instead of matrices of votes combining GAN and reinforce-
ment learning. To run recommendation training, the neural 
collaborative generative adversarial network (NCGAN) [23] 
incorporates a regular GAN that processes the intermediate 
CF results provided by a neural network stage. The recurrent 
generative adversarial network (RecGAN) [24] combines 
a recurrent neural network (RNN) and a GAN to process 
temporal patterns. Unbalanced datasets are managed using 
a Wasserstein GAN acting as a generator and the packing 
generative adversarial network (PacGAN) as a discriminator 
[25]. Finally, a conditional generative adversarial network 
(CGAN) performs a conditional generation of ratings [26].
The RS state-of-the-art method that generates synthetic 
CF datasets is divided into statistical and machine learning 
approaches. Solutions in the first group allow us to param-
eterize the results (to change the number of items, users, 
etc.), but they do not adequately capture the complex non-
linear relations between users and items. Consequently, the 
accuracy of this method is poor. The second group makes 
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2473

use of deep learning generative adversarial networks to cre-
ate fake profiles or fake samples. The accuracy is improved, 
but the GAN architectures in this group take discrete and 
heavily sparse CF datasets as a source, leading to results that 
are obtained slowly and the subject-to-mode collapse prob-
lem. In addition, the number of items cannot be changed. 
Nevertheless, the existing generative adversarial network for 
recommender systems (GANRS) method makes its GAN 
generation start from dense and continuous latent space 
embeddings, obtaining more accurate results and enabling 
us to choose the number of users and the number of items 
in the synthetic datasets. The method proposed in this paper 
borrows the GANRS architecture, making the necessary 
changes to introduce the Wasserstein concept. Wasserstein 
generative adversarial networks (WGANs) are designed to 
reduce the inherent ‘mode collapse’ of the GAN architecture. 
In the model regularization Wasserstein generative adver-
sarial network (MRWGAN) [27], an RS is implemented. 
Moreover, an autoencoder is used to implement the genera-
tive model, and a model-Wasserstein regularized distance 
is used as the function loss. It achieves better accuracy with 
missing data than the state-of-the-art methods. Analogously, 
an L1 regularized Wasserstein loss function has been used 
for autoencoder-based CF [28] to learn a low-rank repre-
sentation of variables in the latent space. The Wasserstein 
distance has also been implemented to tackle the cold-start 
issue in CF, minimizing it under user embedding constraints.
GAN approaches also have their own drawbacks, particu-
larly: a) a long training time, b) a long inference time when 
the GAN model is very deep, c) difficulty to set relevant 
CF parameters such as the number of items and the number 
of users in the dataset, d) possibility of suffering from the 
‘mode collapse’ behaviour, e) difficulty to adequately learn 
from the sparse data sets of CF, and e) lack of fine tuning the 
variation of the results in successive executions.
No standard machine learning quality measures are 
defined to compare synthetic datasets created or gener-
ated using different statistical or generative models. This 
is because these models are designed to catch the complex 
nonlinear patterns of the source data, and there are no simple 
comparations able to discriminate the quality of the gen-
erated results, such as in regression (MAE, MSD, etc.) or 
classification (accuracy, precision, recall, etc.). To better 
understand this drawback, we can analyse the face image 
quality assessment strategies [29], which are based on the 
character, fidelity, and utility features of facial biometrics. In 
the RS field we do not have such type of information to make 
a similar process, since what we are generating are user/
item vector profiles. In addition, face image quality makes 
a distinction between approaches that require a reference, 
a reduced reference, and no reference of faces, where only 
the two first cases have some accurate quality measure; pre-
cisely those situations that RS cannot manage as they do 
not have the equivalent ‘reference’ to the face image quality 
field. Finally, the conceptual problem of the ‘quality para-
dox’ inherent in these quality measures is heavily present 
in the RS scenario, where a generated dataset should not be 
too similar to the source, otherwise it would not be useful, 
and should not be too different from the source, otherwise 
it would not be representative. Therefore, research papers 
that generate synthetic datasets [23–25] test them by run-
ning several CF Machine Learning models and comparing 
the results obtained on different instances of the generated 
datasets from the same source data. This is the strategy that 
our paper follows in its ‘Results’ section.
In the seminal GANRS paper [30], relevant innovations 
are incorporated, and the previous RS GAN architectures are 
compared to generate synthetic datasets. However, a signifi-
cant drawback occurs. The process to convert from the latent 
space generated samples (dense, small, and continuous) to 
the raw samples (sparse, large, and discrete) that form the 
synthetic dataset generates duplicated samples that must be 
removed. This is a common drawback in a discretization 
task, but if the number of duplicated samples is high, a ‘col-
lapse’ in the GAN generation can occur. The innovation of 
our proposed Wasserstein GAN approach (WGANRS) is the 
introduction of the Wasserstein design (function loss, weight 
constraints, etc.) to the existing GANRS method in the hope 
that the mode collapse situations are reduced. The proposed 
method borrows the stages defined in [30] and replaces the 
regular GAN generation kernel by a Wasserstein approach 
(WGAN). The WGAN provides four relevant improvements: 
1) it incorporates a new loss function that is interpretable 
and has clear stopping criteria, 2) it empirically returns 
better results, 3) the GAN mode collapse is significantly 
reduced, and 4) it provides a clear theoretical backing. The 
WGAN loss function is based on the earth mover’s distance, 
and it incorporates an fw function that acts as a discrimina-
tor model, called the ‘critic’. The critic estimates the earth 
mover’s distance, processing the highest difference between 
the generated distribution and the real distribution under sev-
eral parameterizations of the fw function. The critic makes 
the generator work harder by looking at different projec-
tions. Our most relevant predictor that measures the mode 
collapse mitigation will reduce the removed samples and 
consequently increase the number of samples of the syn-
thetic files (their sizes). We will also test some other quality 
measures such as the precision, recall, and the distribution 
of the ratings, users, and items.
Figure 1 shows the innovation of the proposed method com-
pared to SOTA, particularly with the most currently published 
baseline (GANRS) on which our method is based. As shown 
in a) (top of Fig. 1), SOTA methods that generate CF datasets 
take only raw profiles and generate synthetic RS datasets. This 
is an analogous process to fake face creation, which can be 
generated by GANs from datasets of real faces. It is known 
J. Bobadilla, A. Gutiérrez
2474

that a recurrent problem in these processes is mode collapse 
[30], which leads to a lack of balanced generation of samples. 
Some categories are overrepresented, whereas other categories 
are underrepresented. As an example, we could obtain an enor-
mous quantity of fake samples of Number 7 by using the Modi-
fied National Institute of Standards and Technology (MNIST) 
dataset. However, some other numbers are rarely generated. 
Notably, in Fig. 1a (SOTA methods), the GAN module is fed 
with RAW data, such as image pixels or in our case, user pro-
files. These RAW data are large, discrete, and sparse, leading 
to the mode collapse problem.
The most current research in the area is the GANRS method 
(our baseline). Its high-level architecture is shown in Fig. 1b, 
where the GAN model is not fed with RAW data. Instead, it is 
fed with deep learning embedded data. These embedded data are 
short, continuous, and dense vectors. The embedded data contain 
compressed information on the items and users in the RS. As a 
result, both the performance and the accuracy of the GANRS are 
improved compared to the previous SOTA models and methods.
In the GANRS baseline [30], the mode collapse problem 
is reduced, and the RS datasets generated are less biased 
than those created using SOTA methods. Regardless, the 
mode collapse remains, and it produces a certain degree of 
redundant samples. To reduce mode collapse even further, 
our proposed WGANRS method introduces the Wasserstein 
concept into the GAN kernel (Fig. 1c). The Wasserstein 
approach has been shown to yield better results by reducing 
mode collapse when applied to GANs [27]. This approach 
requires the introduction of a new loss function and benefits 
from a theoretical backing and a defined stopping criterion.
The hypothesis of the paper claims that incorporating 
the Wasserstein concept into the generative kernel of the 
GANRS method will lead to a decrease in the mode collapse 
problem inherent to the GAN when applied to CF scenarios. 
Consequently, the proposed WGANRS is expected to gener-
ate more accurate CF datasets.
The structure of the paper is as follows: In Section 2, the 
proposed WGANRS method (from the existing GANRS infor-
mation) is explained and formalized. In Section 3, the design 
of the experimental executions of code is introduced, and the 
results are shown and analysed using the MovieLens and Net-
flix* datasets as a source. Moreover, the most relevant results 
are provided. In Section 4, the most remarkable conclusions are 
presented, and some future work is proposed. Additionally, the 
references section includes current representative papers in the 
main RS area and in the specific GAN generation of CF datasets.
2  Method
This section is divided into two subsections. In the first 
subsection, the proposed method concepts, its architecture, 
and the sequence of processes and stages to both train the 
model and generate the synthetic datasets in a feedforward 
prediction are explained. The second subsection contains the 
necessary equations to formalize the method, grouped into 
the main stages of the architecture. The Python and Keras 
codes of the proposed method is available in (https://​github.​
com/​jesus​bobad​illa/​ganrs.​git).
2.1  Concepts and architecture
The proposed deep learning architecture is based on five 
sequential stages in which a neural CF, a WGAN model, and 
a clustering process are involved. Moreover, a CF dataset is 
used as the source, and a synthetic dataset that has the same 
format as the source dataset and similar data distributions 
is generated. The key issue involving both the GANRS [30] 
seminal baseline and the WGANRS proposed architecture is 
that the GAN or WGAN stages are fed with dense, short, and 
continuous embeddings in the latent space instead of sparse, 
large, and discrete raw data. It makes the work of both the 
generator and the discriminator models easier, faster, and 
more accurate. The obvious drawback of the proposed 
design is the theoretical loss of quality involved in the com-
pression stage (coder) and, particularly, in the subsequent 
decompression (decoder). However, when converting from 
embedded to raw samples, a significant benefit emerges: 
we can choose the target number of users and items, mak-
ing the GANRS and WGANRS models more flexible and 
useful than the state-of-the-art methods. Figure 2 shows an 
overview of the proposed WGANRS architecture. From an 
existing source dataset (most-left side in Fig. 2), such as 
MovieLens, a synthetic dataset (most-right side in Fig. 2) is 
generated with the same format (to be useful to researchers) 
and similar patterns and distributions of the users, items, 
and ratings. This dataset can be generated by choosing the 
desired number of users, items, and starting number of sam-
ples to be useful for companies and researchers as a base 
for simulations, anticipating diverse future scenarios, or as 
ground data for new machine learning models.
The proposed WGANRS first converts (compresses) the 
input sparse dataset to its embedding-based representation 
and converts (decompresses) the generated (fake) embed-
ding-based dataset to its raw and sparse representation. A 
DeepMF model (left side in Fig. 2) was chosen to perform 
the compression stage due to its simplicity and performance, 
and K-means clustering (right side in Fig. 2) was used to run 
the necessary decompression. In this scenario, the K-means 
algorithm has the advantage of setting the K* number of 
users and K** number of items we want the generated dataset 
to hold. Finally, our architecture kernel is based on a WGAN 
model (centre of Fig. 2) to generate fake embedding samples 
from real embedding samples.
The DeepMF model used for the compression task has a 
previous learning stage (top-left draw in Fig. 3) where the 
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2475

embedding weights are set by means of backpropagation 
optimization. The DeepMF model contains two separate 
embedding layers: one layer for users and the other layer for 
items. These embeddings must have the same size, which 
usually ranges from 5 to 15 neurons. It is expected that 
similar users will be coded with similar embedding maps 
(codes), and the same applies for items. Once the DeepMF 
model has been trained, we can feedforward each existing 
user ID to obtain its embedding representation (top-right 
draw in Fig. 3). The same process is performed with all 
existing item IDs. Thus, we obtain a matrix I x E containing 
the embedding representations of the items, where I is the 
number of items in the source dataset and E is the embed-
ding size. Analogously, we obtain a matrix U x E containing 
the embedding representations of the users, where U is the 
number of users in the source dataset.
By combining the source dataset (left side of Fig. 2), the 
compressed item matrix, and the compressed user matrix 
(top right draw in Fig. 3), we can obtain the embedding rep-
resentation of the source dataset, as shown in the “embed-
ding-based CF dataset” in the bottom left graph of Fig. 3 
and in Fig. 2.
Starting from the embedding-based CF dataset as a 
source, the proposed WGANRS architecture generates the 
“fake embedding-based CF dataset” (centre of Fig. 2), and it 
is performed by means of a Wasserstein GAN. The first stage 
of this generative task is the WGAN training (bottom-left in 
Fig. 3), where the generator model creates synthetic samples 
from Gaussian stochastic vectors containing random noise. 
Then, the Wasserstein critic (discriminator) performs the 
necessary binary classification to label samples as ‘real’ or 
‘fake’. Notably, the ‘fake’ samples come from the generator 
model, whereas the ‘real’ samples are randomly taken from 
the previously generated ‘real embedding-based CF data-
set’. Once the training process has finished, we can forget 
the critic model and take the generator model to create as 
many fake embedding samples as needed. The whole pro-
cess (training and feedforward generation) is expected to be 
fast due to the compressed embedding representation and 
accurate due to the Wasserstein restrictions to avoid mode 
collapse.
In the last stage of the proposed architecture, it is neces-
sary to decompress the ‘fake embedding-based CF dataset’ 
(right side in Fig. 2 and bottom right draw in Fig. 3). In 
this stage, we have generated a very large number of fake 
samples, consisting of tuples < user_embedding,item_
embedding,rating > , where both the user and the item 
embeddings produce vectors of real numbers. We have to 
convert this set of tuples to a discretized version < user_
ID,item_ID,rating > , where user_IDs are integers in the 
range [1..number of users], and analogously item_IDs are 
integers in the range [1..number of items]. Once the neural 
network has been trained, the user embedding assigns simi-
lar codes to similar users (same with the embedding layer). 
This inherent property of the embedding layers makes it 
possible to incorporate a clustering process to the proposed 
Fig. 1   Innovation of the proposed method and its expected impact 
in solving the GAN mode collapse. Figure 1a shows the traditional 
GAN approach in the CF context, Fig.  1b shows the improvement 
introduced in the baseline method to adequately process sparse data, 
and Fig. 1c details the proposed introduction of the Wasserstein ker-
nel to reduce the mode collapse problem
J. Bobadilla, A. Gutiérrez
2476

method, in charge of grouping similar users and items to the 
desired number of users and items in each synthetic dataset.
This is a discretization process in which the WGANRS has 
been designed to set the desired number of users and items in 
the generated dataset. To implement it, K-Means clustering 
[31] was performed, since it allows us to set the K* number of 
users and the K** number of items. Figure 4 shows the follow-
ing concept: a K-means is used to cluster K* users, whereas 
another K-Means process is used to cluster K** items. Since 
similar users should have similar embeddings, it is expected 
that they will be grouped in the same clusters, analogously 
with items. Each user number in the generated dataset corre-
sponds to the cluster number in the K-Means where the fake 
user embedding has been grouped. For example, the left-most 
sample in the fake embedding-based CF dataset (left side of 
Fig. 4) was grouped with its user (green colour) in the K* 
group and its item (blue colour) in Group 3. Consequently, 
the generated fake sample in the synthetic dataset is < K*, 3, 
rating > . In this example, the ‘rating’ is the value of the first 
sample of the source dataset (left side of Fig. 2).
Finally, due to the discretization process, duplicated sam-
ples can be found. This happens when two different generated 
samples share the same user ID and the same item ID. When 
the chosen number of users and items is high, it is more diffi-
cult to find duplicated samples since there is a wider variety of 
clusters, and it is expected that the users and the items will be 
assigned to the groups in a balanced way. Duplicated samples 
can be managed by simply removing the spare samples. The 
expected balance in the clustering groups could be ‘broken’ if 
the mode collapse is happening in the GAN. Indeed, the Was-
serstein concept is used in this paper to avoid mode collapse, 
and the number of removed samples will be used as a quality 
measure in the results section. The lower the removed samples 
are, the better the method is.
Since two of the hyperparameters in the proposed model 
are the number of users and the number of items, the cluster-
ing method that better fits this information is K-Means, where 
directly we can set K* as the number of users and K** as the 
number of items. In fact, this is one of the unusual situations 
where the K value is known before the clustering process. 
Thus, other relevant clustering methods such as hierarchical, 
distribution, density or fuzzy-based ones are not adequate in 
this scenario.
In the following subsection, the WGANRS approach is for-
malized. Moreover, equations have been provided.
2.2  Formalization
2.2.1  CF definitions
First, we define the main sets in the RS: set of users U, items 
I, range of ratings V, and existing samples S.
(1)
We let U be the set of users who make use of a CF RS
(2)
We let I be the set of items available to vote on in the CF RS
(3)
We let V be the range of allowed votes, where V = {1, 2, 3, 4, 5}
(4)
We let S be the set of samples contained in the CF dataset, where N = |S| = the total number of cast votes
(5)
S = {< u, i, v >1, < u, i, v >2, … , < u, i, v >N}, where each u ∈{1, … , |U|}each i ∈{1, … , |I|}, and each v ∈{1, … , |V|}
Fig. 2   Overview of the proposed WGANRS architecture
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2477

The formalization of the defined CF dataset consists of 
a set of tuples <userID, itemID, rating (number of stars)>, 
where the ‘rating’ is the vote assigned for the ‘userID’ to 
the ‘itemID’.
2.2.2  DeepMF training
(6)
The Deep MF training (top −left graph in Figure 3 )is conducted to create
a model that can embed each user ID and each item ID.These two
embeddings feed the WGANRS generative stage. Each embedding is a
compressed representation of the user or the item. The embeddings are
unidimensional vectors of size E + 1. We define f eu(u) as the function
that compresses the user  u  information and analogously f ei(i) as the
function that compresses the item  i  information
(7)
We let E + 1 be the size of the two neural layer embeddings used to
vectorize each user and each item belonging to U and I, respectively.
We let f eu(u) = ⃗eu = [eu
0, eu
1, … , eu
E], where f eu is the embedding
layer output of the users and u ∈{1, … , |U|}
(8)
We let f ei(i) = ⃗ei = [ei
0, ei
1, … , ei
E], where f ei is the embedding layer
output of the items and i ∈{1, … , |I|}
Fig. 3   DeepMF and WGAN models involved in the WGANRS architecture
Below, the most relevant equations in the back propaga-
tion algorithm are defined, and we set the output error as the 
mean squared differences metric. These equations are not 
distinctive of the proposed method.
2.2.3  DeepMF feedforward
Once DeepMF has learned, we can collect the embedding 
representation of each user and each item in the CF RS. 
Therefore, all the existing itemID and userID in the RS 
(9)
By combining the dense vectors of the user and item embeddings
(⃗eu = [eu
0, eu
1, … , eu
E] and ⃗ei = [ei
0, ei
1, … , ei
E]), we can make rating
predictions in the DeepMF training stage. The dot product of the user
embedding and the item embedding in each< u, i, v >j ∈S provides its rating prediction.
yj = f eu(u) ∙f ei(i) = ⃗eiu ∙⃗ei = [eu
0, eu
1, … , eu
E] ∙[ei
0, ei
1, … , ei
E]
(10)
1
2 (yj −̂yj)
2is the output error used in the DeepMF neural network to start the
back propagation algorithm, where the neural weights are iteratively
improved from the 훿j values, Δwji = 훼yjf (Neti) ∑
k wik훿k, where k is
a hidden layer, and Δwji = 훼yjf (Neti) 1
2 (yk −̂yk)
2if k is the output
layer.i, j, and k are successive sequential layers.
J. Bobadilla, A. Gutiérrez
2478

dataset feed the trained DeepMF model, and their embed-
ded representations can then be obtained. It can be done 
by making the feedforward prediction operation (top-right 
graph in Figure 3) on the trained DeepMF model.
(11)
We let E∗= {< u, ⃗eu = [eu
0, eu
1, … , eu
E
] >, ∀u ∈U} be the set of embeddings for all the RS users
(12)
We let E∗(u) = ⃗eu = [eu
0, eu
1, … , eu
E
]
(13)
Let E∗∗=
{
< i, ⃗eu =
[
eu
0, eu
1, … , eu
E
]
, ⃗ei =
[
ei
0, ei
1, … , ei
E
]
>, ∀i ∈I
}
be the set of embeddings for all the RS items.
(14)
We let E∗∗(i) = ⃗ei = [ei
0, ei
1, … , ei
E]
2.2.4  Setting the dataset of the embeddings
Now, we collect the above embedding representations of all 
the itemID and userID in the RS to translate the set ‘S’ (5) to 
the set ‘R’ (15). The set ‘R’ is the embedding-based dataset 
version of the original RAW dataset.
We let
(15)
R = [< E∗(u), E∗∗(i), v >
], ∀< u, i, v >j ∈S be the embedding −based dataset of real samples
2.2.5  WGAN training
The core concept of the GAN methodology is to jointly 
train a generator model and a discriminator model. Once 
the architecture has been trained, the generator model 
creates new samples that follow the distribution of the 
training samples. The discriminator model attempts to 
differentiate between real samples and generated ones. 
This is a min-max optimization problem of the form:
MinGMaxD(피x∽pdata
[logD(x)
] + 피z∽platent
[log(1 −D(G(z))
]
) ,  
w h e r e 
G ∶Z →X  is the generator model, which maps from the 
latent space Z to the input space X ; D ∶X →ℝ is the dis-
criminator model, which maps from the input space X to a clas-
sification value (real/fake). ℝ→ℝ is a concave function. The 
above formula is the optimization function, the expression that 
both networks (generator and discriminator) try to optimize. G 
aims to minimize it, whereas D wants to maximize it.
The bottom-left graph in Fig. 3 shows the generative 
learning. The GAN architecture consists of a discriminator 
classifier (16) and a generator model (17). The generator 
creates fake samples (RS profiles, in our case), whereas 
the discriminator tries to detect fake samples. In generative 
adversarial training, the discriminator progressively learns 
to accurately differentiate fake profiles from real profiles. 
At the same time, the generator learns to make fake sam-
ples difficult to distinguish from real profiles. We call f D 
(16) the discriminator model and f G (17) the generator 
Fig. 4   Clustering stage in the 
WGANRS architecture
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2479

model. Both f D and f G will iteratively learn and improve 
themselves by minimizing a loss function (18) f GD.
f GD = MinGMaxDf(D, G) = ER[fw(R)] + Ez[fw(G(z)] , where 
ER is the expected value for real samples, z is the random 
noise that feeds generator G, and Ez is the expected value for 
the generated fake profiles G(z). fw is the Wasserstein function 
based on the earth mover’s distance . This function satisfies 
the 1 – Lipschitz constraint: |||f
(
x1
)
−f
(
x2
)||| ≤||x1 −x2||, ∀x1, x2. R refers 
(16)
We let f D be the discriminator D model belonging to a GAN model
(17)
We let f G be the generator G model belonging to a GAN model
(18)
We let f GD be the cost function of the GAN model
to (15). Notably, the Wasserstein concept has been introduced 
in Eq. (18). Mainly, it is implemented in the loss function code 
of the generative model. The Wasserstein approach has been 
designed to reduce the mode collapse problem inherent to the 
GAN. We implement it to reduce the mode collapse in our 
WGANRS proposed method and to consequently reduce the 
number of excessively generated similar profiles in the RS.
2.2.6  WGAN generation
Once the GAN has been trained, we can generate as many 
samples as needed. We can introduce batches of random 
Gaussian noise vectors ‘z’ and implement the feedforward 
process (model.predict). The result are batches of fake 
embedded profiles (bottom-right graph in Figure 3).
(19)
We let F = f G be the generated dataset of fake samples from different random noise vectorsz
2.2.7  Clustering of items and users
Figure 4 shows the clustering process, where we set 
K* (20) as the number of users in the generated dataset 
and K** (21) as the number of items on it. The N gen-
erated fake profiles will contain both fake user embed-
dings and fake item embeddings (where N>>K* and 
N >> K**).
(20)
We let K∗be the number of clusters used to group the embeddings of the users
(21)
We let K∗∗be the number of clusters used to group the embeddings of the items
For each generated fake user (each user of N), we must 
select the nearest user from the K* existing users (22). The 
h∗(u) function makes this group. The same process is used 
for items. For each generated fake item (each one of the N), 
we must select the nearest item from the K** existing items 
(23). The h∗∗(i) function makes this group.
(22)
We let h∗(u) = c|c ∈{1, … , K∗} be the clustering operation that assigns a centroid to each user.
(23)
We let h∗∗(i) = c|c ∈{1, … , K∗∗} be the clustering operation that assigns a centroid to each item
2.2.8  Setting the dataset of the item IDs and user IDs
To create the synthetic dataset, the generated set of embed-
dings F (19) is converted to its discretized version H (24).
We let H be the item ID and user ID discrete dataset 
obtained from the embedding-based dataset F of fake samples.
Sometimes, different generated samples in Set 
F will be discretized to the same user and item: 
< h∗(u), h∗∗(i), v >= < h∗(u), h∗∗(i), v >.
(24)
H = [< h∗(u), h∗∗(i), v >|∀< E∗(u), E∗∗(i), v >∈F]
This particularly happens if the GAN suffers from mode 
collapse. In these cases, there are samples with identical 
information, and we create the set H where duplicated sam-
ples are removed (25).
We let S = {H} be the synthetic generated dataset version 
of H where duplicated samples are removed.
(25)
The last transformation removes samples when a fake user casts
different votes (v, v
) to the same item.
(26)
We let G = {< h∗(u), h∗∗(i), v >∈H|∄< h∗(u), h∗∗(i), v >∈H, where h∗(u) = h∗(u) ∧h∗(i) = h∗∗(i) ∧v ≠v}
J. Bobadilla, A. Gutiérrez
2480

3  Results
The proposed method has been tested using two open-source 
representative CF datasets: MovieLens 1 M (https://​group​lens.​
org/​datas​ets/​movie​lens/​1m/) and a subset of Netflix that we call 
Netflix* [32]. These two datasets were chosen because they are 
representative in the CF field. MovieLens is probably the most 
tested dataset family over the years in CF research. The results 
obtained in MovieLens are very informative for RS researchers. 
On the other hand, Netflix is not widely used due to its enor-
mous size and because it is no longer available. We utilized the 
open version of Netflix* that is randomly shortened [33]. Net-
flix* has been selected as the dataset for this research because 
its internal patterns are different from those of MovieLens. 
MovieLens has been created in a relatively short time in an 
academic environment, whereas Netflix is an enormous com-
mercial dataset that has been growing for a long period. Since 
this research involves catching the internal patterns of a source 
dataset and parameterizing and translating those patterns to a 
generated dataset, it is convenient to use such different sources.
Table 1 shows their main parameter values. The results of 
both datasets follow the same trends. Therefore, to ensure that 
the length of this paper is appropriate, we only explain the Mov-
ieLens results and group the Netflix* results (Figs. 10, 11, 12 
and 13) in Appendix A. To test the WGANRS method, two sets 
of synthetic datasets have been created. The first set has a varied 
number of users, whereas the second set has a varied number of 
items. Each row in Table 2 shows the values of each set. The first 
row indicates that five synthetic datasets have been generated. 
The first dataset contains 500 users and 1000 items, the second 
dataset holds 1000 users and 1000 items, and so on until the 
last dataset has 8000 users and 1000 items. All the generated 
datasets were created starting from 400 thousand samples. This 
set of data allows us to test the impact of changing the number 
of users. Analogously, the second row in Table 2 shows that 
four synthetic datasets have been created. All four datasets have 
4000 users. However, the number of items varies from 500 in 
the first dataset to 4000 in the last set. This allows us to measure 
the impact of changing the number of items.
Since we will use the GANRS method [29] as the baseline, 
all datasets have also been created using GANRS. Thirty-six 
datasets were generated, 5 + 4 using MovieLens as a source 
and 5 + 4 using Netflix*, both for GANRS and for WGANRS. 
To test these datasets, four different exections were conducted:
•	 Number of generated samples: The number of samples 
returned by GANRS and WGANRS was compared. 
The WGANRS method is expected to perform better, 
as it focuses particularly on avoiding the typical ‘mode 
collapse’ in GANs. The better managed the mode col-
lapse is, the more varied the embedding samples that the 
WGANRS generates, the better the performance of the 
clustering process to create the raw samples, and finally, 
the lesser the number of sample collisions, increasing the 
sizes of the synthetic datasets.
•	 Rating distributions: It is important that the rating distri-
bution of the generated datasets be as similar as possible 
to that of the source, particularly on the relevant ratings 
(usually 4 and 5 stars). This is an indication that the pat-
terns of the fake profiles are correct, and they contain an 
adequate proportion of relevant and nonrelevant votes.
•	 User and item distributions: It is interesting to test the user 
and item distributions as the number of users and items var-
ies, comparing them to the source dataset. It is expected 
that the Gaussian random noise used to create the stochastic 
vector that feeds the WGANRS generator will force the dif-
ferent Gaussian distributions of users and items.
•	 Precision and recall: Regarding the ground distributions, 
balanced votes, and high number of samples, the gen-
erated datasets are used to adequately analyse them on 
the CF task, and their recommendation quality measures 
return suitable values and trends.
The above executions cover the potential comparatives avail-
able on the generative creation of synthetic datasets in the CF 
area. Figure 5 shows the concept. Figure 5a (top graph) shows 
the traditional CF analysis. Different state-of-the-art methods or 
models are applied to one or several existing CF datasets, and 
the recommendation results can be measured using traditional 
quality prediction and recommendation quality metrics, such 
as the precision, recall, F1, and mean absolute error (MAE). 
However, the field of synthetic CF dataset generation is com-
pletely different. From a source dataset (Fig. 5b), we create one 
or several synthetic datasets. We can set different numbers of 
users, items, samples, etc., and each generated dataset will hold. 
To compare the proposed generative method with the selected 
state-of-the-art baseline, we must create a synthetic dataset or 
group of synthetic datasets using the proposed method (orange 
datasets in Fig. 5b) and a different dataset or group of datasets 
using the SOTA baseline (blue datasets in Fig. 5b). Now, we 
need to determine which of these datasets (or groups) are bet-
ter. Comparing generated datasets (Fig. 5b) is a very different 
task than comparing methods or models applied to an existing 
dataset (Fig. 5a). Figure 5b shows three types of code execution 
that we can perform to decide whether the generated datasets 
using the proposed method (orange datasets) are better than the 
generated datasets using the baseline method (blue datasets):
Table 1   Main parameter values of the tested datasets
Dataset
#users
#items
#ratings
scores
sparsity
Movielens 1 M
6,040
3,706
911,031
1 to 5
95,94
Netflix*
23,012
1,750
535,421
1 to 5
98.68
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2481

1)	 The mode collapse impact can be measured in the CF 
field by removing very similar user profiles. The GAN 
can collapse to a reduced number of source profiles. The 
higher the number of deleted profiles is, the higher the 
mode collapse impact. The higher the deleted profiles 
are, the lower the number of samples in the generated 
dataset. Figure 5b1 shows a comparison where the y-axis 
represents the number of samples. The proposed method 
(orange colour) improves the SOTA baseline.
2)	 The generated datasets should have probability distribu-
tions that are similar to that of the source dataset. The 
user, item, and rating distributions of the synthetic data-
sets can be compared. Figure 5b2 shows the distribu-
tion of the ratings from the source dataset (green colour) 
compared to the ‘baseline’ dataset (orange colour, in this 
graph) and the ‘proposed method’ dataset (blue colour). 
Notably, the synthetic datasets are not expected to have 
the same distributions as the source dataset. The genera-
tive process should create similar datasets, not identical 
datasets. Identical datasets have no value, just as rep-
licating real faces is not valuable in field of computer 
vision. Therefore, there is no absolute metric to measure 
this type of quality. Extreme distribution similarities and 
large distribution differences must be avoided.
3)	 The quality of the generated datasets can be indirectly meas-
ured by running state-of-the-art CF methods and models on 
them. We expect similar behaviours to those obtained when 
we apply these methods to the source dataset. Very different 
trends or absolute values in the generated dataset graphs, 
compared to those in the source dataset, will tell us that the 
generated dataset does not contain the main patterns of the 
source dataset. Figure 5b3 shows the precision and recall 
results on the source dataset (left graph) and the generated 
dataset (right graph) when measured with several SOTA CF 
deep learning models. Both trends and values are similar. As 
explained above, we do not expect identical behaviours and 
values since synthetic datasets should mimic the source pat-
terns and not copy them. For this reason, there is no standard 
quality measure to compare the graphs in Fig. 5b3.
Taking into account the above considerations, several 
experiments have been performed on the three explained 
approaches, as shown in b1, b2, and b3 of Fig. 5. Individual 
executions and their explained results have been structured in 
SubSects. 3.1 to 3.4, followed by the Discussion Subsection.
3.1  Number of generated samples
As explained in the previous section, the proposed method 
uses a WGAN to generate synthetic embedding samples. These 
dense and continuous samples are then converted to their 
sparse and discrete versions by means of the clustering process 
and their translation to the raw tuples in the synthetic dataset. 
This discretization stage causes a proportion of ‘collisions’ 
where identical or similar generated samples must be removed. 
The smaller the number of samples removed, the more accu-
rate the generative model, and the richer the synthetic dataset. 
The Wasserstein GAN is expected to improve the results, as 
it is designed to prevent mode collapse inherent to the GAN 
models. Please note that the hypothesis is that by reducing 
the mode collapse, the variability of the generated embedded 
samples will increase, and then the clustering process will be 
able to spread users and item IDs in a more homogeneous 
way. Consequently, the number of discretized samples that are 
repeated (and deleted) will decrease. Overall, the total size of 
the generated datasets will increase as the GAN mode collapse 
is reduced using the Wasserstein approach.
The final number of samples generated is a relevant qual-
ity measure since it is directly related to the impact of mode 
collapse in the generative process. The baseline GANRS 
method suffers from the mode collapse problem, leading to 
the generation of repeated fake profiles. The method han-
dles this situation by removing spare profiles, but it does not 
provide the necessary diversity of samples. The proposed 
WGANRS is expected to improve the results due to the 
Wasserstein ability to reduce the mode collapse and then 
to improve diversity and increase the synthetic dataset size.
Figure 6 shows the comparison of GANRS (gan) versus 
WGANRS (wgan) both for synthetic datasets where the 
number of users varies (left graph) and for synthetic datasets 
where the number of items varies (right graph). Overall, the 
proposed approach (wgan) significantly improves the base-
line (gan). Specifically, it duplicates the number of gener-
ated samples. A 213% improvement in the left graph and a 
191% improvement in the right graph are achieved. This is a 
relevant predictor of the superiority of the proposed method. 
Additionally, as expected, the higher the number of users 
or items there are, the higher the number of generated sam-
ples. This is because the clustering process can better spread 
the samples in the latent space as the number of centroids 
increases. Then, the number of duplicated samples decreases.
The results show a relevant improvement when the pro-
posed method is applied compared to the baseline. This con-
firms that the paper hypothesis is fulfilled. Moreover, incor-
porating the Wasserstein concept into the generative kernel 
of the GANRS method will lead to a decrease in the mode 
collapse problem inherent to the GAN when applied to CF 
scenarios. Generated datasets have less redundant profiles. 
Accordingly, they are more diverse, and they contain more 
Table 2   Parameter values
initial #samples
#users
#items
400,000
{500, 1000, 2000, 4000, 8000}
1,000
400,000
4,000
{500, 1000, 
2000, 
4000}
J. Bobadilla, A. Gutiérrez
2482

samples. Overall, the proposed WGANRS method generates 
richer, unbiased, and longer synthetic datasets.
3.2  Rating distributions
The distribution of the ratings (one star, two stars, …, five 
stars) is an important quality measure in the CF synthetic 
dataset generation process. Recommendation models are 
very sensitive to the relevant versus non-relevant thresh-
old, which is usually set to four or five stars in CF datasets 
containing five possible ratings. It is not enough that the 
Gaussian distribution of ratings in the generated dataset has 
a similar mean to the Gaussian distribution in the source 
dataset. It is also necessary that their standard deviation 
be analogous. Figure 7 shows that the proposed WGANRS 
generates a Gaussian distribution more similar to the Mov-
ieLens distribution than the baseline GANRS. Specifically, 
it achieves a 271.21% improvement. The improvement aver-
age obtained using the synthetic datasets in the first row of 
Table 2 (the number of users varies) is 304% (541% in Net-
flix*), whereas the second row (the number of items varies) 
returns a 357% improvement on average (399% in Netflix*). 
It is expected that these positive results will contribute to 
providing adequate recommendation quality results in the 
next subsection.
Beyond the numeric improvement values shown before, we 
can compare the shapes of the probability distribution in Fig. 7. 
The probability distribution of the source MovieLens dataset 
(green-colour bars) is the target. The proposed WGANRS 
method (blue-colour bars) is much closer to the target than 
the baseline GANRS method (orange-colour bars). This is the 
reason for the relevant numerical improvements shown in the 
above paragraph. Additionally, the baseline method generates a 
Gaussian distribution excessively centred in the average rating 
(three stars), whereas the proposed method adequately fits its 
Gaussian distribution to the correct four-star mean. Regard-
ing the Gaussian standard deviation, the baseline method does 
not adequately catch the source dataset shape. Its deviation is 
smaller, and consequently, it does not generate enough profiles 
in the distribution edges (one star and five stars). In contrast, 
the proposed method performs nearly perfectly on both edges 
of the source distribution. Thus, the samples generated using 
the proposed WGANRS method are more diverse and unbiased 
than those obtained running the SOTA baseline. Additionally, 
the obtained result better follows the Gaussian distribution that 
describes the source shape of ratings. This result reinforces and 
Fig. 5   Traditional CF validation of the methods and models versus the validation of the synthetic datasets generated by the GAN
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2483

complements that obtained in Sect. 3.1. Overall, the proposed 
method a) reduces repeated samples, b) generates more sam-
ples, c) increases diversity, d) decreases the bias, and e) better 
mimics the probability distributions of the ratings.
3.3  Precision and recall
In this subsection, we show the recommendation quality 
results obtained on synthetic datasets obtained using Mov-
ieLens as a source. The WGANRS method was used for this 
experiment to generate the synthetic datasets. The state-of-
the-art NCF (neural collaborative filtering) deep learning 
model has been used to make predictions and recommenda-
tions. The relevancy 휃 threshold was set to five. The top graphs 
in Fig. 8 show the results when the number of users varies, 
whereas the bottom graphs show the results when the number 
of items varies. Both the values and the trends obtained from 
the synthetic datasets (coloured curves) are similar and com-
patible with the source datasets (black curves), which indi-
cates that the proposed method generates suitable synthetic 
datasets to be used in the RS field. Additionally, as expected, 
the higher the number of users, the higher the recall is, since 
each user profile will contain fewer relevant ratings (recall 
denominator). Conversely, the higher the number of users, 
the lower the precision is, since the denominator is the con-
stant N (number of recommendations), whereas the numerator 
contains the true positives of relevant ratings, where a high 
number of users involves less relevant ratings per user.
Additionally, from the set of synthetic datasets where the 
number of users varies, the dataset that holds 1000 users pro-
vides more precision and recall results like the MovieLens 
source. Since MovieLens 1 M contains 6000 users, it tells us 
that the GAN-based method generates data patterns where 
recommendations are easier than using the source dataset. 
This result is consistent with the one reported in [30]. Most 
importantly, the evolution of all the recommendation curves 
in the generated datasets (coloured curves) follow the same 
trends as those exhibited by the source MovieLens (black 
curve), indicating that the internal patterns of the source data-
set have been adequately captured by the proposed WGANRS 
method. Regarding the results when the number of items var-
ies, similar conclusions can be drawn, underlying that rec-
ommendation qualities worsen in absolute values compared 
to the source dataset. This probably occurs because the dis-
tribution of the item ratings is highly variable compared to 
the distribution of the user ratings, leading to more difficult 
pattern extraction. There is a number of items holding a very 
low number of ratings.
3.4  User and item distributions
Once the rating distributions have been tested, it is also 
convenient to compare the user and the item distributions 
obtained by using both the proposed and the baseline methods. 
The user and item distributions of the synthetic datasets are 
very dependent on the Gaussian parameter values with which 
the noise vectors that feed the generative model have been 
created. In the original paper [28] that serves as a baseline, 
the standard deviation has been customized for each tested 
dataset. In contrast, by using the proposed method, we fixed it 
to one and then removed this hyperparameter, making it easier 
to fine tune the proposed approach compared to the baseline 
method. The top graph in Figure 9 shows the results when 
the number of users varies, while its bottom graph shows the 
result by varying the number of items. Dashed lines repre-
sent the baseline results, and solid lines show the proposed 
approaches. In all cases, as expected, the higher the number 
of users there are, the lower the number of ratings assigned to 
Fig. 6   Number of samples generated using the baseline GANRS 
method (gan) versus the proposed WGANRS method (wgan).  Source 
dataset: MovieLens 1  M. Number of samples needed: 40,000. Left 
graph: generated datasets with 1000 items and a range of 500, 1000, 
2000, 4000 and 8000 users. Right graph: generated datasets with 
4000 users and a range of 500, 1000, 2000 and 4000 items. The 
higher the number of generated samples, the better the model is
J. Bobadilla, A. Gutiérrez
2484

each user, since the number of ratings in each dataset is fixed. 
It can also be observed that the proposed method generates 
Gaussian distributions with higher standard deviations than 
the baseline approach, which has been heuristically tailored to 
the dataset. Both the proposed and baseline methods generate 
suitable user and item distributions for the CF area.
3.5  Discussion
The experimental results show the superiority of the proposed 
WGANRS method compared to the GANRS baseline. Par-
ticularly relevant is the high improvement (approximately 
200%) in the number of generated samples. This indicates 
that the proposed Wasserstein approach effectively reduces 
the amount of mode collapse of the GAN, The WGANRS 
method also effectively mimics the rating distribution of the 
source dataset, obtaining high improvements compared to the 
baseline and making it possible that their quality precision and 
recall values and trends are compatible with those from the 
source dataset. Furthermore, even no standard quality meas-
ures exist to test RS generated data, the user and item distribu-
tions obtained using the proposed approach are comparable 
to those of the baseline method. Additionally, the proposed 
method has the advantage that it is not necessary to assign 
heuristic values to the standard deviation of the Gaussian dis-
tribution used to create the noisy random vectors that feed 
the generator model of the WGAN. Finally, the results using 
the Netflix* dataset reinforce the results obtained by testing 
MovieLens. Appendix A shows the Netflix* results.
Overall, the proposed method improves both the sta-
tistical baselines and state-of-the-art generative methods. 
Statistical baselines are reported to reach poor accuracy. In 
contrast, they support adequate parameterization. Generative 
baselines operate quite differently. They do not support full 
parameterization and exceed the accuracy of statistical meth-
ods [24]. Our proposed method is proven to provide both full 
Fig. 7   Comparative rating distributions among the MovieLens 1  M 
(ML) source dataset, the baseline GANRS method (gan), and the pro-
posed WGANRS method (wgan). The 8000-user and 1000-item syn-
thetic dataset has been chosen as a representative case from the set 
of generated data in the paper. The closer the distribution is to the 
source ML distribution, the better the model is
Fig. 8   Quality of the recom-
mendation: precision and recall 
obtained by varying the number 
N of recommendations from 2 
to 10. The relevancy threshold 휃 
was set to 5. The upper graphs 
show the results on the synthetic 
datasets containing 500 to 8000 
users. The lower graphs show 
the results on the synthetic 
datasets containing 500 to 4000 
items. Precision can be seen in 
the left graphs, whereas recall is 
shown in the right graphs. The 
MovieLens dataset was used. 
The higher the values are, the 
better the results
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2485

parameterization and high accuracy, in addition to a strong 
reduction of the mode collapse problem inherent to the GAN 
architectures. On the other hand, our method inherits the 
most positive and the most negative features of its baseline 
[30]. However, its accuracy and performance are very high 
due to the short, dense, and continuous vectors that its GAN 
model takes as input. Its main drawback comes from the 
clustering stage of the method (Fig. 4), which requires addi-
tional execution time and involves a discretization process 
that increases the probability of generating duplicated sam-
ples. For this reason, the Wasserstein concept has been pro-
posed to alleviate the explained drawback. The results show 
that the proposed method adequately reduces the mode col-
lapse problem, maintains the baseline advantages, reduces 
its disadvantages, and confirms the hypothesis of this paper.
4  Conclusions
The most relevant conclusion in this paper is that the Wasser-
stein approach reduces the mode collapse in the GAN genera-
tion of the CF fake samples compared to the state-of-the-art 
methods. This positive effect is reflected in a relevant reduction 
in duplicated samples and consequently in the generation of 
larger synthetic datasets. Furthermore, the proposed approach 
returns very improved distributions of ratings, which facilitates 
obtaining correct values and trends in recommendation qual-
ity measures. Finally, the distributions of the users and items 
are comparable to those of the state-of-the-art methods; these 
distributions act as quality measures due to the lack of stand-
ard quality measures for RS generated data. Moreover, exist-
ing hyperparameters are avoided in the proposed method. The 
standard deviation of the Gaussian distribution is used to create 
the noisy vectors that feed the generator model in the GAN. 
Overall, the results of the experiment show that by applying 
the Wasserstein distance and weight clipping to CF data, the 
generative process is improved compared to the state-of-the-art 
methods that use Wasserstein-based GANs. Proposed future 
work includes a) testing the proposed method on different RS 
datasets, with several sparsity ratios and different numbers of 
users or items, b) comparing the existing biases in the source 
datasets with the generated biases in the synthetic datasets, and 
c) checking the ability of the generated samples to serve as data 
augmentation when they are added to the source datasets.
Fig. 9   Top graph: distribu-
tion of the ratings when the 
number of users varies from 
500 to 8000; comparative 
of the proposed WGANRS 
(wgan) method and the baseline 
GANRS (gan) method. Bottom 
graph: distribution of ratings 
when the number of items var-
ies from 500 to 4000; compari-
son of the proposed WGANRS 
(wgan) method and the baseline 
GANRS (gan) method. The 
source MovieLens (ML) dataset 
is used in both graphs
J. Bobadilla, A. Gutiérrez
2486

Appendix
In this section, the same figures used for MovieLens are 
shown. However, in this case, the Netflix* dataset has been 
used as a source.
Fig. 10   Number of samples generated using the baseline GANRS 
method (gan) versus the proposed WGANRS method (wgan).  Source 
dataset: Netflix*. Number of needed samples: 40,000. Left graph: 
generated datasets with 1000 items and a range of 500, 1000, 2000, 
4000 and 8000 users; right graph: generated datasets with 4000 users 
and a range of 500, 1000, 2000 and 4000 items. The higher the num-
ber of generated samples, the better the model is
Fig. 11   Comparative rating distributions among the Netflix* source dataset, 
the baseline GANRS method (gan) and the proposed WGANRS method 
(wgan). The 8000-users and 1000-items synthetic dataset has been chosen as 
a representative case from the set of generated data in the paper. The closer 
the distribution is to the source ML distribution, the better the model is
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2487

Fig. 12   Quality of recommen-
dation: precision and recall 
obtained by varying the number 
N of recommendations from 2 
to 10. The relevancy threshold 휃 
was set to 5. The upper graphs 
show the results on the synthetic 
datasets containing 500 to 8000 
users. The lower graphs show 
the results on the synthetic 
datasets containing 500 to 4000 
items. Precision can be seen in 
the left graphs, whereas recall is 
shown in the right graphs. The 
Netflix* dataset was used. The 
higher the values are, the better 
the results
Fig. 13   Top graph: distribu-
tion of the ratings when the 
number of users varies from 
500 to 8000. Comparison 
of the proposed WGANRS 
(wgan) method and the baseline 
GANRS (gan) method. Bottom 
graph: distribution of ratings 
when the number of ratings var-
ies from 500 to 4000. Compari-
son of the proposed WGANRS 
(wgan) method and the baseline 
GANRS (gan) method. The 
source Netflix* dataset is used 
in both graph
J. Bobadilla, A. Gutiérrez
2488

Author contributions  Abraham Gutiérrez ran most of the executions 
and prepared the figures and the paper format.
Jesús Bobadilla provided the paper concept, the model design, the 
experimental design, and wrote the paper.
Funding  Open Access funding provided thanks to the CRUE-CSIC 
agreement with Springer Nature. This work was partially supported 
by the Ministerio de Ciencia e Innovación of Spain under the pro-
ject PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de 
Madrid under Convenio Plurianual with the Universidad Politécnica 
de Madrid in the actuation line of Programa de Excelencia para el 
Profesorado Universitario.
Data availability  The datasets generated during and/or analysed during 
the current study are available in the GitHub repository, https://​github.​
com/​jesus​bobad​illa/​ganrs.​git
Declarations 
Ethics for obtaining the data  In this paper, all the conditions specified 
for the use of the open datasets taken as a source for the generative 
process are satisfied, including the reference to the paper stated in the 
README file.
Conflicts of interest  The authors have no competing interests to de-
clare that are relevant to the content of this article and agree to the 
publishing of its content.
Open Access  This article is licensed under a Creative Commons Attri-
bution 4.0 International License, which permits use, sharing, adapta-
tion, distribution and reproduction in any medium or format, as long 
as you give appropriate credit to the original author(s) and the source, 
provide a link to the Creative Commons licence, and indicate if changes 
were made. The images or other third party material in this article are 
included in the article’s Creative Commons licence, unless indicated 
otherwise in a credit line to the material. If material is not included in 
the article’s Creative Commons licence and your intended use is not 
permitted by statutory regulation or exceeds the permitted use, you will 
need to obtain permission directly from the copyright holder. To view a 
copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.
References
	 1.	 Shokeen J, Rana C (2020) A study on features of social recom-
mender systems. Artif Intell Rev 53(2):965–988. https://​doi.​org/​
10.​1007/​s10462-​019-​09684-w
	 2.	 Bobadilla J, Gutiérrez A, Alonso S, González-Prieto A (2022) 
Neural Collaborative Filtering Classification Model to Obtain 
Prediction Reliabilities. International Journal of Interactive Mul-
timedia and Artificial Intelligence 7(4):18–26. https://​doi.​org/​10.​
9781/​ijimai.​2021.​08.​010
	 3.	 Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender 
systems leveraging multimedia content. ACM Computing Surveys 
(CSUR) 53(5):1–38. https://​doi.​org/​10.​1145/​34071​90
	 4.	 Bobadilla J, González-Prieto A, Ortega F, Lara-Cabrera R (2021) 
Deep learning feature selection to unhide demographic recom-
mender systems factors. Neural Comput Appl 33(12):7291–7308. 
https://​doi.​org/​10.​1007/​s00521-​020-​05494-2
	 5.	 Bobadilla J, Lara-Cabrera R, González-Prieto Á, Ortega F (2021) 
DeepFair: Deep Learning for Improving Fairness in Recom-
mender Systems. International Journal of Interactive Multimedia 
and Artificial Intelligence 6(6):86–94. https://​doi.​org/​10.​9781/​
ijimai.​2020.​11.​001
	 6.	 Kulkarni S, Rodd SF (2020) Context aware recommendation 
systems: A review of the state of the art techniques. Computer 
Science Review 37:100255. https://​doi.​org/​10.​1016/j.​cosrev.​2020.​
100255
	 7.	 Wang Z (2023) Intelligent recommendation model of tourist 
places based on collaborative filtering and user preferences. Appl 
Artif Intell 37(1):2203574. https://​doi.​org/​10.​1080/​08839​514.​
2023.​22035​74
	 8.	 Ray B, Garain A, Sarkar R (2021) An ensemble-based hotel rec-
ommender system using sentiment analysis and aspect categoriza-
tion of hotel reviews. Applied Soft Computing 98:106935. https://​
doi.​org/​10.​1016/j.​asoc.​2020.​106935
	 9.	 Kabul MS, Setiawan EB (2022) Recommender System with User-Based 
and Item-Based Collaborative Filtering on Twitter using K-Nearest 
Neighbors Classification. Journal of Computer System and Informat-
ics 3:478–484. https://​doi.​org/​10.​47065/​josyc.​v3i4.​2204
	10.	 Eslami G, Ghaderi F (2023) Incremental trust-aware matrix factor-
ization for recommender systems: towards Green AI. Appl Intell 
53:12599–12612. https://​doi.​org/​10.​1007/​s10489-​022-​04150-7
	11.	 Mehdi HA (2022) A novel constrained non-negative matrix fac-
torization method based on users and items pairwise relationship 
for recommender systems. Expert Syst Appl 195:116593. https://​
doi.​org/​10.​1016/j.​eswa.​2022.​116593
	12.	 Gheorghe P, Pérez-Jiménez M, Grzegorz R (2023) Infinite Spike 
Trains in Spiking Neural P Systems. Romanian Journal of Infor-
mation Science and Technology 2023:251–275. https://​doi.​org/​
10.​59277/​ROMJI​ST.​2023.3-​4.​01
	13.	 Liu H, Zheng C, Li D, Shen X, Lin K, Wang J, Zhang Z, Zhang 
Z, Xiong N (2021) EDMF: Efficient Deep Matrix Factorization 
With Review Feature Learning for Industrial Recommender 
System. IEEE Trans Industr Inf 18(7):4361–4371. https://​doi.​
org/​10.​1109/​TII.​2021.​31282​40
	14.	 Bobadilla J, Ortega F, Gutiérrez A, González-Prieto Á (2022) 
Deep variational models for collaborative filtering-based recom-
mender systems. Neural Comput Appl 35:7817–7831. https://​
doi.​org/​10.​1007/​s00521-​022-​08088-2
	15.	 Hai C, Fulan Q, Jie C, Shu Z, Yanping Z (2021) Attribute-based 
Neural Collaborative Filtering. Expert Syst Appl 185:115539. 
https://​doi.​org/​10.​1016/j.​eswa.​2021.​115539
	16.	 Min G, Junwei Z, Junliang Y, Jundong L, Junhao W, Qingyu X 
(2021) Recommender systems based on generative adversarial 
networks: A problem-driven perspective. Inf Sci 546:1166–
1185. https://​doi.​org/​10.​1016/j.​ins.​2020.​09.​013
	17.	 Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of 
a recommender system with ensemble learning and graph embed-
ding: a case on MovieLens. Multimedia tools and applications 
80(5):7805–7832. https://​doi.​org/​10.​1007/​s11042-​020-​09949-5
	18.	 Kumar A, Aggarwal RK (2022) An exploration of semi-
supervised and language-adversarial transfer learning using 
hybrid acoustic model for hindi speech recognition. J Reli-
able Intell Environ 8:117–132. https://​doi.​org/​10.​1007/​
s40860-​021-​00140-7
	19.	 Deldjoo Y, Noia DT, Merra FA (2021) Survey on Adversarial 
Recommender Systems: From Attack/Defense Strategies to Gen-
erative Adversarial Networks. ACM Comput Surv 54(2):1–38. 
https://​doi.​org/​10.​1145/​34397​29
	20.	 Chae DK, Kang JS, Kim SW, Lee JT (2018) CFGAN: a generic 
collaborative filtering framework based on generative adversarial 
networks. In: Proceedings of the 27th, ACM International Con-
ference on Information and Knowledge Management, CIKM 
2018. Association for Computing Machinery, New York, NY, pp 
137–146. https://​doi.​org/​10.​1145/​32692​06.​32717​43
	21.	 Guo G, Zhou H, Chen B et al (2022) IPGAN: Generating informa-
tive item pairs by adversarial sampling. IEEE Transactions on 
Wasserstein GAN-based architecture to generate collaborative filtering synthetic datasets
2489

Neural Networks and Learning Systems 33(2):694–706. https://​
doi.​org/​10.​1109/​TNNLS.​2020.​30285​72
	22.	 Zhao J, Li H, Qu L, Zhang Q, Sun Q, Huo H, Gong M (2022) DCF-
GAN: An adversarial deep reinforcement learning framework with 
improved negative sampling for session-based recommender systems. 
Inf Sci 596:222–235. https://​doi.​org/​10.​1016/j.​ins.​2022.​02.​045
	23.	 Sun J, Liu B, Ren H, Huang W (2022) WNCGAN: A neural adver-
sarial collaborative filtering for recommender system. Journal of 
intelligent & fuzzy systems 42(4):2915–2923. https://​doi.​org/​10.​
3233/​jifs-​210123
	24.	 Bharadhwaj H, Park H, Lim BY (2018) RecGAN: recurrent gen-
erative adversarial networks for recommendation systems. In: Pro-
ceedings of the 12th ACM Conference on Recommender Systems, 
RecSys september 2019. Association for Computing Machinery, 
New York, NY, pp 372–376. https://​doi.​org/​10.​1145/​32403​23.​
32403​83
	25.	 Shafqat W, Byun YC (2022) A Hybrid GAN-Based Approach to 
Solve Imbalanced Data Problem in Recommendation Systems. 
IEEE access 10:11036–11047. https://​doi.​org/​10.​1109/​ACCESS.​
2022.​31417​76
	26.	 Wen J, Zhu XR, Wang CD, Tian Z (2022) A framework for per-
sonalized recommendation with conditional generative adversarial 
networks. Knowl Inf Syst 64(10):2637–2660. https://​doi.​org/​10.​
1007/​s10115-​022-​01719-z
	27.	 Wang Q, Huang Q, Ma K, Zhang X (2021) A Recommender 
System Based on Model Regularization Wasserstein Generative 
Adversarial Network. Inf Sci 546:1166–1185. https://​doi.​org/​10.​
1016/j.​ins.​2020.​09.​013
	28.	 Zhang X, Zhong J, Liu K (2021) Wasserstein autoencoders for 
collaborative filtering. Neural Comput Appl 33(7):2793–2802. 
https://​doi.​org/​10.​1007/​s00521-​020-​05117-w
	29.	 Schlett T, Rathgeb C, Henniger O, Galbally J, Fierrez J, Busch C 
(2022) Face Image Quality Assessment: A Literature Survey. ACM 
Comput Surv 54(10):1–49. https://​doi.​org/​10.​1145/​35079​01
	30.	 Bobadilla J, Gutiérrez A, Yera R, Martínez L (2023) Creating Syn-
thetic Datasets for Collaborative Filtering Recommender Systems 
using Generative Adversarial Networks. Knowledge Based Systems 
280(1):111016. https://​doi.​org/​10.​1016/j.​knosys.​2023.​111016
	31.	 Ioan-Daniel B, Radu-Emil P, Alexandra-Bianca B (2022) 
Improvement of K-means Cluster Quality by Post Processing 
Resulted Clusters. Procedia Computer Science 199:63–70. https://​
doi.​org/​10.​1016/j.​procs.​2022.​01.​009
	32.	 Ortega F, Mayor J, López-Fernández D, Lara-Cabrera R (2021) 
CF4J 2.0: adapting collaborative filtering for java to new chal-
lenges of collaborative filtering based recommender sys-
tems. Knowledge-Based Syst 215(4):106629. https://​doi.​org/​10.​
1016/j.​knosys.​2020.​106629
	33.	 Gong Y (2023) Distribution constraining for combating mode 
collapse in generative adversarial networks. J Electron Imaging 
32(4):43029–43030. https://​doi.​org/​10.​1117/1.​JEI.​32.4.​043029
Publisher's note  Springer Nature remains neutral with regard to 
jurisdictional claims in published maps and institutional affiliations.
Jesús Bobadilla   received the 
B.S. and the Ph.D. degrees in 
computer science from the Uni-
versidad Politécnica de Madrid 
and the Universidad Carlos III. 
Currently, he is a full professor 
with the Department of Informa-
tion Systems, Universidad Poli-
técnica de Madrid. He is a habit-
ual author of programming 
languages books working with 
McGraw- Hill, Ra-Ma and Alfa 
Omega publishers. His research 
interests include information 
retrieval, recommender systems 
and speech processing. He over-
sees the FilmAffinity.com research team working on the collaborative 
filtering kernel of the web site. He has been a researcher into the Inter-
national Computer Science Institute at Berkeley University and into 
the Sheffield University.
Abraham Gutiérrez   received 
the B.S. and the Ph.D. degrees in 
computer science from the Uni-
versidad Politécnica de Madrid. 
Currently, he is currently an 
associate professor with the 
Department of Information Sys-
tems, Universidad Politécnica de 
Madrid. He is the author of 
research papers in most prestig-
ious international journals. He is 
a habitual author of program-
ming languages books working 
with McGraw-Hill, Ra-Ma and 
Alfa Omega publishers. His 
research interests include P-Sys-
tems, machine learning, data analysis and artificial intelligence. He is 
in charge of this group innovation issues, including the commercial 
projects.
J. Bobadilla, A. Gutiérrez
2490
"
Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender Systems.pdf,"See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/370797388
Comprehensive Evaluation of Matrix Factorization Models for Collaborative
Filtering Recommender Systems
Article  in  International Journal of Interactive Multimedia and Artificial Intelligence · January 2023
DOI: 10.9781/ijimai.2023.04.008
CITATIONS
3
READS
46
4 authors:
Jesus Bobadilla
Universidad Politécnica de Madrid
97 PUBLICATIONS   6,377 CITATIONS   
SEE PROFILE
Jorge Dueñas-Lerín
Comunidad de Madrid
7 PUBLICATIONS   14 CITATIONS   
SEE PROFILE
Fernando Ortega
Universidad Politécnica de Madrid
69 PUBLICATIONS   5,648 CITATIONS   
SEE PROFILE
Abraham Gutierrez
Universidad La Salle México
39 PUBLICATIONS   588 CITATIONS   
SEE PROFILE
All content following this page was uploaded by Jorge Dueñas-Lerín on 20 July 2023.
The user has requested enhancement of the downloaded file.

- 1 -
* Corresponding author.
E-mail addresses: jesus.bobadilla@upm.es (J. Bobadilla), 
jorgedl@alumnos.upm.es (J. Dueñas-Lerìn), fernando.ortega@upm.es 
(F. Ortega), abraham.gutierrez@upm.es (A. Gutierrez).
Please cite this article in press as:  
J. Bobadilla, J. Dueñas-Lerín, F. Ortega, A. Gutierrez. Comprehensive Evaluation of Matrix Factorization Models for Collaborative Filtering Recommender 
Systems, International Journal of Interactive Multimedia and Artificial Intelligence, (2023), http://dx.doi.org/10.9781/ijimai.2023.04.008
Keywords
Collaborative Filtering, 
Matrix Factorization, 
Recommender Systems.
Abstract
Matrix factorization models are the core of current commercial collaborative filtering Recommender Systems. 
This paper tested six representative matrix factorization models, using four collaborative filtering datasets. 
Experiments have tested a variety of accuracy and beyond accuracy quality measures, including prediction, 
recommendation of ordered and unordered lists, novelty, and diversity. Results show each convenient matrix 
factorization model attending to their simplicity, the required prediction quality, the necessary recommendation 
quality, the desired recommendation novelty and diversity, the need to explain recommendations, the adequacy 
of assigning semantic interpretations to hidden factors, the advisability of recommending to groups of users, 
and the need to obtain reliability values. To ensure the reproducibility of the experiments, an open framework 
has been used, and the implementation code is provided.
DOI:  10.9781/ijimai.2023.04.008
Comprehensive Evaluation of Matrix Factorization 
Models for Collaborative Filtering Recommender 
Systems
Jesús Bobadilla, Jorge Dueñas-Lerín, Fernando Ortega, Abraham Gutierrez *
Dpt. Sistemas Informáticos and KNODIS Research Group, Universidad Politécnica de Madrid (Spain)
Received 2 July 2022 | Accepted 4 March 2023 | Early Access 28 April 2023 
I.	 Introduction
R
ecommender System (RS) [1] is the field of artificial intelligence 
specialized in user personalization. Mainly, RSs provide accurate 
item recommendations to users: movies, trips, books, music, etc. 
Recommendations are made following some filtering approach. The 
most accurate filtering approach is the Collaborative Filtering (CF) [2], 
where recommending to an active user involves a first stage to make 
predictions about all his or her not consumed or voted items. Then, 
the top predicted items are recommended to the active user. The CF 
approach assumes the existence of a dataset that contains explicitly 
voted items or implicitly consumed items from a large number of 
users. Remarkable commercial RSs are Amazon, Spotify, Netflix, or 
TripAdvisor.
Regardless of the machine learning model used to implement 
CF, the key concept is to extract user and item patterns and then to 
recommend to the active user those items that he or she has not voted 
or consumed, and that similar users have highly valued. It fits with the 
K Nearest Neighbors (KNN) memory-based algorithm [3], and it is the 
reason why the initial RS research was based on KNN. There are also 
some other filtering approaches such as demographic, social, content-
based, context-aware, and their ensembles. Demographic filtering [4] 
makes use of user information such as gender, age, or zip code, and item 
information such as movie genre, country to travel, etc. Social filtering 
[5], [6] has a growing importance in current RS, due to the social 
networks boom. The existence of trust relations and graphs [7] can 
improve the quality of the CF recommendations. In this decentralized 
and dynamic environment, trust between users provides additional 
information to the centralized set of ratings. Trust relationships 
can be local, collective, or global [8]; local information is based on 
shared users’ opinions, collective information uses friends’ opinions, 
whereas global information relates to users’ reputation [9]. Content-
based filtering [10] recommends items with the same type (content) to 
consumed items (e.g. to recommend Java books to a programmer that 
bought some other Java book). Context-aware filtering [11] uses GPS 
information, biometric sensor data, etc. Finally, ensemble architectures 
[12] get high accuracy by merging several types of filtering.
Memory-based algorithms have two main drawbacks: their accuracy 
is not high, and each recommendation process requires to recompute 
the whole dataset. Model-based approaches solve both problems: 
their accuracy is higher than that of memory-based methods, and 
they first create a model from the dataset. From the created model we 
can make many different recommendations, and it can be efficiently 
updated when the dataset changes. Matrix Factorization (MF) [13] 
is the most popular approach to implement current RSs: it provides 
accurate predictions, it is conceptually simple, it has a straightforward 
implementation, the model learns fast, and also updates efficiently. 
The MF model makes a compression of information, coding very 
sparse and large vectors of discrete values (ratings) to low dimensional 
embeddings of real numbers, called hidden factors. The hidden factors, 
both from the user vector and from the item vector, are combined 
by means of a dot product to return predictions. This is an iterative 
process in which the distance between training predictions and their 
target ratings is minimized.
The Probabilistic Matrix Factorization (PMF) model based on MF 
[13] scales linearly with the size of the data set. It also returns accurate 

- 2 -
International Journal of Interactive Multimedia and Artificial Intelligence
results when applied to sparse, large, and imbalanced CF datasets. 
PMF has also been extended to include an adaptive prior on the model 
parameters, and it can generalize adequately, providing accurate 
recommendation to cold-start users. CF RSs are usually biased. A 
typical CF bias source comes from the fact that some users tend to 
highly rate items (mainly 4 and 5 stars), whereas some other users 
tend to be more restrictive in their ratings (mainly 3 and 4 stars). This 
fact leads to the extension of the MF model to handle biased data. An 
user-based rating centrality and an item-based rating centrality [14] 
have been used to improve the accuracy of the regular PMF. These 
centrality measures are obtained by processing the degree of deviation 
of each rating in the overall rating distribution of the user and the item. 
non-Negative Matrix Factorization (NMF) [15] can extract significant 
features from sparse and non-negative CF datasets (please note that 
CF ratings are usually a non-negative number of stars, listened songs, 
watched movies, etc.). When nonnegativity is imposed, prediction 
errors are reduced and the semantic interpretability of hidden factors 
is easier. The Bernoulli Matrix Factorization (BeMF) [16] has been 
designed to provide both prediction and reliability values; this 
model uses the Bernoulli distribution to implement a set of binary 
classification approaches. The results of the binary classification 
are combined by means of an aggregation process. The Bayesian 
non-Negative Matrix Factorization (BNMF) [17] was designed to 
provide useful information about user groups, in addition to the PMF 
prediction results. The authors factorize the rating matrix into two 
nonnegative matrices whose components lie within the range [0, 1]. 
The resulting hidden factors provide an understandable probabilistic 
meaning. Finally, The User Ratings Profile Model (URP) is a generative 
latent variable model [18]; it produces complete rating user profiles. 
In the URP model, first attitudes for each item are generated, then a 
user attitude for the item is selected from the set of existing attitudes. 
URP borrows several concepts from LDA [19] and the multinomial 
aspect model [20].
The set of MF models mentioned above: PMF, Biased Matrix 
Factorization (BiasedMF), NMF, BeMF, BNMF, and URP, can be 
considered representative in the CF area. These models will be used in 
this paper to compare their behavior when applied to representative 
datasets. Specifically, the following quality measures will be tested: 
Mean Absolute Error (MAE), novelty, diversity, precision, recall, 
and Normalized Discounted Cumulative Gain (NDCG). Prediction 
accuracy will be tested using MAE [21], whereas NDCG, Precision 
and Recall [22] will be used to test recommendation accuracy. 
Modern CF models should be tested not only regarding accuracy, but 
also beyond accuracy properties [23]: novelty [24], [25] and diversity 
[26]. Novelty can be defined as the quality of a system to avoid 
redundancy; diversity is a quality that helps to cope with ambiguity 
or under-specification. The models have been tested using four CF 
datasets: MovieLens (100K and 1M versions) [27], Filmtrust [28] and 
MyAnimeList [29]. These are representative open datasets and are 
popular in RS research.
Overall, this paper provides a complete evaluation of MF methods, 
where the PMF, BiasedMF, NMF, BeMF, BNMF, and URP models 
have been tested using representative CF quality measures, both for 
prediction and recommendation, and also beyond accuracy ones. As 
far as we know this is the experimental most complete work evaluating 
current MF models in the CF area.
The rest of the paper is structured as follows: Section II introduces 
the tested models, the experiment design, the selected quality measures, 
and the chosen datasets. Section III shows the obtained results and 
provides their explanations in Section IV. Section V highlights the 
main conclusions of the paper and the suggested future works. Finally, 
a references section lists current research in the area.
II.	 Methods and Experiments
This section abstracts the fundamentals of each baseline model 
(PMF, BiasedMF, NMF, BeMF, BNMF, URP), introduces the tested 
quality measures (MAE, precision, recall, NDCG, novelty, diversity), 
and shows the main parameters of the tested datasets (Movielens, 
FilmTrust, MyAnimeList). Experiments are performed by combining 
the previous entities.
The vanilla MF [13], [30] is used to generate rating predictions from 
a matrix of ratings R. This matrix contains the set of casted ratings 
(explicit or implicit) from a set of users U to a set of items I. Since 
regular users only vote or consume a very limited subset of the available 
items, matrix R is very sparse. The MF key concept is to compress the 
very sparse item and user vectors of ratings to small size and dense 
item and user vectors of real numbers; these small size dense vectors 
can be considered as embeddings, and they usually are called ‘hidden 
factors’, since each embedding factor codes some complex non-lineal 
(‘hidden’) relation of user or item features. The parameter K is usually 
chosen to set the embedding (hidden factors) size. MF makes use of 
two matrices: P(|U|*K) to contain the K hidden factors of each user, 
and Q(|I|*K) to contain the K hidden factors of each item. To predict 
how much a user u likes an item i, we compare each hidden factor of u 
with each corresponding hidden factor of i. Then, the dot product u ⋅ i 
can be used as suitable CF prediction measure. MF predicts ratings by 
minimizing errors between the original R matrix and the predicted  
matrix:
	
(1)
	
(2)
Using gradient descent, we minimize learning errors (differences 
between real ratings r and predicted ratings ).
	
(3)
To minimize the error, we differentiate equation (3) with respect to 
puk and qki:
	
(4)
	
(5)
Introducing the learning rate α, we can iteratively update the 
required hidden factors puk and qki:
	
(6)
	
(7)
CF datasets have biases, since different users vote or consume 
items in different ways. In particular, there are users who are more 
demanding than others when rating products or services. Analogously, 
there are items more valued than others on average. Biased MF [14] is 
designed to consider data biases; The following equations extend the 
previous ones, introducing the bias concept and making the necessary 
regularization to maintain hidden factor values in their suitable range:
	
(8)
where μ, bu, bi are the average bias, the user bias and the item bias.

- 3 -
Article in Press
We minimize the regularized squared error:
	
(9)
where λ is the regularization term.
Obtaining the following updating rules:
	
(10)
	
(11)
	
(12)
	
(13)
NMF [15] can be considered as a regular MF subject to the following 
constraints:
	
(14)
In the NMF case, predictions are made by linearly combining 
positive coefficients (hidden factors). NMF hidden factors are easier 
to semantically interpret than regular MF ones: sometimes it is not 
straightforward to assign semantic meanings to negative coefficient 
values. In the CF context, another benefit of using NMF decomposition 
is the emergence of a natural clustering of users and items. Intuitively, 
users and items can be clustered according to the dominant factor (i.e. 
the factor having the highest value). In the same way, the original 
features (gender, age, item type, item year, etc.) can be grouped 
according to the factor (from the k hidden factors) on which they 
have the greatest influence. This is possible due to the condition of 
positivity of the coefficients.
BeMF [16] is an aggregation-based architecture that combines 
a set of Bernoulli factorization results to provide pairs <prediction, 
reliability>. BeMF uses as many Bernoulli factorization processes as 
possible scores in the dataset. Reliability values can be used to detect 
shilling attacks, to explain the recommendations, and to improve 
prediction and recommendation accuracy [31]. BeMF is a classification 
model based on the Bernoulli distribution. It adequately adapts to the 
expected binary results of each of the possible scores in the dataset. 
Using BeMF, the prediction for user u to item i is a vector of probabilities 
, where 
 is the probability that i is assigned the s-th score 
from user u. The BeMF model can be abstracted as follows:
Let S = {s1, …, sD} be the set of D possible scores in the dataset (e.g. 1 
to 5 stars: D = 5). From R we generate D distinct matrices 
; 
each 
 matrix is a sparse matrix such that 
. BeMF will 
attempt to fit the matrices 
 by performing D parallel MFs
The BeMF assumes that, given the user P matrix and the item Q 
matrix containing k > 0 hidden factors, the rate Rui is a Bernoulli 
distribution with the success probability ψ(Pu . Qi). The mass function 
of this random variable is:
	
(15)
The associated likelihood is:
	
(16)
The BeMF updating equations are:
	
(17)
	 (18)
And the aggregation to obtain the final output Φ:
	
(19)
where 
. Let 
; 
the prediction is: 
, and the reliability is 
.
BNMF [17] provides a Bayesian-based NMF model that not only 
allows accurate prediction of user ratings, but also to find groups of 
users with the same tastes, as well as to explain recommendations. 
The BNMF model approximates the real posterior distribution 
 by the distribution:
	
(20)
where:
•	
 is a random variable from a categorical distribution.
•	
 is a random variable from a Binomial 
distribution (which takes values from 0 to D − 1)
•	
 (a and b are hidden matrices).
•	
 
 
•	
 follows a Dirichlet distribution.
•	
 follows a Beta distribution.
•	
 follows a categorical distribution
•	 λuik are parameters to be learned: 
 
BNMF iteratively approximates parameters 
:
	
(21)
	
(22)
	
(23)
	
(24)
	
(25)
	
(26)
	
(27)
where ψ is the digamma function as the logarithmic derivative of 
the gamma function.
URP is a generative latent variable model [18]. The model assigns 
to each user a mixture of user attitudes. Mixing is performed by a 
Dirichlet random variable:
	 (28)
	
(29)
	
(30)

- 4 -
International Journal of Interactive Multimedia and Artificial Intelligence
	
(31)
	
(32)
	
(33)
In this paper, baseline models will be tested using a) prediction 
measure, b) recommendation measures, and c) beyond accuracy 
measures. The chosen prediction measure is the MAE, where the 
absolute differences of the errors are averaged. Absolute precision 
and relative recall measures are tested to compare the quality 
of an unordered list of N recommendations. The ordered lists of 
recommendations will be compared using the NDCG quality measure. 
From the beyond accuracy metrics, we have selected novelty and 
diversity. Novelty returns the distance from the items the user ‘knows’ 
(has voted or consumed) to his recommended set of items. Diversity 
tells us about the distance between the set of recommended items. 
Recommendations with high novelty values are valuable, since they 
show to the user unknown types of items. Diverse recommendations 
are valuable because they provide different types of items (and each 
type of item can be novel, or not, to the user).
The GroupLens research group [27] made available several CF 
datasets, collected over different intervals of time. MovieLens 100K 
and MovieLens 1M describe 5-star rating and free-text tagging 
activity. These data were created from 1996 to 2018. In the Movielens 
100K dataset, users were selected at random from those who had rated 
at least 20 movies, whereas the MovieLens 1M dataset has not this 
constraint. Only movies with at least one rating or tag are included 
in the dataset. No demographic information is included. Each user 
is represented by an ‘id’, and no other information is provided. The 
dataset files are written as comma-separated values files with a 
single header row. Columns that contain commas (,) areescapedusing 
double-quotes (""). These files are encoded as UTF-8. All ratings are 
contained in the file named ‘ratings.csv’. Each line of this file after 
the header row represents one rating of one movie by one user, 
and has the following format: ‘userId, movieId, rating, timestamp’. 
The lines within this file are ordered first by ‘userId’, then, within 
user, by ‘movieId’. Timestamps represent seconds since midnight 
Coordinated Universal Time (UTC) of January 1, 1970. FilmTrust is 
a small dataset crawled from the entire FilmTrust website in June, 
2011. As the Movielens datasets, it contains ratings voted from users 
to items; additionally, it provides social information structured as a 
graph network. Finally, MyAnimeList contains information about 
anime and ‘otaku’ consumers (anime, manga, video games and 
computers). Each user is able to add ‘animes’ to their completed list 
and give them a rating; this data set is a compilation of those ratings. 
The MyAnimeList CF information is contained in the file ‘Anime.csv’, 
where their main columns are ‘anime_id’: myanimelist.net’s unique 
‘id’ identifying an anime; ‘name’: full name of anime; ‘genre’: comma 
separated list of genres for this anime; ‘type’: movie, TV, OVA, etc; 
‘episodes’: how many episodes in this show; ‘rating’: average rating 
out of 10 for this anime. These datasets are available in the Kaggle and 
GitHub repositories, as well as in the KNODIS research group CF4J 
[32] repository https://github.com/ferortega/cf4j.
Table I contains the values of the main parameters of the 
selected CF data sets: Movielens 100K, Movielens 1M, FilmTrust and 
MyAnimeList. We have run the explained MF models on each of the 
four Table I datasets, testing the chosen quality measures. Please note 
that the MyAnimeList dataset ratings range from 1 to 10 , whereas 
MovieLens datasets range from 1 to 5 and FilmTrust ranges from 0 
to 5 with 0.5 increments. It is also remarkable the sparsity difference 
between FilmTrust and the rest of the tested datasets.
TABLE I. Main Parameter Values of the Tested Datasets
Dataset
#users
#items
#ratings
Scores
Sparsity
MovieLens100k
943
1682
99,831
1 to 5
93.71
MovieLens1M
6,040
3,706
911,031
1 to 5
95.94
MyAnimeList
19,179
2,692
548,967
1 to 10
98.94
FilmTrust
1,508
2,071
35,497
0 to 5
87.98
Experiments have been performed using random search and 
applying four-fold cross-validation. To ensure reproducibility, we 
used a seed in the random process. Results shown in the paper are 
the average of the partial results obtained by setting the number k of 
latent factors to {4, 8, 12}, and the number of MF iterations to {20, 50, 
75, 100}. Additionally, to run the PMF, BiasedMF, and BeMF models, 
both the learning rate and the regularization parameters have been 
set to {0.001, 0.01, 0.1, 1.0}. The BNMF model requires two specific 
parameters: α and β; the chosen values por these parameters are:  
α = {0.2, 0.4, 0.6, 0.8}, and β = {5, 15, 25}. The tested number of 
recommendations N ranges from 1 to 10. We have used 4 stars as 
recommendation threshold θ for datasets whose ratings range from 
1 to 5 , while the testing threshold has been 8 when MyAnimeList 
was chosen. The experiments have been implemented using the 
open framework [33] and the code has been made available at  
https://github.com/KNODIS-Research-Group/choice-of-mf-models.
III.	Results
The prediction quality obtained by testing each baseline model is 
shown in table II. The bold numbers correspond to the best results, 
and, of them, those highlighted gray are the top ones. As can be seen, 
BiasedMF and BNMF models provide the best CF prediction results. 
PMF, NMF, BeMF and URP seem to be more sensitive to the type of 
CF input data.
TABLE II. Prediction Quality Results Using the Mean Absolute Error 
(MAE). The Lower the Error Value, the Better the Result
PMF
BiasedMF
NMF
BeMF
BNMF
URP
MovieLens 100K
0.770
0.754
0.804
0.805
0.748
0.837
MovieLens 1M
0.729
0.712
0.744
0.748
0.693
0.795
FilmTrust
0.863
0.652
0.876
0.712
0.666
0.831
MyAnimeList
1.110
0.926
1.147
1.034
0.943
1.159
Fig. 1 shows the quality of recommendation obtained using the 
Precision measure. The most remarkable in Fig. 1 is the superiority of 
the models PMF and BiasedMF. For the remaining models, URP and 
BeMF provide the worst results, whereas the nonnegative NMF and 
BNMF return an intermediate quality. It is important to highlight the 
good performance of the BiasedMF model for both the prediction and 
the recommendation tasks.
To test the quality of CF recommendations of unordered 
recommendations, precision and recall measures are usually processed, 
and they are provided separately, or joined in the F1 score. We have 
done these experiments and we have not found appreciable differences 
in Recall values for the tested models in the selected datasets. In order 
to maintain the paper as short as possible, Fig. 2 only shows the Recall 
results obtained by processing the Movielens 1M dataset. Results from 
the rest of datasets are very similar; consequently, the Recall quality 
measure does not help, in this context, to find out the best MF models 
in the CF area.

- 5 -
Article in Press
Fig. 2. Recall Recommendation quality results obtained in the 
MovieLens 1M dataset. The results of the other three considered 
datasets are very similar to this one; to maintain the paper as short as 
possible, the results of other datasets are not shown.
PMF
0,6
0,5
0,4
0,3
0,2
0,1
0
1
3
5
Number of recommendations
Recall
7
9
BiasedMF
BeMF
NMF
BNMF
URP
Fig. 2. Recall Recommendation quality results obtained in the MovieLens 1M 
dataset. The results of the other three considered datasets are very similar 
to this one; to maintain the paper as short as possible, the results of other 
datasets are not shown.
 
In the RSs field, recommendations are usually provided in an 
ordered list. Users’ trust in RSs quickly decays when the first 
recommendations in the list do not meet their expectations; for that 
reason, the NDCG quality measure particularly penalizes errors in the 
first recommendations of the list. Fig. 3 (NDCG results) shows a similar 
behavior to Fig. 1, where the BiasedMF and PMF models provide the 
best recommendation quality. So, these two models perform fine both 
in recommending ordered and unordered lists.
Traditionally, RSs have been evaluated attending to their 
prediction and recommendation accuracy; nevertheless, there are 
some other valuable beyond accuracy aims and their corresponding 
quality measures. The Diversity measure tests the variety of 
recommendations, penalizing recommendations focused on the 
same ‘area’ (Star Wars III, Star Wars I, Star Wars V, Han Solo). Fig. 4 
shows the Diversity results obtained by testing the selected models; 
the most diverse recommendations are usually returned when the 
BiasedMF model is used, followed by both PMF and NMF. This fact 
is particularly interesting, since it is not intuitive that the same 
model (BiasedMF) can, simultaneously, provide accurate and diverse 
recommendations.
Novelty is an important beyond accuracy objective in RSs. Users 
appreciate accurate recommendations, but they also want to discover 
unexpected (and accurate enough) recommendations. Please note that 
a set of recommendations can be diverse and not novel, as they can 
be novel and not diverse. It would be great to receive, simultaneously, 
accurate, novel, and diverse recommendations, but usually improving 
some of the objectives leads to worsening others. Fig. 5 shows the results 
of the novelty quality measure: NMF returns novel recommendations, 
compared to other models; NMF provides a balance between accuracy 
and novelty. BiasedMF and PMF also provide novel recommendations 
compared to BeMF and URP.
PMF
0,85
0,8
0,75
0,7
0,65
0,9
0,95
0,85
0,8
0,75
0,7
1
3
5
Number of recommendations
Precision
7
9
1
3
5
7
9
BiasedMF
BeMF
NMF
BNMF
URP
PMF
BiasedMF
BeMF
NMF
BNMF
URP
PMF
0,85
0,81
0,83
0,79
0,75
0,77
0,70
1
3
5
Number of recommendations
Precision
7
9
BiasedMF
BeMF
NMF
BNMF
URP
PMF
0,85
0,87
0,89
0,81
0,83
0,79
0,75
0,77
0,73
1
3
5
Number of recommendations
Number of recommendations
Precision
Precision
7
9
BiasedMF
BeMF
NMF
BNMF
URP
(a)
(b)
(c)
(d)
Fig. 1. Precision recommendation quality results; a) MovieLens100K, b) MovieLens 1M, c) FilmTrust, d) MyAnimeList. The higher the values, the better the 
results.

- 6 -
International Journal of Interactive Multimedia and Artificial Intelligence
PMF
0,85
0,8
0,75
0,7
0,65
0,6
0,65
0,67
0,69
0,71
0,73
0,75
0,77
0,79
0,81
0,83
1
3
5
Number of recommendations
NDCG
NDCG
7
9
1
3
5
7
9
BiasedMF
BeMF
NMF
BNMF
URP
PMF
BiasedMF
BeMF
NMF
BNMF
URP
PMF
0,95
0,9
0,85
0,75
0,8
0,7
1
3
5
Number of recommendations
NDCG
7
9
BiasedMF
BeMF
NMF
BNMF
URP
PMF
0,79
0,82
0,73
0,76
0,7
0,64
0,67
0,61
1
3
5
Number of recommendations
Number of recommendations
NDCG
7
9
BiasedMF
BeMF
NMF
BNMF
URP
(a)
(b)
(c)
(d)
Fig. 3. Normalized Discounted Cumulative Gain recommendation quality results; a) MovieLens100K, b) MovieLens 1M, c) FilmTrust, d) MyAnimeList. The 
higher the values, the better the results.
PMF
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
0.001
Diversity
Diversity
2
4
6
8
10
BiasedMF
BeMF
NMF
BNMF
URP
PMF
BiasedMF
BeMF
NMF
BNMF
URP
PMF
Diversity
BiasedMF
BeMF
NMF
BNMF
URP
PMF
Number of recommendations
2
4
6
8
10
Number of recommendations
2
4
6
8
10
Number of recommendations
2
4
6
8
10
Number of recommendations
Diversity
BiasedMF
BeMF
NMF
BNMF
URP
(a)
(b)
(c)
(d)
Fig. 4. Diversity beyond accuracy results; a) MovieLens100K, b) MovieLens 1M, c) FilmTrust, d) MyAnimeList. The higher the values, the better the results.

- 7 -
Article in Press
IV.	Discussion
In this section, we provide a comparative discussion of the most 
adequate MF models when applied to a set of different CF databases. To 
judge each MF model, we simultaneously measure a set of conflicting 
goals: prediction accuracy, recommendation accuracy (unordered and 
ordered lists) and beyond accuracy aims. We will promote some MF 
models as ‘winners’, attending to their high performance (overall 
quality results) when applied to the tested datasets. We also provide 
a summary table to better identify those MF models that perform 
particularly fine on any individual quality objective: novelty, diversity, 
precision, etc., as well as any combination of those quality measures.
TABLE III. MF Models Comparative
PMF
BiasedMF
NMF
BeMF
BNMF
URP
MAE
++
+++
+
+
+++
+
Precision
+++
+++
++
+
++
+
NDCG
+++
+++
+
+
+
+
Diversity
++
+++
++
+
+
+
Novelty
++
++
+++
+
+
+
Total
12
14
9
5
8
5
Table III summarizes the results of this section. BiasedMF is the 
most appropriate model when novelty of recommendations is not 
a particularly relevant issue. PMF can be used instead BiasedMF 
when simplicity is required (e.g. educational environments). BeMF 
should only be used when reliability information is required or when 
reliability values are used to improve accuracy [31]. NMF and BNMF 
are adequate when semantic interpretation of hidden factors is needed. 
NMF is the best choice when we want to be recommended with novel 
items. BNMF provides good accuracy and it is designed to recommend 
to group of users.
V.	 Conclusions
This paper makes a comparative of relevant MF models applied 
to 
collaborative 
filtering 
recommender 
systems. 
Prediction, 
recommendation, and beyond accuracy quality measures have been 
tested on four representative datasets. The results show the superiority 
of the BiasedMF model, followed by the PMF one. BiasedMF arises 
as the most convenient model when novelty is not a particularly 
important feature. PMF combines simplicity with accuracy; it can be 
the best choice for educational or not commercial implementations. 
NMF and BNMF are adequate when we want to do a semantic 
interpretation of their non-negative hidden factors. NMF is preferable 
to BNMF when beyond accuracy (novelty and diversity) results are 
required, whereas it is better to make use of BNMF when prediction 
accuracy is required or when recommending to group of users, or 
when explaining recommendations is needed. NMF and BiasedMF are 
the best choices when beyond accuracy aims are selected, whereas 
PMF or BiasedMF performs particularly well in recommendation task, 
both for unordered and ordered options. BeMF can only be selected 
when reliability values are required or when they are used to improve 
accuracy. Finally, URP does not seem to be an adequate choice in any 
of the combinations tested. As future work, it is proposed to add new 
MF models, quality measures, and datasets to the experiments, as well 
as the possibility of including neural network models such as DeepMF 
or Neural Collaborative Filtering (NCF).
PMF
9,4
9,6
9,8
10
10,2
10,4
10
10
10
10
10
11
11
8,6
8,8
9,2
9
9,4
9,6
9,8
06
06
07
07
07
07
07
Novelty
Novelty
BiasedMF
BeMF
NMF
BNMF
URP
PMF
BiasedMF
BeMF
NMF
BNMF
URP
PMF
Novelty
BiasedMF
BeMF
NMF
BNMF
URP
PMF
Number of recommendations
Number of recommendations
1
3
5
7
9
1
3
5
7
9
1
3
5
7
9
1
3
5
7
9
Number of recommendations
Number of recommendations
Novelty
BiasedMF
BeMF
NMF
BNMF
URP
(a)
(b)
(c)
(d)
Fig. 5. Novelty beyond accuracy quality results; a) MovieLens100K, b) MovieLens 1M, c) FilmTrust, d) MyAnimeList. The higher the values, the better the 
results.

- 8 -
International Journal of Interactive Multimedia and Artificial Intelligence
Acknowledgments
This work has been co-funded by the Ministerio de Ciencia e 
Innovación of Spain and European Regional Development Fund 
(FEDER) under grants PID2019-106493RB-I00 (DL-CEMG) and the 
Comunidad de Madrid under Convenio Plurianual with the Universidad 
Politécnica de Madrid in the actuation line of Programa de Excelencia 
para el Profesorado Universitario.
References
[1]	
Z. Batmaz, A. Yurekli, A. Bilge, C. Kaleli, “A review on deep learning for 
recommender systems: challenges and remedies,” Artificial Intelligence 
Review, vol. 52, no. 1, pp. 1–37, 2019. 
[2]	
J. Bobadilla, S. Alonso, A. Hernando, “Deep learning architecture for 
collaborative filtering recommender systems,” Applied Sciences, vol. 10, 
no. 7, p. 2441, 2020. 
[3]	
B. Zhu, R. Hurtado, J. Bobadilla, F. Ortega, “An efficient recommender 
system method based on the numerical relevances and the non-numerical 
structures of the ratings,” IEEE Access, vol. 6, pp. 49935–49954, 2018.
[4]	
J. Bobadilla, R. Lara-Cabrera, Á. González-Prieto, F. Ortega, “Deepfair: 
Deep learning for improving fairness in recommender systems,” 
International Journal of Interactive Multimedia and Artificial Intelligence, 
vol. 6, no. 6, pp. 86–94, 2021, doi: 10.9781/ijimai.2020.11.001. 
[5]	
J. Carbó, J. M. Molina, J. Dávila, “Fuzzy referral based cooperation in 
social networks of agents,” AI Communications, vol. 18, pp. 1–13, 2005. 1. 
[6]	
D. Medel, C. González-González, S. V. Aciar, “Social relations and 
methods in recommender systems: A systematic review,” International 
Journal of Interactive Multimedia and Artificial Intelligence, vol. 7, no. 4, p. 
7, 2022, doi: 10.9781/ijimai.2021.12.004. 
[7]	
M. Caro-Martínez, G. Jiménez-Díaz, J. A. Recio- García, “Local model-
agnostic explanations for black-box recommender systems using 
interaction graphs and link prediction techniques,” International Journal 
of Interactive Multimedia and Artificial Intelligence, vol. InPress, no. 
InPress, p. 1, 2021, doi: 10.9781/ijimai.2021.12.001. 
[8]	
S. Afef, Z. Brahmi, M. Gammoudi, “Trust-based recommender systems: 
An overview,” in 27th IBIMA Conference, 05 2016. 
[9]	
I. Pinyol, J. Sabater-Mir, “Computational trust and reputation models for 
open multi-agent systems: a review,” Artificial Intelligence Review, vol. 40, 
pp. 1–25, Jun 2013, doi: 10.1007/s10462-011-9277-z.
[10]	 Y. Deldjoo, M. Schedl, P. Cremonesi, G. Pasi, “Recommender systems 
leveraging multimedia content,” ACM Computing Surveys (CSUR), vol. 53, 
no. 5, pp. 1–38, 2020. 
[11]	 S. Kulkarni, S. F. Rodd, “Context aware recommendation systems: A 
review of the state of the art techniques,” Computer Science Review, vol. 
37, p. 100255, 2020. 
[12]	 S. Forouzandeh, K. Berahmand, M. Rostami, “Presentation of a 
recommender system with ensemble learning and graph embedding: a 
case on movielens,” Multimedia Tools and Applications, vol. 80, no. 5, pp. 
7805–7832, 2021. 
[13]	 R. Salakhutdinov, A. Mnih, “Probabilistic matrix factorization,” in 
Proceedings of the 20th International Conference on Neural Information 
Processing Systems, NIPS’07, Red Hook, NY, USA, 2007, p. 1257–1264, 
Curran Associates Inc. 
[14]	 Z. Wu, H. Tian, X. Zhu, S. Wang, “Optimization matrix factorization 
recommendation algorithm based on rating centrality,” in International 
Conference on Data Mining and Big Data, 2018, pp. 114–125, Springer. 
[15]	 C. Févotte, J. Idier, “Algorithms for nonnegative matrix factorization with 
the β-divergence,” Neural computation, vol. 23, no. 9, pp. 2421–2456, 2011. 
[16]	 F. Ortega, R. Lara-Cabrera, Á. González-Prieto, J. Bobadilla, “Providing 
reliability in recommender systems through bernoulli matrix 
factorization,” Information Sciences, vol. 553, pp. 110–128, 2021.
[17]	 A. Hernando, J. Bobadilla, F. Ortega, “A non negative matrix factorization 
for collaborative filtering recommender systems based on a bayesian 
probabilistic model,” Knowledge-Based Systems, vol. 97, pp. 188–202, 2016. 
[18]	 B. M. Marlin, “Modeling user rating profiles for collaborative filtering,” 
Advances in neural information processing systems, vol. 16, 2003. 
[19]	 D. M. Blei, A. Y. Ng, M. I. Jordan, “Latent dirichlet allocation,” Journal of 
machine Learning research, vol. 3, no. Jan, pp. 993–1022, 2003. 
[20]	 T. Hofmann, “Learning what people (don’t) want,” in European Conference 
on Machine Learning, 2001, pp. 214– 225, Springer. 
[21]	 A. Gunawardana, G. Shani, “Evaluating recommender systems,” in 
Recommender systems handbook, Springer, 2015, pp. 265–308. 
[22]	 C. C. Aggarwal, “Evaluating recommender systems,” in Recommender 
systems, Springer, 2016, pp. 225–254. 
[23]	 J. Bobadilla, A. Gutiérrez, S. Alonso, Á. González- Prieto, “Neural 
collaborative filtering classification model to obtain prediction 
reliabilities,” International Journal of Interactive Multimedia and Artificial 
Intelligence, vol. 7, no. 4, pp. 18–26, 2022, doi: 10.9781/ijimai.2021.08.010. 
[24]	 S. Vargas, P. Castells, “Rank and relevance in novelty and diversity 
metrics for recommender systems,” in Proceedings of the fifth ACM 
conference on Recommender systems, 2011, pp. 109–116. 
[25]	 P. Castells, S. Vargas, J. Wang, “Novelty and diversity metrics for 
recommender systems: choice, discovery and relevance,” in Proceedings 
of the 33rd European Conference on Information Retrieval (ECIR’11), 2011. 
[26]	 S. Vargas, P. Castells, D. Vallet, “Intent-oriented diversity in recommender 
systems,” in Proceedings of the 34th international ACM SIGIR conference on 
Research and development in Information Retrieval, 2011, pp. 1211– 1212. 
[27]	 F. M. Harper, J. A. Konstan, “The movielens datasets: History and 
context,” Acm transactions on interactive intelligent systems (tiis), vol. 5, 
no. 4, pp. 1–19, 2015, doi: https://doi.org/10.1145/2827872. 
[28]	 J. Golbeck, J. A. Hendler, “Filmtrust: movie recommendations using trust 
in web-based social networks,” CCNC 2006. 2006 3rd IEEE Consumer 
Communications and Networking Conference, 2006., vol. 1, pp. 282–286, 
2006, doi: 10.1109/CCNC.2006.1593032. 
[29]	 J. Miller, G. Southern, “Recommender system for animated video,” Issues 
in Information Systems, vol. 15, no. 2, pp. 321–7, 2014. 
[30]	 Y. Koren, R. Bell, C. Volinsky, “Matrix factorization techniques for 
recommender systems,” Computer, vol. 42, no. 8, pp. 30–37, 2009.
[31]	 J. Bobadilla, A. Gutiérrez, S. Alonso, Á. González- Prieto, “Neural 
collaborative filtering classification model to obtain prediction 
reliabilities,” International Journal of Interactive Multimedia and Artificial 
Intelligence, vol. 7, no. 4, pp. 18–26, 2022, doi: 10.9781/ijimai.2021.08.010. 
[32]	 F. Ortega, B. Zhu, J. Bobadilla, A. Hernando, “Cf4j: Collaborative filtering 
for java,” Knowledge- Based Systems, vol. 152, pp. 94–99, 2018, doi: https://
doi.org/10.1016/j.knosys.2018.04.008. 
[33]	 F. Ortega, J. Mayor, D. López-Fernández, R. Lara- Cabrera, “Cf4j 
2.0: Adapting collaborative filtering for java to new challenges of 
collaborative filtering based recommender systems,” Knowledge-Based 
Systems, vol. 215, p. 106629, 2021.
Jorge Dueñas-Lerín
Jorge Dueñas-Lerín received the B.S. in computer science 
from the Universidad Politécnica de Madrid. He received 
the M.S. degree in highschool, vocational training and 
languages teacher from the Universidad Nacional de 
Educación a Distancia. He is currently a Ph.D. student as 
part of the KNOledge Discovery and Information Systems 
- KNODIS research group.
Jesús Bobadilla
Jesús Bobadilla received the B.S. and the Ph.D. degrees 
in computer science from the Universidad Politécnica de 
Madrid and the Universidad Carlos III. Currently, he is a 
full professor with the Department of Applied Intelligent 
Systems, Universidad Politécnica de Madrid. He is a 
habitual author of programming languages books working 
with McGraw-Hill, Ra-Ma and Alfa Omega publishers. 
His research interests include information retrieval, recommender systems and 
speech processing. He oversees the FilmAffinity.com research teamworking on 
the collaborative filtering kernel of the web site. He has been a researcher into 
the International Computer Science Institute at Berkeley University and into the 
Sheffield University.

- 9 -
Article in Press
Fernando Ortega
Fernando Ortega was born in Madrid, Spain, in 1988. 
He received the B.S. degree in software engineering, the 
M.S. degree in artificial intelligence, and the Ph.D. degree 
in computer sciences from theUniversidad Politécnica 
de Madrid, in 2010, 2011, and 2015, respectively. He is 
currently Associate Professor in the Universidad Politécnica 
de Madrid. He is author of more than 50 research papers 
in most prestigious international journals. He leads several national projects 
to include machine learning algorithms into the society. His research interests 
include machine learning, data analysis, and artificial intelligence. He is the 
head researcher of the KNOledge Discovery and Information Systems - 
KNODIS research group.
Abraham Gutiérrez
Abraham Gutiérrez received the B.S. and the Ph.D. degrees 
in computer science from the Universidad Politécnica de 
Madrid. Currently, he is currently an associate professor 
with the Department of Information Systems, Universidad 
Politécnica de Madrid. He is the author of search papers 
in most prestigious international journals. He is a habitual 
author of programming languages books working with 
McGraw-Hill, Ra-Ma and Alfa Omega publishers. His research interests include 
P-Systems, machine learning, data analysis and artificial intelligence. He is in 
charge of this group innovation issues, including the commercial projects.
View publication stats
"
Recommender systems survey.pdf,"Recommender systems survey
J. Bobadilla ⇑, F. Ortega, A. Hernando, A. Gutiérrez
Universidad Politécnica de Madrid, Ctra. De Valencia, Km. 7, 28031 Madrid, Spain
a r t i c l e
i n f o
Article history:
Received 7 October 2012
Received in revised form 4 March 2013
Accepted 19 March 2013
Available online 6 April 2013
Keywords:
Recommender systems
Collaborative ﬁltering
Similarity measures
Evaluation metrics
Prediction
Recommendation
Hybrid
Social
Internet of things
Cold-start
a b s t r a c t
Recommender systems have developed in parallel with the web. They were initially based on demo-
graphic, content-based and collaborative ﬁltering. Currently, these systems are incorporating social infor-
mation. In the future, they will use implicit, local and personal information from the Internet of things.
This article provides an overview of recommender systems as well as collaborative ﬁltering methods
and algorithms; it also explains their evolution, provides an original classiﬁcation for these systems, iden-
tiﬁes areas of future implementation and develops certain areas selected for past, present or future
importance.
 2013 Elsevier B.V. All rights reserved.
1. Introduction
Recommender Systems (RSs) collect information on the prefer-
ences of its users for a set of items (e.g., movies, songs, books, jokes,
gadgets, applications, websites, travel destinations and e-learning
material). The information can be acquired explicitly (typically
by collecting users’ ratings) or implicitly [134,60,164] (typically
by monitoring users’ behavior, such as songs heard, applications
downloaded, web sites visited and books read). RS may use demo-
graphic features of users (like age, nationality, gender). Social
information, like followers, followed, twits, and posts, is commonly
used in Web 2.0. There is a growing tend towards the use of infor-
mation from Internet of things (e.g., GPS locations, RFID, real-time
health signals).
RS make use of different sources of information for providing
users with predictions and recommendations of items. They try
to balance factors like accuracy, novelty, dispersity and stability
in the recommendations. Collaborative Filtering (CF) methods play
an important role in the recommendation, although they are often
used along with other ﬁlterning techniques like content-based,
knowledge-based or social ones.
CF is based on the way in which humans have made decisions
throughout history: besides on our own experiences, we also base
our decisions on the experiences and knowledge that reach each of
us from a relatively large group of acquaintances.
Recently, RS implementation in the Internet has increased,
which has facilitated its use in diverse areas [171]. The most com-
mon research papers are focused on movie recommendation stud-
ies [53,230]; however, a great volume of literature for RS is
centered on different topics, such as music [134,162,216], televi-
sion [238,18], books [164,88], documents [206,184,183,185], e-
learning [241,30], e-commerce [104,54], applications in markets
[67] and web search [154], among others.
The kinds of ﬁltering most used at the beginning of the RS (col-
laborative, content-based and demographic) were described in
[177]. Breese et al. [43] evaluated the predictive accuracy of differ-
ent algorithms for CF; later, the classical paper [94] describes the
base for evaluating the Collaborative Filtering RS.
The evolution of RS has shown the importance of hybrid tech-
niques of RS, which merge different techniques in order to get
the advantages of each of them. A survey focused on the hybrid
RS has been presented in [47]. However, it does not deal with
the role of social-ﬁltering, a technique which has become more
popular in the recent years through social networks.
The neighborhood-based CF has been the recommendation
method most popular at the beginning of the RS; Herlocker et al.
[93] provides a set of guidelines for designing neighborhood-based
prediction systems. Adomavicius and Tuzhilin [3] present an over-
view on the RS ﬁeld standing out the most complex areas on which
0950-7051/$ - see front matter  2013 Elsevier B.V. All rights reserved.
http://dx.doi.org/10.1016/j.knosys.2013.03.012
⇑Corresponding author. Tel.: +34 913365133; fax: +34 913367527.
E-mail address: jesus.bobadilla@upm.es (J. Bobadilla).
Knowledge-Based Systems 46 (2013) 109–132
Contents lists available at SciVerse ScienceDirect
Knowledge-Based Systems
journal homepage: www.elsevier.com/locate/knosys

researchers in RS should focus in the ‘‘next generation of RS’’: lim-
ited content analysis and overspecialization in content-based
methods, cold-start and sparsity in CF methods, model-based tech-
niques, nonintrusiveness, ﬂexibility (real-time customization), etc.
While researchers have been developing RS, different survey
papers have been published summarizing the most important is-
sues in this ﬁeld. In view of the impossibility of showing every de-
tail of all these techniques in just a paper, this publication selects
those issues the authors have felt most suitable to understand
the evolution of RS.
While the existing surveys focus on the most relevant methods
and algorithms of the RS ﬁeld, our survey instead tries to enhance
the evolution of the RS: from a ﬁrst phase based on the tradi-
tional Web to the present second phase based on social Web,
which is presently progressing to a third phase (Internet of
things). With the purpose of being useful to the new readers of
RS ﬁeld, we have included in this survey some traditional topics:
RS foundations, k-Nearest Neighbors algorithm, cold-start issues,
similarity measures, and evaluation of RS. The rest of the paper
deals with novel topics that existing surveys do not consider.
Through this survey, advanced readers in RS will study in depth
concepts, classiﬁcations and approaches related to social informa-
tion (social ﬁltering: followers, followed, trust, reputation, credi-
bility, content-based ﬁltering of social data; social tagging and
taxonomies), recommending to groups of users and explaining
recommendations. Readers interested in brand new and future
applications will ﬁnd this survey useful since it informs about
the most recent works in location-aware RS trends and bio-in-
spired approaches. They will also discover some important issues,
such as privacy, security, P2P information and Internet of things
use (RFID data, health parameters, surveillance data, teleopera-
tion, telepresence, etc.).
According to the idea that RS tend to make use of different
sources of information (collaborative, social, demographic, content,
knowledge-based, geographic, sensors, tags, implicit and explicit
data acquisition, etc.), this survey emphasizes hybrid architectures,
based on making recommendations through different known tech-
nologies (each one designed on behalf of a speciﬁc source of
information).
Much of the quality of a survey can be measured by an appro-
priate choice of its references. This survey contains 249 references
systematically obtained, which have been selected taking into ac-
count factors like the number of recent citations and the impor-
tance of the journal in which the paper has been published.
The remainder of this article is structured as follows: In Sec-
tion 2, we explain concisely the methodology used to select the
most signiﬁcative papers on the RS ﬁeld. Section 3 describes the
RS foundations: methods, algorithms and models used for provid-
ing recommendations based from the information of the tradi-
tional
web:
ratings,
demographic
data
and
item
data
(CF,
demographic ﬁltering, content-based ﬁltering and hybrid ﬁltering).
Section 4 describes measures for evaluating the quality of the RS
predictions and recommendations. Section 5 shows the use of so-
cial information from Web 2.0 for making recomendations through
concepts like trust, reputation and credibility. We will also de-
scribe techniques based on content-based for social information
(e.g. tags and posts). Section 6 focusses on two important areas
(although not very well studied yet): recommendation to group
of users and explanation of recommendations. Section 7 focusses
on recommender system trends, covering bio-inspired approaches
and Web 3.0 information ﬁltering such as location-aware RS. Sec-
tion 8 explains related works and the original contributions of this
survey.
The concluding section summarizes the RS history and focuses
on the type of data used as well as the development of algorithms
and evaluation measures. The conclusions section also indicates
seven new areas that we consider likely to be the focus of RS re-
search in the scientiﬁc community in the near future.
2. Methodology
An initial study was performed to determine the most represen-
tative topics and terms in the RS ﬁeld. First, 300 RS papers were se-
lected from journals, with a higher priority for current and for
often-cited articles. Next, we extracted from these 300 papers the
most signiﬁcant terms. We gave the most emphasis to keywords,
less emphasis to titles and, ﬁnally, the least emphasis to abstracts.
We have overlooked common words, like articles, prepositions
and general-use words from the remaining pool, we selected 300
terms represented in the RS ﬁeld. From a matrix of arti-
cles  words, wherein we stored the importance of each word from
each article, we generated a tree of relationships between the
words. Fig. 1 depicts the most signiﬁcant section of the graph
(due to space constraints, the entire tree is not shown, but it is pro-
vided as additional material in Fig. 1 AdditionalData.png). The short
distances between words indicate the highest similarities; warm
colors indicate a greater reliability for the relationships. The size
of the nodes indicates the importance of the words as a function
of the parameters Nk, Nt, Na (number of signiﬁcative words in the
keywords, title and abstract) and Nk
w; Nt
w; Na
w (number of times that
the word w appears in the keywords, title and abstract). The equa-
tion used to determine the importance of each word w is as
follows:
fw ¼ 1
3
Nk
w
Nk þ
Nt
w
Nt log Na
Nt
þ Na
w
Na Na
Nt
 
!
Example: we will consider a paper where Nk = 5 keywords, Nt = 11
words in the title, and Na = 52 words of abstract length. We will
get the values of ffactorization and fmatrix, where the word ‘factorization’
appears once as a keyword, once in the title and three times in the
abstract; the word ‘matrix’ does not appear as a keyword, but it is
contained once in the title and twice in the abstract. The importance
of these words will be:
ffactorization ¼ 1
3
1
5 þ
1
11 log 52
11
þ
3
52 52
11
 
!
¼ 0:09
f matrix ¼ 1
3
0
5 þ
1
11 log 52
11
þ
2
52 52
11
 
!
¼ 0:02
The information depicted in Fig. 1 is used to identify the most
relevant aspects of RS. They are represented by the most signiﬁcant
words in the graph and the related terms. The articles referenced
herein were chosen based on the following criteria: (a) the tran-
scendence of the subject according to the importance of the words
in Fig. 1; (b) its historical contribution (a signiﬁcant fraction of the
classic reference articles are included); (c) the number of times the
article is cited; (d) articles published in journals with an impact
factor were preferred over conferences and workshops; and (e) re-
cent articles were preferred over articles published many years
ago. Fig. 2 shows a temporal distribution for the referenced papers.
We use the clusters of words in Fig. 1 to structure the explica-
tions of the survey. For each concept explained: (1) we have ob-
tained
their
keywords
and
all
the
words
related
to
them
according to Fig. 1; (2) we have identiﬁed, among the set of 300 pa-
pers, those which are more related to the set of words associated to
the concept; (3) we have selected the subset of papers which deal
with the concept, giving priority to those with high values in crite-
ria like importance of the paper and the number of cites; and (4)
we have tried to balance the number of times a paper is referenced
in our survey, aiming to reference most of the 300 papers selected.
110
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

3. Recommender systems foundations
This section presents the most relevant concepts on which the
traditional RS are based. Here, we provide general descriptions
on the classical taxonomies, algorithms, methods, ﬁltering ap-
proaches, databases, etc. Besides, we show a graphic depicting
the traditional models of recommendations and their relations.
Next, we will describe the cold-start problem, which will illustrate
the difﬁculty of making collaborative recommendation when the
RS contains a small amount of data. Next, we will describe the
kNN algorithm; the most used algorithm for implementing RS
based on CF. Finally, we will describe different proposed similarity
measures for comparing users or items. We will show graphics for
measuring the quality of these similarity measures.
3.1. Fundamentals
The process for generating an RS recommendation is based on a
combination of the following considerations:
 The type of data available in its database (e.g., ratings, user reg-
istration information, features and content for items that can be
ranked, social relationships among users and location-aware
information).
 The ﬁltering algorithm used (e.g., demographic, content-based,
collaborative, social-based, context-aware and hybrid).
 The model chosen (e.g., based on direct use of data: ‘‘memory-
based,’’ or a model generated using such data: ‘‘model-based’’).
 The employed techniques are also considered: probabilistic
approaches, Bayesian networks, nearest neighbors algorithm;
bio-inspired algorithms such as neural networks and genetic
algorithms; fuzzy models, singular value decomposition tech-
niques to reduce sparsity levels, etc.
 Sparsity level of the database and the desired scalability.
 Performance of the system (time and memory consuming).
 The objective sought is considered (e.g., predictions and top N
recommendations) as well as
 The desired quality of the results (e.g., novelty, coverage and
precision).
Research in RS requires using a representative set of public dat-
abases to facilitate investigations on the techniques, methods and
algorithms developed by researchers in the ﬁeld. Through these
databases, the scientiﬁc community can replicate experiments to
validate and improve their techniques. Table 1 lists the current
public databases referenced most often in the literature. Last.Fm
and Delicious incorporate implicit ratings and social information;
their data were generated from the versions released in the HetRec,
2011 data sets, hosted by the GroupLens research Group.
The internal functions for RS are characterized by the ﬁltering
algorithm. The most widely used classiﬁcation divides the ﬁltering
algorithms into [3,51,203]: (a) collaborative ﬁltering, (b) demo-
graphic ﬁltering, (c) content-based ﬁltering and (d) hybrid ﬁltering.
Fig. 1. Words represented in the recommender systems research ﬁeld. Short distances indicate higher similarities, and a warm color indicates greater reliability. The size of
the nodes is proportional to the importance of the words.
Fig. 2. Temporal distribution for the referenced papers.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
111

Content-based ﬁltering [131,11,158] makes recommendations
based on user choices made in the past (e.g. in a web-based e-com-
merce RS, if the user purchased some ﬁction ﬁlms in the past, the
RS will probably recommend a recent ﬁction ﬁlm that he has not
yet purchased on this website). Content-based ﬁltering also gener-
ates recommendations using the content from objects intended for
recommendation; therefore, certain content can be analyzed, like
text, images and sound. From this analysis, a similarity can be
established between objects as the basis for recommending items
similar to items that a user has bought, visited, heard, viewed
and ranked positively.
Demographic ﬁltering [177,126,185] is justiﬁed on the principle
that individuals with certain common personal attributes (sex,
age, country, etc.) will also have common preferences.
Collaborative Filtering [3,94,92,51,212] allows users to give rat-
ings about a set of elements (e.g. videos, songs, ﬁlms, etc. in a CF
based website) in such a way that when enough information is
stored on the system, we can make recommendations to each user
based on information provided by those users we consider to have
the most in common with them. CF is an interesting open research
ﬁeld [232,34,32]. As noted earlier, user ratings can also be
Table 1
Most often used memory-based recommender systems public databases.
Without social information
With social information (hosted by the GroupLens)
MovieLens 1M
MovieLens 10M
Netﬂix
Jester
EachMovie
Book-crossing
ML
Last.Fm
Delicious
Ratings
1 million
10 million
100 million
4.1 million
2.8 million
1.1 million
855,598
92,834
104,833
Users
6040
71,567
480,189
73,421
72,916
278,858
2113
1892
1867
Items
3592
10,681
17,770
100
1628
271,379
10,153
17,632
69,226
Range
{1,. . .,5}
{1,. . .,5}
{1,. . .,5}
10, 10
[0,1]
{1,. . .,10}
{1,. . .,5}
Implicit
Implicit
Tags
N/A
N/A
N/A
N/A
N/A
N/A
13222
11946
53388
Tags assignment
N/A
N/A
N/A
N/A
N/A
N/A
47957
186479
437593
Friends relations
N/A
N/A
N/A
N/A
N/A
N/A
N/A
25434
15328
Items
Movies
Movies
Movies
Jokes
Movies
Books
Movies
Music
URL’s
Fig. 3. Traditional models of recommendations and their relationships.
112
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

implicitly acquired (e.g., number of times a song is heard, informa-
tion consulted and access to a resource).
The most widely used algorithm for collaborative ﬁltering is the
k Nearest Neighbors (kNN) [3,203,32]. In the user to user version,
kNN executes the following three tasks to generate recommenda-
tions for an active user: (1) determine k users neighbors (neighbor-
hood) for the active user a; (2) implement an aggregation approach
with the ratings for the neighborhood in items not rated by a; and
(3) extract the predictions from in step 2 then select the top N
recommendations.
Hybrid ﬁltering [47,185]. Commonly uses a combination of CF
with demographic ﬁltering [224] or CF with content-based ﬁltering
[18,60] to exploit merits of each one of these techniques. Hybrid
ﬁltering is usually based on bioinspired or probabilistic methods
such as genetic algorithms [76,99], fuzzy genetic [7], neural net-
works [133,62,192], Bayesian networks [50], clustering [209] and
latent features [199].
A widely accepted taxonomy divides recommendation methods
into memory-based and model-based method categories:
Memory-based methods [3,51,123,214]. Memory-based methods
can be deﬁned as methods that (a) act only on the matrix of user
ratings for items and (b) use any rating generated before the refer-
ral process (i.e., its results are always updated). Memory-based
methods usually use similarity metrics to obtain the distance be-
tween two users, or two items, based on each of their ratios.
Model-based methods [3,212]. Use RS information to create a
model that generates the recommendations. Herein, we consider
a method model-based if new information from any user outdates
the model. Among the most widely used models we have Bayesian
classiﬁers [59], neural networks [107], fuzzy systems [234], genetic
algorithms [76,99], latent features [251] and matrix factorization
[142], among others.
To reduce the problems from high levels of sparsity in RS dat-
abases, certain studies have used dimensionality reduction tech-
niques
[202].
The
reduction
methods
are
based
on
Matrix
Factorization [124,142,143]. Matrix factorization is especially ade-
quate for processing large RS databases and providing scalable ap-
proaches [215]. The model-based technique Latent Semantic Index
(LSI) and the reduction method Singular Value Decomposition
(SVD) are typically combined [224,244,48]. SVD methods provide
good prediction results but are computationally very expensive;
they can only be deployed in static off-line settings where the
known preference information does not change with time.
RS can use clustering techniques to improve the prediction qual-
ity and reduce the cold-start problem when applied to hybrid ﬁl-
tering. It is typical to form clusters of items in hybrid RS
[209,237]. A different common approach uses clustering both for
items and users (bi-clustering) [252,85]. RS comprising social infor-
mation have been clustered to improve the following areas: tagging
[208], explicit social links [179] and explicit trust information
[181,70].
The graph in Fig. 3 shows the most signiﬁcant traditional meth-
ods, techniques and algorithms for the recommendation process as
well as their relationships and groupings. Different sections of this
paper provide more detail on the most important aspects involved
in the recommendation process.
As may be seen in Fig. 3, we can use some of the traditional ﬁl-
tering methods (content-based, demographic and collaborative)
applied to databases. Model-based technologies (genetic algo-
rithms, neural networks, etc.) make use of this kind of information.
Typical memory-based approaches are: item to item; user to user;
and hybrids of the two previous. The main purpose of both mem-
ory-based and model-based approaches is to get the most accurate
predictions in the tastes of users. The accuracy of these predictions
may be evaluated through the classical information retrieval mea-
sures, like MAE, precision, and recall. Researchers make use of
these
measures
in
order
to
improve
the
RS
methods
and
technologies.
3.2. Cold-start
The cold-start problem [203,3] occurs when it is not possible to
make reliable recommendations due to an initial lack of ratings.
We can distinguish three kinds of cold-start problems: new com-
munity, new item and new user. The last kind is the most important
in RS that are already in operation.
The new community problem [204,129] refers to the difﬁculty,
when starting up a RS, in obtaining, a sufﬁcient amount of data
(ratings) for making reliable recommendations. Two common
ways are used for tackling this problem: to encourage users to
make ratings through different means; to take CF-based recom-
mendations when there are enough users and ratings.
The new item problem [174,172] arises because the new items
entered in RS do not usually have initial ratings, and therefore, they
are not likely to be recommended. In turn, an item that is not rec-
ommended goes unnoticed by a large part of the community of
users, and as they are unaware of it they do not rate it; this way,
we can enter a vicious circle in which a set of items of the RS are
left out of the ratings/recommendations process. The new item
problem has less of an impact on RS in which the items can be dis-
covered via other means (e.g. movies) than in RS where this is not
the case (i.e. e-commerce, blogs, photos, videos, etc.). A common
solution to this problem is to have a set of motivated users who
are responsible for rating each new item in the system.
The new user problem [190,197] represents one of the great dif-
ﬁculties faced by the RS in operation. Since new users in the RS
have not yet provided any rating in the RS, they cannot receive
any personalized recommendations based on memory-based CF;
when the users enter their ﬁrsts ratings they expect the RS to offer
them personalized recommendations, but the number of ratings
introduced in the RS is usually not yet sufﬁcient to be able to make
reliable CF-based recommendations, and, therefore, new users may
feel that the RS does not offer the service they expected and they
may stop using it.
The common strategy to tackle the new user problem consists
of turning to additional information to the set of ratings in order
to be able to make recommendations based on the data available
for each user. The cold-start problem is often faced using hybrid
approaches (usually CF-content based RS, CF-demographic based
RS, CF-social based RS) [118,140]. Leung et al. [135] propose a no-
vel content-based hybrid approach that makes use of cross-level
association rules to integrate content information about domains
items. Kim et al. [118] use collaborative tagging employed as an
approach in order to grasp and ﬁlter users’ preferences for items
and they explore the advantages of the collaborative tagging for
data sparseness and cold-start users (they collected the dataset
by crawling the collaborative tagging delicious site). Weng et al.
[228] combine the implicit relations between users’ items prefer-
ences and the additional taxonomic preferences to make better
quality recommendations as well as alleviate the cold-start prob-
lem. Loh et al. [140] represent user’s proﬁles with information ex-
tracted from their scientiﬁc publications. Martinez et al. [148]
present a hybrid RS which combines a CF algorithm with a knowl-
edge-based one. Chen and He [56] propose a number of common
terms/ term frequency (NCT/TF) CF algorithm based on demo-
graphic vector. Saranya and Atsuhiro [199] propose a hybrid RS
that utilizes latent features extracted from items represented by
a multi-attributed record using a probabilistic model. Park et al.
[173] propose a new approach: they use ﬁlterbots, and surrogate
users that rate items based only on user or item attributes.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
113

3.3. The k nearest neighbors recommendation algorithm
The k Nearest Neighbors (kNN) recommendation algorithm is the
reference algorithm for the collaborative ﬁltering recommendation
process. Its primary virtues are simplicity and reasonably accurate
results; its major pitfalls are low scalability and vulnerability to
sparsity in the RS databases. This section provides a general expla-
nation of this algorithm function.
CF based on the kNN algorithm is conceptually simple, with a
straightforward implementation; it also generally produces good-
quality predictions and recommendations. However, due to the
high level of sparsity [142,29] in RS databases, similarity measures
often encounter processing problems (typically from insufﬁcient
mutual ratings for a comparison of users and items) and cold start
situations (users and
items
with low number of rankings)
[204,98,36,135].
Another major problem for the kNN algorithm is its low scalabil-
ity [142]. As the databases (such as Netﬂix) increase in size (hun-
dreds of thousands of users, tens of thousands of items, and
hundreds of millions of rankings), the process for generating a
neighborhood for an active user becomes too slow; The similarity
measure must be processed as often as new users are registered
in the database. The item to item version of the kNN algorithm sig-
niﬁcantly reduces the scalability problem [200]. To this end, neigh-
bors are calculated for each item; their top n similarity values are
stored, and for a period of time, predictions and recommendations
are generated using the stored information. Although the stored
information does not include the ratings from previous process-
ing/storage, outdated information for items is less sensitive than
for the users.
A recurrent theme in CF research is generating metrics to calcu-
late with accuracy and precision the existing similarity for the
users (or items). Traditionally, a series of statistical metrics have
been used [3,51], such as the Pearson correlation, cosine, constraint
Pearson correlation and mean squared differences. Recently, metrics
have been designed to ﬁt the constraints and peculiarities of RS
[31,35]. The relevance (signiﬁcance) concept was introduced to af-
ford more importance to more relevant users and items [34,227].
Additionally, a group of metrics was speciﬁcally designed to ade-
quately function in cold-start situations [6,36].
The kNN algorithm is based on similarity measures. Next sub-
section provides further details on the current RS similarity mea-
sures. The similarity approaches typically compute the similarity
between two users x and y (user to user) based on both users’ item
ratings. The item to item kNN version computes the similarity be-
tween two items i and j.
A formal approach of the kNN algorithm may be found in [32].
In this section, we will provide an illustrative example of this algo-
rithm. The method for making recommendations is based on the
following three steps:
(a) Using the selected similarity measure, we produce the set of
k neighbors for the active user a. The k neighbors for a are
the nearest k (similar) users to u.
(b) Once the set of k users (neighbors) similar to active a has
been calculated, in order to obtain the prediction of item i
on user a, one of the following aggregation approaches is
often used: the average, the weighted sum and the adjusted
weighted aggregation (deviation-from-mean).
(c) To obtain the top-n recommendations, we choose the n
items, which provide most satisfaction to the active user
according to our predictions.
Fig. 4 shows a case study using the user to user kNN algorithm
mechanism.
In the item to item version [200,77] of the kNN algorithm, the
following three tasks are executed: (1) determine q items neigh-
bors for each item in the database; (2) for each item i not ranked
by the active user a, calculate its prediction based on the ratings
of a from the q neighbors of i; and (3) select the top n recommen-
dations for the active user (typically the n major predictions from
a). Step (1) can be executed periodically, which facilitates an accel-
erated recommendation with regard to the user to user version.
The item to item and user to user versions of the kNN algorithm
can be combined [188] to take advantage of the positive aspects
from each approach. These approaches are typically fused by pro-
cessing the similarity between objects.
3.4. Similarity measures
A metric or a Similarity Measure (SM) determines the similarity
between pairs of users (user to user CF) or the similarity between
pairs of items (item to item CF). For this purpose, we compare the
ratings of all the items rated by two users (user to user) or the rat-
ings of all users who have rated two items (item to item).
The kNN algorithm is based essentially on the use of traditional
similarity metrics of statistical origin. These metrics require, as the
only source of information, the set of votes made by the users on
the items (memory-based CF). Among the most commonly used
traditional metrics we have: Pearson correlation (CORR), cosine
(COS), adjusted cosine (ACOS), constrained correlation (CCORR),
Mean Squared Differences (MSD) and Euclidean (EUC) [51,3].
We will describe and compare a representative group of SM
used in the kNN algorithm. The SM discussed include the following
variations: (a) cold-start and general cases, (b) based or not based
on models, and (c) using trust information or only ratings. Table 2
shows a classiﬁcation of the memory-based CF SM which will be
tested in this section.
A new metric (JMSD) has recently been published, which be-
sides using the numerical information from the ratings (via mean
squared differences) also uses the non-numerical information pro-
vided by the arrangement of these (via Jaccard) [31]. Ortega et al.
[169] use Pareto dominance to perform a pre-ﬁltering process
eliminating less representative users from the k-neighbur selection
process while retaining the most promising ones.
A specialization of the memory-based CF SM, which appeared
recently [35], uses the information contained in the votes of all
users, instead of restricting it to the ratings of the two users com-
pared (user to user) or the two items compared (item to item). We
will call this SM SING (singularities).
The possibility exists to create a model (model-based CF) from
the full set of users’ ratings in order to later determine the similar-
ity between pairs of users or pairs of items based on the model cre-
ated. The potential advantages of this focus are an increase in the
accuracy obtained, in the performance (time consuming) achieved
or in both. The drawback is that the model must be regularly up-
dated in order to consider the most recently entered set of ratings.
Fig. 4. User to user kNN algorithm example, k = 3. Similarity measure: 1 – (mean
squared differences). Aggregation approach: average.
114
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

Bobadilla et al. [33] provides a metric based on a model generated
using genetic algorithms. We will call this SM GEN (genetic-based).
As a result of the increase in web 2.0 websites on the Internet, a
set of metrics has appeared which use the new social information
available (friends, followers, followeds, etc.). Most of these SM are
grouped in papers related to trust, reputation and credibility
[71,239,138], although this situation is also produced in other
ﬁelds [30]. These metrics could not be considered strictly mem-
ory-based CF, as they use additional information which not all RS
have. In this sense, each SM proposed is tailored to a speciﬁc RS
or at most to a very small set of RS which share the same structure
in their social information.
There are SM [112,127] which aim to extract information re-
lated to trust and reputation by only using the users’ set of ratings
(memory-based CF). The advantage is that their use can be general-
ized to all CF RS; the drawback is that the social information ex-
tracted is really poor. We will call TRUST the SM proponed in
Jeong et al. [112]. Currently, two new interesting SM get more cov-
erage [38] and accuracy [61].
Fig. 5 shows the results from several evaluation measures gen-
erated by applying the SM discussed in this section. The results
show that the RS-tailored SM are superior compared with the tra-
ditional SM from statistics. Processing for the memory-based infor-
mation and results from Fig. 5 follow the framework schematic
published previously [32].
There are so far research papers dealing with the cold-start prob-
lem through the users’ ratings information. Ahn [6] presents a heu-
ristic SM named PIP, that outperforms the traditional statistical SM
Table 2
Tested collaborative ﬁltering similarity measures.
Not based on models
Model-based
No trust extraction
Trust extraction
Traditional (only the ratings of both users or both items)
Not tailored to cold-start users
JMSD, CORR, CCORR, COS, ACOS, MSD, EUC
GEN
Tailored to cold-start users
PIP
UERROR
NCS
Extended to all the ratings
SING
TRUST
Fig. 5. Evaluation measures results obtained from current similarities measures; MovieLens database. (A) Prediction results, (B) recommendation results, (C) novelty results,
and (D) trust results.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
115

(Pearson correlation, cosine, etc.). Heung-Nam et al. [98] proposes a
method (UERROR) that predicts ﬁrst actual ratings and subsequently
identiﬁes prediction errors for each user. Taking into account this er-
ror information, some speciﬁc ‘‘error-reﬂected’’ models, are de-
signed. Bobadilla et al. [36] presents a metric based on neural
learning (model-based CF) and adapted for new user cold-start situ-
ations, called NCS.
Fig. 6 shows results from several evaluation measures gener-
ated by applying the cold-start SM presented in this section; These
results show that the RS-tailored SM are superior compared with
the traditional SM from statistics. Since the database Movielens
does not take into account cold-start users, we have removed rat-
ings of this database in order to achieve cold-start users. Indeed,
we have removed randomly between 5 and 20 ratings of those
users who have rated between 20 and 30 items. In this way, we
will regard those users who now result to rate between 2 and 20
items as cold-start users.
4. Evaluation of recommender systems results
Since RS research began, evaluation of predictions and recom-
mendations has become important [94,201]. Research in the RS
ﬁeld requires quality measures and evaluation metrics [90] to know
the quality of the techniques, methods, and algorithms for predic-
tions and recommendations. Evaluation metrics [94,95] and evalua-
tion frameworks [92,32] facilitate comparisons of several solutions
for the same problem and selection from different promising lines
of research that generate better results.
Because of evaluation measures, RS recommendations have
gradually been tested and improved [48]. A representative set of
existing evaluation measures has standard formulations, and a
group of open RS public databases has been generated. These two
advances have facilitated quality comparisons for new proposed
recommendation methods and previously published methods; thus,
RS methods and algorithms research has progressed continuously.
The most commonly used quality measures are the following
[90,95]: (1) prediction evaluations, (2) evaluations for recommen-
dation as sets, and (3) evaluations for recommendations as ranked
lists. Fig. 5 shows results from applying several evaluation mea-
sures to a set of representative similarity measures.
Evaluation metrics [12] can be classiﬁed as [94,95] (a) predic-
tion metrics: such as the accuracy ones: Mean Absolute Error
(MAE), Root of Mean Square Error (RMSE), Normalized Mean Average
Error (NMAE); and the coverage (b) set recommendation metrics:
such as Precision, Recall and Receiver Operating Characteristic
(ROC) [204] (c) rank recommendation metrics: such as the half-life
[43] and the discounted cumulative gain [17] and (d) diversity met-
rics: such as the diversity and the novelty of the recommended
items [105]. The validation process is performed by employing
the most common cross validation techniques (random sub-sam-
pling and k-fold cross validation) [21]; for cold-start situations,
due to the limited number of users (or items) votes involved, the
usual method chosen to carry out the experiments is leave-one-
out cross validation [36].
Hernández and Gaudioso [95] propose an evaluation process
based on the distinction between interactive and non-interactive
Fig. 6. Evaluation results obtained from current cold-start similarities measures. (A) Prediction results, (B) recommendation results, (C) novelty results, and (D) trust results.
116
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

subsystems. General publications and reviews also exist which in-
clude the most commonly accepted evaluation measures: mean
absolute error, coverage, precision, recall and derivatives of these:
mean squared error, normalized mean absolute error, ROC and fallout;
Goldberg et al. [87] focuses on the aspects not related to the eval-
uation, Breese et al. [43] compare the predictive accuracy of vari-
ous methods in a set of representative problem domains.
The majority of articles discuss attempted improvements to the
accuracy of RS results (RMSE, MAE, etc.). It is also common to at-
tempt an improvement in recommendations (precision, recall,
ROC, etc.). However, additional objectives should be considered
for generating greater user satisfaction [253], such as topic diversi-
ﬁcation and coverage serendipity.
Currently, the ﬁeld has a growing interest in generating algo-
rithms with diverse and innovative recommendations, even at
the expense of accuracy and precision. To evaluate these aspects,
various metrics have been proposed to measure recommendation
novelty and diversity [105,220].
The frameworks aid in deﬁning and standardizing the methods
and algorithms employed by RS as well as the mechanisms to eval-
uate the quality of the results. Among the most signiﬁcant papers
that propose CF frameworks are Herlocker et al. [92] which
evaluates the following: similarity weight, signiﬁcance weighting,
variance weighting, selecting neighborhood and rating normaliza-
tion; Hernández and Gaudioso [95] proposes a framework in which
any RS is formed by two different subsystems, one of them to
guide the user and the other to provide useful/interesting items.
Koutrika et al. [125] is a framework which introduces levels of
abstraction in CF process, making the modiﬁcations in the RS more
ﬂexible. Antunes et al. [12] presents an evaluation framework
assuming that evaluation is an evolving process during the system
lifecicle.
The majority of RS evaluation frameworks proposed until now
present two deﬁciencies: the ﬁrst of these is the lack of formal-
ization. Although the evaluation metrics are well deﬁned, there
are a variety of details in the implementation of the methods
which, in the event they are not speciﬁed, can lead to the
generation
of
different
results
in
similar
experiments.
The
second deﬁciency is the absence of standardization of the evalu-
ation measures in aspects such as novelty and trust of the
recommendations.
Bobadilla et al. [32] provides a complete series of mathematical
formalizations based on sets theory. Authors provide a set of eval-
uation measures, which include the quality analysis of the follow-
ing aspects: predictions, recommendations, novelty and trust.
Presented next is a representative selection of the RS evaluation
quality measures most often used in the bibliography.
4.1. Quality of the predictions: mean absolute error, accuracy and
coverage
In order to measure the accuracy of the results of an RS, it is
usual to use the calculation of some of the most common predic-
tion error metrics, amongst which the Mean Absolute Error
(MAE) and its related metrics: mean squared error, root mean
squared error, and normalized mean absolute error stand out.
We deﬁne U as the set of the RS users, I as the set of the RS
items, ru,i the rating of user u on item i,  the lack of rating (ru,i = 
means user u has not rated item i), pu,i the prediction of item i on
user u.
Let Ou = {i 2 Ijpu,i –  ^ ru,i – }, set of items rated by user u hav-
ing prediction values. We deﬁne the MAE and RMSE of the system
as the average of the user’s MAE. We remark that the absolute dif-
ference between prediction and real value, jpu,i  ru,ij, informs
about the error in the prediction.
MAE ¼ 1
#U
X
u2U
1
#Ou
X
i2Ou
jpu;i  ru;ij
 
!
ð1Þ
RMSE ¼ 1
#U
X
u2U
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
#Ou
X
i2Ou
ðpu;i  ru;iÞ2
s
ð2Þ
The coverage could be deﬁned as the capacity of predicting from
a metric applied to a speciﬁc RS. In short, it calculates the percent-
age of situations in which at least one k-neighbor of each active
user can rate an item that has not been rated yet by that active
user. We deﬁned Ku,i as the set of neighbors of u which have rated
the item i. We deﬁne the coverage of the system as the average of
the user’s coverage:
Let
Cu ¼ fi 2 Ijru;i ¼  ^ Ku;i – £g;
Du ¼ fi 2 Ijru;i ¼ g
coverage ¼ 1
#U
X
u2U
100  #Cu
#Du


ð3Þ
4.2. Quality of the set of recommendations: precision, recall and F1
The conﬁdence of users for a certain RS does not depend directly
on the accuracy for the set of possible predictions. A user gains
conﬁdence on the RS when this user agrees with a reduced set of
recommendations made by the RS.
In this section, we deﬁne the following three most widely used
recommendation quality measures: (1) precision, which indicates
the proportion of relevant recommended items from the total
number of recommended items, (2) recall, which indicates the pro-
portion of relevant recommended items from the number of rele-
vant items, and (3) F1, which is a combination of precision and
recall.
Let Xu as the set of recommendations to user u, and Zu as the set
of n recommendations to user u. We will represent the evaluation
precision, recall and F1 measures for recommendations obtained
by making n test recommendations to the user u, taking a h rele-
vancy
threshold.
Assuming
that
all
users
accept
n
test
recommendations:
precision ¼ 1
#U
X
u2U
#fi 2 Zujru;i P hg
n
ð4Þ
recall ¼ 1
#U
X
u2U
#fi 2 Zujru;i P hg
#fi 2 Zujru;i P hg þ # i 2 Zc
u
ru;i P h


ð5Þ
F1 ¼ 2  precision  recall
precision þ recall
ð6Þ
4.3. Quality of the list of recommendations: rank measures
When the number n of recommended items is not small, users
give greater importance to the ﬁrst items on the list of recommen-
dations. The mistakes incurred in these items are more serious er-
rors than those in the last items on the list. The ranking measures
consider this situation. Among the ranking measures most often
used are the following standard information retrieval measures:
(a) half-life (7) [43], which assumes an exponential decrease in
the interest of users as they move away from the recommenda-
tions at the top and (b) discounted cumulative gain (8) [17], wherein
decay is logarithmic.
HL ¼ 1
#U
X
u2U
X
N
i¼1
maxðru;pi  d; 0Þ
2ði1Þ=ða1Þ
ð7Þ
DCGk ¼ 1
#U
X
u2U
ru;p1 þ
X
k
i¼2
ru;pi
log2ðiÞ
 
!
ð8Þ
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
117

p1,. . .,pn represents the recommendation list, ru,pi represents
the true rating of the user u for the item pi, k is the rank of the eval-
uated item, d is the default rating, a is the number of the item on
the list such that there is a 50% chance the user will review that
item.
4.4. Novelty and diversity
The novelty evaluation measure indicates the degree of differ-
ence between the items recommended to and known by the user.
The diversity quality measure indicates the degree of differentia-
tion among recommended items.
Currently, novelty and diversity measures do not have a stan-
dard;
therefore,
different
authors
propose
different
metrics
[163,220]. Certain authors have [105] used the following:
diversityZu ¼
1
#Zuð#Zu  1Þ
X
i2Zu
X
j2Zu;j–i
½1  simði; jÞ
ð9Þ
noveltyi ¼
1
#Zu  1
X
j2Zu
½1  simði; jÞ;
i 2 Zu
ð10Þ
Here, sim(i, j) indicates item to item memory-based CF similar-
ity measures. Zu indicates the set of n recommendations to user u.
4.5. Stability
The stability in the predictions and recommendations inﬂu-
ences on the users’ trust towards the RS. A RS is stable if the pre-
dicitions it provides do not change strongly over a short period
of time. Adomavicius and Zhang [4] propose a quality measure of
stability, MAS (Mean Absolute Shift). This measure is deﬁned
through a set of known ratings R1 and a set of predictions of all un-
known ratings, P1. For an interval of time, users of the RS will have
rated a subset S of these unknown ratings and the RS can now
make new predictions, P2. MAS is deﬁned as follows:
stability ¼ MAS ¼ 1
jP2j
X
ðu;iÞ2P2
jP2ðu; iÞ  P1ðu; iÞj
ð11Þ
4.6. Reliability
The reliability of a prediction or a recommendation informs
about how seriously we may consider this prediction. When RS
recommends an item to a user with prediction 4.5 in a scale
{1,. . .,5}, this user hopes to be satisﬁed by this item. However, this
value of prediction (4.5 over 5) does not reﬂect with which certain
degree the RS has concluded that the user will like this item (with
value 4.5 over 5). Indeed, this prediction of 4.5 is much more reli-
able if it has obtained by means of 200 similar users than if it has
obtained by only two similar users.
In Hernando et al. [96], a realibility measure is proposed accord-
ing the usual notion that the more reliable a prediction, the less lia-
ble to be wrong. Although this reliability measure is not a quality
measure used for comparing different techniques of RS through
cross validation, this can be regarded as a quality measure associ-
ated to a prediction and a recommendation. In this way, the RS pro-
vides a pair of values (prediction value, reliability value), through
which users may balance its preference: for example users would
probably prefer the option (4,0.9) to the option (4.5,0.1). Conse-
quently, the reliability measure proposed in Hernando et al. [96]
provides a new understandable factor, which users may consider
for taking its decisions. Nevertheless, the use of this reliability
measure is just constrained to those RS based on the kNN
algorithm.
The
deﬁnition
of
reliability
on
the
prediction,
pu,i,
is
based on two numeric factors: su,i and vu,i. su,i measures the similar-
ity of the neighbors used for making the prediction pu,i; vu,i
measures the degree of disagreement between these neighbors
rating the item i. Finally, the reliablity measure is deﬁned as
follows:
fSðsu;iÞ ¼ 1 
s
s þ su;i
;
su;i ¼
X
v2Ku;i
simðu;vÞ
ð12Þ
where
fSðsu;iÞ ¼ 1 
s
s þ su;i
;
su;i ¼
X
v2Ku;i
simðu;vÞ
ð13Þ
Fig. 7. Recommender systems evaluation process.
118
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

fvðvu;iÞ ¼
max  min vu;i
max  min


ln 0:5
lnmax  min v
max  min ;
vu;i
¼
P
v2Ku;isimðu;vÞðrv;i  rv  pu;i þ ruÞ2
P
v2Ku;isimðu;vÞ
ð14Þ
where s and v are respectively the median of the values of su,i and
vu,i in the speciﬁc RS. Ku,i is the set of neighbors of u which have
rated the item i. {min,. . .,max} is the discrete range of rating values.
Fig. 7 shows the general mechanism for cross validation used to
generate quality results form the evaluation measures. The data-
base is divided in training and test areas for both users and items.
In the ﬁrst phase (top on the left side), k-neighbors are calculated
for the active user (while the active user is selected from the set of
test users, the k-neighbors are selected from the set of training
users). In the aggregation phase (top on the right side), predictions
are calculated for the active user (from the set of test items). Final-
ly, evaluation metrics are used to compare the predictions and rec-
ommendations obtained with the real ratings of the user; the more
accurate the predictions and recommendations, better quality of
the proposed recommendation algorithm.
5. Social information
As the web 2.0 has developed, RS have increasingly incorpo-
rated social information (e.g., trusted and untrusted users, fol-
lowed and followers, friends lists, posts, blogs, and tags). This
new contextual information [145,216] improves the RS. Social
information improves the sparsity problem inherent in memory-
based RS because social information reinforces traditional mem-
ory-based information (users ratings): users connected by a net-
work of trust exhibit signiﬁcantly higher similarity on items and
meta-data that non-connected users [132].
Social information is used by researchers with three primary
objectives: (a) to improve the quality of predictions and recom-
mendations [53,13], (b) propose or generate new RS [139,210],
and (c) elucidate the most signiﬁcant relationships between social
information and collaborative processes [100,178].
Trust and reputation is an important area of research in RS [166];
this area is closely related to the social information currently in-
cluded in RS [114]. The most common approachs to generating
trust and reputation measurements are the following: (a) user
trust: to calculate the credibility of users through explicit informa-
tion of the rest of users [239,138] or to calculate the credibility of
users through implicit information obtained in a social network
[59,150] and (b) item trust: to calculate the reputation of items
through a feedback of users [114] or to calculate the reputation
of items studying how users work with these items [58,122].
In the social RS ﬁeld, users can introduce labels associated with
items. The set of triples huser, item, tagi form information spaces
referred to as folksonomies. Fundamentally, folksonomies are used
in the following two ways: (1) to create tag recommendation sys-
tems (RS based only on tags) [147] and (2) to enrich the recom-
mendation processes using tags [81].
Content-based ﬁltering has recently become more important
due to the surge in social networks. RS show a clear trend to allow
users to introduce content [13,178], such as comments, critiques,
ratings, opinions and labels as well as to establish social relation-
ship links (e.g., followed, followers, like user and dislike user). This
additional information increases the accuracy of predictions and
recommendations, which has generated a variety of research arti-
cles: Kim et al. [117], Zheng and Li [248] and Carrer-Neto et al. [53].
The rest of this section deal is dealt with the concepts and re-
search in the two lines considered previously: Filtering of social
information and content ﬁltering.
5.1. Social Filtering
Social information can be gathered explicitly or implicitly
through identiﬁcation of a community network or afﬁnity network
[196] using the individual information that users generate (e.g.,
communications and web logs) [178]. Even using only the ratings
from the users, it is possible to improve the RS results creating
an implicit social networking [180]. Both implicit and explicit
information sources can be combined to generate recommenda-
tions [144].
The explicit social information can be used via a trust-based CF
in order to improve the quality of recommendations. Trust infor-
mation can be generated or used through different approaches,
such as trust propagation mechanisms [42], a ‘follow the leader’
approach [8,186], personality-based similarity measures [101],
trust networks [239,221], distrust analysis [223,20], and dynamic
trust based on the ant colonies metaphor [20].
Most of the research work that uses social information applied
to RS aims to obtain improvements in the recommendations made
by referring to the extra information provided by the social infor-
mation used. Among the most relevant current work which uses
this approach we have: Woerndl and Groh [231] use social net-
works to enhance collaborative ﬁltering; Their evaluation shows
that the social recommender outperforms traditional collaborative
ﬁltering algorithms in the used scenario. Arazy et al. [13] improve
accuracy by using data from online social networks and electronic
communication tools. Xin et al. [233] propose an approach for
improving RS through exploiting the learners note taking activity.
They maintain that notes’ features can be exploited by collabora-
tive learning systems in order to enrich and extend the user proﬁle
and improve personalized learning. The Bonhard and Sasse [41] re-
search has shown that the relationship between advice-seeker and
recommender is extremely important, so ways of indicating social
closeness and taste overlap are required. They thus suggest that
drawing on similarity and familiarity between the user and the
persons who have rated the items can aid judgment and decision
making. Fengkun and Hong [75] developed a way to increase rec-
ommendation effectiveness by incorporating social network infor-
mation into CF. They collected data about users’ preference ratings
and their social network relationships from a social networking
web site; then, they evaluated CF performance with diverse neigh-
bor groups combining groups of friends and nearest neighbors.
Carmagnola et al. [52] state that joining in a network with other
people exposes individuals to social dynamics which can inﬂuence
their attitudes, behaviors and preferences: They present SoNARS,
an algorithm for recommending content in social RS. SoNARS tar-
gets users as members of social networks, suggesting items that re-
ﬂect the trend of the network itself, based on its structure and on
the inﬂuence relationships among users. In Ramaswamy et al.
[189] the design of the social network based RS incorporates three
features that complement each other to derive highly targeted ads.
First, they analyze information such as customer’s address books to
estimate the level of social afﬁnity among various users. This social
afﬁnity information is used to identify the recommendations to be
sent to an individual user.
Another group of research work uses social information to cre-
ate or enable RS. That is, the aim is not to improve the results of a
particular RS in operation, the aim is to propose or make possible
RS which still do not exist, or if they do exist they are not based
on social information: The Siersdorfer and Sergei [210] objective
is to construct social recommender systems that predict the utility
of items, users, or groups based on the multi-dimensional social
environment of a given user; they do a mining of the rich set of
structures and social relationships that provides the folksonomies.
In the Li and Chen [137] study they propose a blog recommenda-
tion mechanism that combines trust model, social relation and
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
119

semantic analysis and illustrates how it can be applied to a presti-
gious online blogging system. In the Jason [111] research project,
they have applied a system to discover the social networks be-
tween mobile users by collecting a dataset from about two millions
of users. They argue that social network is applicable to generate
context-based recommendation services. Jyun and Chui [115] pa-
per uses trading relationships to calculate level of recommendation
for trusted online auction sellers. They demonstrate that network
structures formed by transactional histories can be used to expose
such
underlying
opportunistic
collusive
seller
behaviors.
In
Dell’amico and Capra [69] users’ trustworthiness has been mea-
sured according to one of the following two criteria: taste similar-
ity (i.e., ‘‘I trust those who agree with me’’), or social ties (i.e., ‘‘I
trust my friends, and the people that my friends trust’’). They argue
that, in order to be trusted, users must be both well intentioned
and competent. Based on this observation, they propose a novel
approach that they call social ﬁltering.
A third group of work provides the foundation of the research to
discover the most signiﬁcant relationships between social informa-
tion and collaborative processes, without creating, proposing or
improving any particular RS. This research moves at a higher level
of abstraction, with the aim of establishing bases and general prin-
ciples. Bonhard [40] paper explains that qualitative research con-
ducted
to
date
has
shown
that
the
relationship
between
recommender and recommendee has a signiﬁcant impact on deci-
sion-making. Hossain and Fazio [100] present a study exploring the
connection between social networks and collaborative process.
They focus on exploring academics’ network position and its effect
on their collaborative networks. By deﬁning network position in
this way, they develop a social network that uses the academics
as nodes within the network instead of each published paper.
The Esslimani et al. [72] paper presents a new CF approach based
on a behavioral network that uses navigational patterns to model
relationships between users and exploits social networks tech-
niques. Golbeck and Kuter [86] present an experimental study of
several types of trust inference algorithms to answer the following
questions on trust and change: How far does a single change prop-
agate through the network? How large is the impact of that
change? How does this relate to the type of inference algorithm?
The experimental results provide insights into which algorithms
are most suitable for certain applications.
Research in the ﬁeld of trust and reputation could provide a
suitable starting point to create social interaction among users of
the RS, however, the most relevant work on the subject is limited
to the use of trust relationships to improve the quality of the rec-
ommendation services. O’donovan [165] book chapter examines
the diversity of sources from which trust information can be har-
nessed within social web applications and discusses a high level
classiﬁcation of those sources. It is shown that harnessing an in-
creased amount of information upon which to make trust decisions
greatly enhances the user experience with the social web applica-
tion. Massa and Avesani [151] explain that RS making use of trust
information are the most effective in term of accuracy while pre-
serving a good coverage. This is especially evident on users who
provided few ratings. Yuan et al. [239] choose the trust aware RS
as an example to demonstrate the advantages by making use of
the veriﬁed small-world nature of the trust network. Li and Kao
[138] present a RS based on the trust of social networks; Through
the trust computing, the quality and the veracity of peer produc-
tion services can be appropriately assessed. The experimental re-
sults show that the proposed RS can signiﬁcantly enhance the
quality of peer production services.
Table 3 classiﬁes the current approaches to address user credi-
bility and item reputation in social-based RS.
In the CF ﬁeld, the trust of users is used to make predictions,
weighting trust values. That is to say, the more trust a user has,
the
more
important
its
ratings
are
for
making
predictions
[58,112,239]. In Ma et al. [145], they propose a probabilistic factor
analysis framework, combining ratings and trusted friends; this
framework can be applied to pure user-item rating matrix.
5.2. Content-based ﬁltering
Content-based ﬁltering (CBF) tries to recommend items to the
active user similar to those rated positively in the past. It is based
on the concept that items with similar attributes will be rated sim-
ilarly [16,177,203]. For example, if a user likes a web page with the
words ‘‘car’’, ‘‘engine’’ and ‘‘gasoline’’, the CBF will recommend
pages related to the automotive world.
CBF is becoming especially important as RS incorporate infor-
mation on items from users working in web 2.0 environments,
such as tags, posts, opinions and multimedia material.
Two challenging problems for content-based ﬁltering are lim-
ited content analysis and overspecialization [3]. The ﬁrst problem
arises from the difﬁculty in extracting reliable automated informa-
tion from various content (e.g., images, video, audio and text),
which can greatly reduce the quality of recommendations. The sec-
ond problem (overspecialization) refers to the phenomenon in
which users only receive recommendations for items that are very
similar to items they liked or preferred; therefore, the users are not
receiving recommendations for items that they might like but are
unknown (e.g., when a user only receives recommendations about
ﬁction ﬁlms). Recommendations can be evaluated for novelty
[32,105].
For CBF to operate, attributes of the items you wish to recom-
mend must be extracted [176]. Typically, a set of attributes is man-
ually deﬁned for each item depending on its domain. In certain
instances, such as when it is desired to recommend textual infor-
mation, classic information retrieval techniques must be used to
automatically deﬁne such attributes (e.g., term frequency, inverse
document frequency andnormalization to page length).
Fig. 8 shows the CBF mechanism, which includes the following
steps: (1) extract the attributes of items for recommendation, (2)
compare the attributes of items with the preferences of the active
Table 3
State of the art on trust and reputation.
User trust
Item trust
Explicit
trust
systems
The ‘credibility’ of users is calculated through explicit information of the rest of
users. [71,239,240]. Services P2P usually implement this technique [138]
The ‘reputation’ of items is calculated by means of a feedback of users
who are asked about their opinions [114]. E-commerce services often
use this technique
Implicit
trust
systems
The ‘credibility’ of users is calculated through implicit information obtained in a
social network [59,150,200]
The ‘reputation’ of items is calculated studying how users work with
these items (for example, the number of times a song is played)
[58,122]
Memory
based
trust
The ‘credibility’ measure is calculated taking into account the users’ ratings [112,127,145]
120
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

user, and (3) recommend items with characteristics that ﬁt the
user’s interests.
When the attributes of the items and the user proﬁles are
known, the key purpose for CBF [158] is to determine whether a
user will like a speciﬁc item. This task is resolved traditionally by
using heuristic methods [198,15,79] or classiﬁcation algorithms,
such
us:
rule
induction
[65,119],
nearest
neighbors
methods
[236,27], Rocchio’s algorithm [131,16], linear classiﬁers [113], and
probabilistic methods [175,160,84].
The pure CBF has several shortcomings [16,176,212]:
(a) In certain domains (e.g., music, blogs, and videos), it is a
complicated task to generate the attributes for items.
(b) CBF suffers from an overspecialization problem because by
nature it tends to recommend the same types of items.
(c) It is more difﬁcult to acquire feedback from users because
with CBF, users do not typically rate the items (as in CF),
and, therefore, it is not possible to determine whether the
recommendation is correct.
Because of these shortcomings, it is rare to ﬁnd a pure CBF
implementation. It is more common to use the hybrid CBF/CF
Burke 2002. CF solves CBF’s problems because it can function in
any domain; it is less affected by overspecialization; and it ac-
quires feedback from users. CBF adds the following qualities to
CF: improvement to the quality of the predictions, because they
are calculated with more information, and reduced impact from
the cold-start and sparsity problems.
CBF and CF can be combined in different ways [3]. Fig. 9 shows
the different alternatives.
Fig. 9a shows the methods that calculate CBF and CF recom-
mendations separately and subsequently combine them. Claypool
et al. [64] propose to use a weighted average for combining CBF
and CF predictions depending on the type of prediction. In another
study, Pazzani [177] proposes combining the CBF and CF recom-
mendation lists by assigning the items scores according to their
position on the lists. Additionally, Billsus and Pazzani [26] and Tran
and Cohen [218] propose to select the CBF or CF prediction in
accordance with the quality.
Fig. 9b depicts the methods that incorporate CBF characteristics
into the CF approach. Balabanovic and Shoham [16] maintain user
proﬁles based on content analysis and directly compare the pro-
ﬁles to determine similar users for CF recommendations. Good
et al. [89] construct specialized ﬁlterbots using CBF techniques,
which later act as neighbors in the CF stage. Melville et al. [157]
propose to add predictions from the CBF into the ratting matrix
employed by the CF. Li [136] modiﬁes the ratting matrix, which
is input for the CF, by combining it with another matrix generated
from clustering the items according to their attributes. In Hu and
Pu [101], authors incorporate personality characteristics in the CF
similarity measure to minimize the new-user problem.
Fig. 9c illustrates the methods to construct a uniﬁed model
with both CBF and CF characteristics. Basu et al. [19] propose
using CBF and CF characteristics in a single rule-based classiﬁer.
Popescul et al. [182] and Schein et al. [204] propose using prob-
ability models to combine CBF and CF recommendations. In an-
other studies [66,10,50], the authors employ Bayesian networks
to combine CBF and CF characteristics and generate more accu-
rate recommendations. Burke [45] and Middleton et al. [159]
propose using knowledge-based techniques to solve the cold-
start problem.
Fig. 9d shows the methods that incorporate CF characteristics
into a CBF approach. In Soboroff and Nicholas [211], the authors
use LSI to create the user proﬁles used in CBF recommendations
beginning with the CF ratting matrix. Mooney and Roy [160] use
CF system predictions as input for CBF.
The current trend in CBF is to add social information to the
items attributes, such as tags, comments, opinion, and social net-
work sharing. Social tagging systems are the most popular because
they allow users to annotate online resources with arbitrary labels,
which produces rich information spaces (folksonomies). These new
components have opened novel lines of RS research that can be di-
vided into two categories: (1) tag recommendation systems and (2)
use of tags in the recommendation process:
(1) RS tags attempt to provide personalized item recommenda-
tions to users through the most representative tags. In
Jächke et al. [110], the authors compare different mecha-
nisms for tags recommendations. Marinho and Schmidt-Thi-
eme [147] improve tags recommendations by applying
classic recommendation methods. Additionally, Landia and
Anand [130] propose a method that combines clustering-
based CBF with CF to suggest new tags to users.
(2) The methods using tags in the recommendation process
increase the capacity of traditional RS. Tso-Sutter et al.
[219] propose a generic method that allows tags to be incor-
porated to standard CF algorithms. Bogers and Van Den Bosh
[39] examine how to incorporate the tags and other metada-
ta into a hybrid CBF/CF algorithm by replacing the tradi-
tional user-based and item-based similarity measures by
tag overlap. Gemmell et al. [83] propose a weighted hybrid
recommender, wherein they combine the graph-based tag
recommendations with user-based CF and item-based CF.
Gedikli and Jannach [81] propose to use tags as a means to
express which features of an item users particularly like or
dislike. In Gemmell et al. [82], the authors offer a hybrid
RS, wherein they predict the user preferences for items by
only consulting the user’s tagging history.
6. Additional recommender systems objectives
Commercial RS compete in the market by offering the best con-
tent and quality in recommendations as well as greatest variety of
services. Recommendations to user groups [108] facilitate joint
recommendations to user groups (e.g., a group of four friends
who wish to choose a movie). For CF, four design approaches offer
an opportunity for action: (1) acting into the similarity measures
stage [168], (2) acquiring neighbors [37], (3) acquiring predictions
[63], and (4) generating recommendations [17]. Research results
[168] indicate that the quality of the recommendations does not
vary greatly between the different approaches, but the execution
time is dramatically reduced as we advance when it is used (when
the design of a similarity measure for groups is the most efﬁcient
solution).
For the RS generated recommendations to be valuable for users,
they must be explained well in a simple, compelling and accurate
manner. The recommendation explanation ﬁeld has been investi-
gated with new developments in RS [91] until now [170]. Tradi-
tionally, the explanation type is divided into the following
categories: (a) human style (user to user approach), (b) item style
(item to item approach), (c) feature style(items features), and (d)
hybrid. It also employs the use of conversational techniques [155]
and incorporates geo-social information [235].
6.1. Recommending to groups of users
RS that consider groups of users [108] are starting to expand
and to be used in different areas: tourism [14], music [55], TV
[238], web [176].
Given the speciﬁc characteristics of the recommendation to
groups, it is appropriate to establish a consensus for different
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
121

group semantics that formalize the agreements and disagreements
among users [195].
With the aim of presenting the work carried out to date in a
structured way, we provide a classiﬁcation of the recommendation
to groups in CF RS. Fig. 10 graphically illustrates the four basic lev-
els on which we can act in order to unify the group’s users’ data
with the objective of obtaining the data of the group of users: sim-
ilarity metric, establishing the neighborhood, prediction phase,
determination of recommended items.
In Fig. 10, the individual members of a group are represented on
the left, in grey; each graticule represents the matrix of ratings by
the users (horizontal) on the items (vertical). The graph shows the
four representative cases of tackling the solution to recommenda-
tion by groups (one case for each matrix on the left of the ﬁgure).
The circles show key information: they indicate the CF process
phase where the uniﬁcation is performed: ‘‘n users ? 1 group’’.
In the ﬁrst case, at the top of the graph, the data uniﬁcation is
performed in the prediction phase of the CF process: n individual
predictions of n users of the group are combined in one prediction
of the group (predictions aggregation). This approach has been
used by Berkovsky and Freyne [22], García et al. [78] and Christen-
sen and Schiafﬁno [63].
The second case acts on the sets of neighbors of the group’s
users, by unifying them in one neighborhood for the whole group.
This approach has been studied by Bobadilla et al. [37], proposing
the intersection of a large number (k) of neighbors of each user of
the group.
In the third case, the recommendations obtained for each indi-
vidual user of the group are merged into one recommendation for
the group. Baltrunas et al. [17] use rank aggregation of individual
lists of recommendations.
The fourth case [168] uses a similarity metric that acts directly
on the set of ratings of the group of users. This solution is the only
one that directly provides a set of neighbors for the group of users.
A study exists [9] which, prior to any of the previous cases, pro-
poses, as a front-end, the incorporation of a process of estimation
of missing information when dealing with incomplete fuzzy lin-
guistic preference relations.
6.2. Explaining recommendations
An important research subject in the RS ﬁeld focuses on provid-
ing explanations that justify the recommendations the user has re-
ceived. This is an important aspect of an RS because it aids in
maintaining a higher degree of user conﬁdence in the results gen-
erated by the system.
The type of explanations used thus far can be classiﬁed as fol-
lows [170].
Human style explanations (user to user approach). For example,
we recommend movie i because it was liked by the users who
rated movies j, k, m, . . . very positively (j, k, m, . . . are movies
rated well by the active user).
Item style explanations (item to item approach). For example, we
recommend the vacation destination i because you liked the
vacation destinations g, c, r, . . . (g, c, r, . . . are vacation destina-
tions similar to i and rated well by the active user).
Feature style explanations (it is recommended based on items’
features). For example, we recommend movie i because it was
directed by director d, it features actors a, b, and it belongs to
genre g (d, a, b, g are features the active user is interested in).
Hybrid methods. This category primarily includes the following:
human/item, human/feature, feature/item, and human/feature/
item.
Additionally, in geo-social RS (Foursquare, Google latitude, etc.),
location information exists that must be used in the recommenda-
tion explanation mechanism [235]. Geo-social RS typically adopt a
hybrid human/item explanation method based on social, location
and memory-based information.
A reference publication that is a helpful introduction to the RS
explanations research ﬁeld has been published previously [91].
They explore the utility of explanations in CF RS, and they stated
three key research questions: (1) What models and techniques
are effective in supporting explanations? (2) Can explanation facil-
ities increase the acceptance of CF RS? (3) Can explanation facilities
increase the ﬁltering performance of the CF RS users? To answer to
the ﬁrst question, they propose using rating histograms, indica-
tions of past performance, comparisons to similar rated items,
and use of domain speciﬁc content features. The results from the
experiments conducted with RS users support an afﬁrmative re-
sponse to the second question. The third question is unanswered
Fig. 8. Content-based ﬁltering mechanism.
Fig. 9. Different alternatives for combining CF and CBF.
122
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

because users perform ﬁltering based on many different channels
of input.
A dynamic approach that favors the mechanisms for RS expla-
nations includes using conversational techniques, such as the CCBR
(conversational case-base reasoning), explained into McSherry
[155]. As CCBR they use an incremental nearest neighbor process
based on the Pareto case dominance approach. In a different study
[153], a dynamic approach is also adopted, but it employs a differ-
ent perspective. Instead of attempting to justify a particular recom-
mendation they focus on how explanations can help users to
understand the recommendation opportunities that remain if the
current recommendation should not meet their requirements.
They generate compound critiques as explanations: Users have
the opportunity to accept or critique recommendations. If they cri-
tique a recommendation, the critique acts as a ﬁlter over the
remaining recommendations.
In a separate study [24], authors differentiate between the con-
cepts promotion (increasing of the acceptance of the recommended
item) and satisfaction (user satisfaction with the recommended
item). They also produced better results by using the keyword style
explanation (based on content data) compared with the neighbor
style explanation (human style explanation). Authors propose a
new classiﬁcation of the recommendation justiﬁcations: Keyword
Style Explanation (for content-based RS), Neighbor Style Explana-
tion (for collaborative ﬁltering RS) and Inﬂuence Style Explanation
(tells the user how their interactions with the RS inﬂuences the
recommendation). Tintarev and Masthoff [217] describe the
advantages of making justiﬁcations in recommendations: trans-
parency, scutability, trustworthiness, effectiveness, persuasive-
ness, efﬁciency and satisfaction.
Billus and Pazzani [25] propose a recommendation system on
news, which provides keyword style justiﬁcations of the recom-
mendations through the weights used for obtaining these recom-
mendations. Wang et al. [226] describe a system of justiﬁcations
based on the features of users’ preference. Tintarnev and Masthoff
[217] design a recommedation system on ﬁlms whose recommen-
dations are justiﬁed through the features. Vig et al. [222] propose a
mechanism for justifying recommendations called tagsplanations,
which is based on community tags. Trangsplanations have two
key components: tag relevance, the degree to which a tag describes
an item; and tag preference, the user’s sentiment toward a tag.
Fahri [73] provides a framework for organizing justiﬁcations,
used to categorize explanations; they propose the categorization
of the discourse: explicative, theoretical, pragmatic, ethical, moral,
legal, aesthetic, and personal. Although this theoretical framework
has not been used into the research literature, it can be used to de-
sign new types of explanations. Hernando et al. [97] present a no-
vel explanation technique based on the visualization of trees of
items; these trees provide valuable information about the reliabil-
ity of recommendations and the importance of the ratings the user
has made.
The most relevant investigations that produce justiﬁcations in
recommender systems include a study [187] wherein the authors
design a new organization interface where results are grouped
according to their tradeoff properties. They have developed a trust
model for recommender agents based on the Pareto algorithm
(excluding dominated categories). Symeonidis et al. [213] ﬁrst con-
struct a feature proﬁle for the users to reveal their favorite features,
later they group users into biclusters to exploit partial matching
between de preferences of groups of users over groups of items.
Additionally they propose a metric to measure the quality of justi-
ﬁcations: the explain coverage ratio. In Symeonidis et al. [214] they
use a prototype ‘‘MoviExplain’’ to put into the test the research
showed into Symeonidis et al. [213]. In Hu et al. [102] they use im-
plicit feedback to derive an estimate of the user preference (like or
dislike an item) and user conﬁdence for each user-item pair.
7. Recommender systems trends
From the evolution of existing RS and research papers in the
ﬁeld, there is a clear tendency to collect and integrate more and
different types of data. This trend is parallel to the evolution of
the web, which we can deﬁne through the following three primary
stages: (1) at the genesis of the web, RS used only the explicit rat-
ings from users as well as their demographic information and con-
tent-based information included by the RS owners. (2) For the web
2.0, in addition to the above information, RS collect and use social
Fig. 10. Classiﬁcation of the recommendations to groups in CF RS. The ﬁgure represents the four representative cases for approaching the solution to group recommendations.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
123

information, such as friends, followers, followed, both trusted and
untrusted. Simultaneously, users aid in the collaborative inclusion
of such information: blogs, tags, comments, photos and videos. (3)
For the web 3.0 and the Internet of things, context-aware informa-
tion from a variety of devices and sensors will be incorporated
with the above information. Currently, geographic information is
included, and the expected trend is gradual incorporation of
information, such as radio frequency identiﬁcation (RFID) data, sur-
veillancedata, on-line health parameters and food and shopping
habits, as well as teleoperation and telepresence.
Context-aware recommender systems [5,1], focus on additional
contextual information, such as time, location, and wireless sensor
networks [80]. The contextual information can be obtained explic-
itly, implicitly, using data mining or with a mixture of these meth-
ods (hybrid). Currently, mobile applications increasingly use
geographic information; this information enables geographic RS
that can be considered as location-aware RS. For geographic RS
[167,152], recommendations are typically generated by consider-
ing the geographical position of the user that receives the
recommendation.
This section provides an introduction of concepts, which are
gaining popularity in the RS research ﬁeld: Internet of things, pri-
vacy preservation, shilling attacks, new frameworks, etc. In this
introduction, we provide a novel classiﬁcation for analyzing these
RS concepts. Next, we will deal with the research on the loca-
tion-aware RS, which may be regarded as the ﬁrst steps for future
RS based on Web 3.0. Finally, we will describe the most signiﬁca-
tive results on a promising research ﬁeld: the RS based on bio-in-
spired models.
7.1. Introduction
There is a clear trend towards collection of implicit information
instead of a traditional explicit evaluation of items by ratings.
Last.Fm is a good example of this situation; the user ratings are in-
ferred by the number of times they have heard each song. The
same can be applied in a number of everyday situations, such as
for access to web addresses, use of various public transport sys-
tems, food purchased, access to sports facilities and access to learn-
ing resources.
Incorporation of implicit information on the daily habits of
users allows RS to use a variety of data; these data will be used
in future CF processes, which are increasingly useful and accurate.
Privacy and security considerations will be increasingly important
with the widespread trend in using, with consent, devices and sen-
sors for the Internet of things.
Privacy is an important issue for RS [23] because the systems
contain information on large numbers of registered users. For pri-
vacy preservation in RS, a certain level of uncertainty must be intro-
duced into the predictions [156], primarily through tradeoffs
between accuracy and privacy [146]. Furthermore, privacy can be
preserved when different RS companies share information (com-
bining their data) [116,242]. Privacy becomes more important as
RS increasingly incorporate social information.
Because RS are often used in electronic commerce, unscrupulous
producers may ﬁnd proﬁtable to shill RS by lying to the systems in
order to have their products recommended more often than those
of their competitors. RS can experience shilling attacks [128,57],
which generate many positive ratings for a product, while products
from competitors receive negative ratings. RS are still highly vul-
nerable to such attacks [191].
Knowledge-based ﬁltering is emerging as an important ﬁeld of
RS. Knowledge RS [46] ‘‘use knowledge about users and products to
pursue a knowledge-based approach to generating recommenda-
tions, reasoning about what products meet the user’s requeri-
ments’’. Recommendations are based on inferences about users
needs and preferences. User models are based on knowledge struc-
tures such as querys (preferred features por products) [109], cases
(case-based reasoning) [44], constraints (constraint-based reason-
ing) [74], ontologies [159], matching metrics and knowledge vec-
tors [194], and social knowledge [53].
Workﬂow is a current knowledge ﬁeld where the user model is
based on ‘‘users-roles-tasks reference information that describes
which
member
plays
which
roles
or
fulﬁlls
which
tasks’’
[245,246]. Peer-to-peer (P2P) networks are other current knowl-
edge ﬁeld, where user information is based on the distributed
information existing from each peer and the set of peers who
may need her [247].
Gradual incorporation of different types of information (e.g., ex-
plicit ratings, social relations, user contents, locations, use trends,
knowledge-based information) has forced RS to use hybrid ap-
proaches. Once the memory-based, social and location-aware
methods and algorithms are consolidated, the evolution of RS dem-
onstrates a clear trend toward combining existing collaborative
methods.
The latest research in the CF ﬁeld has generated only modest
improvements for predictions and recommendations from a single
type of information (e.g., when the only information used is user
ratings, information from social relations, or item content). The re-
sults improve further when several algorithms are combined with
their respective data types. A growing number of publications ad-
dress hybrid approaches that use current databases to simulta-
neously incorporate memory-based, social and content-based
information.
To unify the above concepts, Fig. 11 provides an original taxon-
omy for RS. The taxonomy is classiﬁed depending on the nature of
the data rather than according to the methods and algorithms
used. The core of the taxonomy focuses on data classiﬁcation by
three factors: (1) the target of the data: user or item; (2) mode of
acquisition: explicit (i.e., ratings to items made by users) or impli-
cit (e.g., number of times a user has heard a song); and (3) informa-
tion level: memory, content or social context.
Fig. 11 shows the recommender methods and algorithms (la-
beled as ‘‘collaborative ﬁltering algorithms’’). Depending on the
information type in each RS database, it adopts a hybrid ﬁltering
approach. Each hybrid approach will use an appropriate subset of
algorithms to consider processing of existing information in a coor-
dinated manner. Future developments will include different rec-
ommendation
frameworks
that
address
the
most
common
situations. These frameworks allow RS to incorporate the CF kernel
with the most appropriate recommendations methods based on
the available information in a simple and straightforward manner.
At higher levels (prediction and recommendation), Fig. 11
incorporates current evaluation quality measures, such as those
for diversity and novelty. The importance of such measures, and
measures developed in the future will grow as users demand novel,
stable and less predictable recommendations.
7.2. Location-aware recommender systems
Due to the increasing use of mobile devices, location-aware sys-
tems are becoming more widespread. These systems show a ten-
dency towards their consolidation as web 3.0 services and this
naturally leads to location-aware CF and location-aware RS, which
can be called geographic CF and geographic RS.
We introduce a classiﬁcation for geographic CF RS and focus on
the most relevant section of the classiﬁcation obtained. Table 4
establishes the different possibilities of tackling a geographic RS
according to the nature of the ratings made (‘‘rating stage’’) and
the recommendation process followed (‘‘recommendation stage’’).
‘‘User’’ indicates that the rating and/or recommendation are made
without having or using the user’s Geographic Information (GI).
124
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

Similarly, ‘‘Item’’ indicates that the rating and/or recommendation
are made without having or using the item’s GI. In the cases la-
beled as ‘‘Userg’’ and ‘‘Itemg’’ the GI is used.
The cases identiﬁed are:
 RS: Traditional RS, in which ratings and recommendations are
made without using geographical information.
 RS + G: Traditional RS, which also contributes the item’s geo-
graphical position. These RS cannot be regarded as geographic
RS, as the GI does not play a part in the recommendation
process.
 GRS: This group of Geographic RS is most likely to become pop-
ular in the near future. In these, ratings are made in a traditional
way, whilst recommendations are made by considering the geo-
graphical position of the user to whom the recommendation is
to be made. A representative example is that of a RS for restau-
rants; the users rate a restaurant using very diverse concepts,
which do not include the distance at the time of voting between
the user and the restaurant. However, users of a Geographic RS
expects a restaurant to be recommended to them not only
because of good ratings from similar users (k-neighbors), but
also according to the distance between their current position
and that of the restaurant. Other possible examples are RS for
cinemas, pubs, supermarkets, cultural activities in a city, lan-
guage learning centers, gyms and sports clubs, etc.
 GRS+: In this case, users establish ratings on items by weighting
the distance between them and the items rated. In this type of
geographic RS two possibilities can be established:
1. Hybrid CF/Demographic ﬁltering: Each item accepts a max-
imum of one vote per user, to which the geographical posi-
tion from which it has been issued is associated.
2. Geographic RS where each item accepts more than one rat-
ing for each user, depending on the geographical position
from which each rating is made.
3. The hybrid RS in case 1 respond to regional or national geo-
graphical approaches, in which recommendations can be
established according to weighting between the similarity
of the votes (CF) and their origin. This type of GRS may
be regarded as an extended case of hybrid CF/demographic
ﬁltering, in which the GI is given for each vote instead of for
each user.
From a theoretical point of view, Type 2 GRS+ are the most com-
plete; however, from a practical point of view, they involve a
semantic difﬁculty in the item rating process, which makes their
use very difﬁcult. Rating items in this GRS+ involves that each user
can rate items according to the relative distances between the user
and the items. In this way, a user can rate a restaurant from their
home differently to how they would rate it from their workplace;
and when the distances are very different, the ratings are also
likely to be so. The mental process would be something like this:
I am 1 km from the restaurant and I rate very positively travelling
1 km to go to that restaurant which I think is good; but after some
time, the same user, who is at work, 24 km away from the restau-
rant, could cast a vote indicating they do not consider it to be po-
sitive to travel 24 km to go to the restaurant even if they think it is
good.
In summary, GRS+ have the advantage that they accept a wider
variety of ratings and that these also contain the relative impor-
tance that each user gives to the items according to the distance re-
quired to access them. The disadvantage is that it is difﬁcult to
involve users in a particularly complex and demanding ratings
process.
This subsection focuses on the GRS-type geographic CF RS. At
present, there are few publications regarding GI-based RS; This is
due, to a great extent, to the lack of public databases that include
ratings and geographic positions capable of being combined in an
RS. Some of the publications that focus more closely on the ﬁeld
are as follows:
Martinez et al. [149] and Biuk-Aghai et al. [28] are examples of
the RS + G group. In Schlieder [205], they propose a novel approach
for modeling the collaborative semantics of geographic folksono-
mies. This approach is based on multi-object tagging, that is, the
analysis of tags that users assign to composite objects. This paper
is based on the concept of groups of people who share a common
geospatial feature data dictionary (including deﬁnitions of feature
relationships) and a common metadata schema.
Wan-Shiou et al. [225] can be considered as a hybrid content
based/geographic RS. The core of the system is a hybrid content
based/geographic recommendation mechanism that analyzes a
customer’s history and position so that vendor information can
be ranked according to the match with the preferences of a
customer.
Matyas and Schlieder [152] show a collaborative system that
we could situate between a RS and a GRS. In this case, the users’
ratings are taken based on the photos they have downloaded from
a Web 2.0 and the photos they have uploaded to the same Web
(the photos have a GPS address associated to them). After this, a
search of k-neighborhoods based on this data is carried out. The
recommendation process does not take into account the user’s
position.
It is possible to collect travel GPS traces from users and use the
database to generate recommendations [249]. The travel GPS
traces can be reinforced with social information based on friends
[250]. Both papers can be classiﬁed as GRS+.
7.3. Bio-inspired approaches
Much of the proposed model-based RS are based on bio-in-
spired approaches, which primarily use Genetic Algorithms (GAs)
and Neural Networks (NNs). Models have also been proposed based
on Artiﬁcial Immune Networks (AINs).
GA are heuristic approaches based on evolutionary principles
such as natural selection and survival of the ﬁtest. GA have mainly
been used in two aspects of RS: clustering [120,243] and hybrid
user models [76,99,7]. A common technique to improve the fea-
tures of RS consists of initially carrying out a clustering on all of
the users, in such a way that a group of classes of similar users is
obtained, after this, the desired CF techniques can be applied to
each of the clusters, obtaining similar results but in much shorter
calculation times; It is usual to use common genetic clustering
algorithms such as GA-based K-means [121].
The RS hybrid user models commonly use a combination of CF
with demographic ﬁltering or CF with content based ﬁltering, to
exploit merits of each one of these techniques. In these cases, the
chromosome structure can easily contain the demographic charac-
teristics and/or those related to content-based ﬁltering.
In order to tackle location-based advertisement, Dao et al. [68]
propose a model-based CF using GA. They combine both user’s
preferences and interaction context. Bobadilla et al. [33] use GA
to create a similarity metric, weighting a set of very simple similar-
ity measures. Hwang et al. [106] employ a GA to learn personal
preferences of customers.
NN is a model based on the observed behavior of biological neu-
rons. This model, intended to simulate the way the brain processes
information, enables the computer to ‘‘learn’’ to a certain degree. A
NN typically consists of a number of interconnected nodes. Each
handles a designated sphere of knowledge, and has several inputs
from the network. Based on the inputs it gets, a node can ‘‘learn’’
about the relationships between sets of data, pattern, and, based
upon operational feedback, are molded into the pattern required
to generate the required results.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
125

The RS most relevant research available in which NN usually fo-
cuses is hybrid RS, in which NN are used for learn users proﬁles;
NN have also been used in the clustering processes of some RS.
The hybrid approaches enable NN to act on the additional infor-
mation to the ratings. In Ren et al. [192] they propose a hybrid rec-
ommender approach that employs Widrow-Hoff [229] algorithm
to learn each user’s proﬁle from the contents of rated items. This
improves the granularity of the user proﬁling. In Christakou and
Stafylopatis [62] they use a combination of content-based and CF
in order to construct a system that provides more precise recom-
mendations concerning movies. In Lee and Woo [133] ﬁrst, all
users are segmented by demographic characteristics and users in
Fig. 11. Recommender systems taxonomy.
126
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

each segment are clustered according to the preference of items
using the Self-Organizing Map (SOM) NN. Kohonon’s SOMs are a
type of unsupervised learning; their goal is to discover some
underlying structure of the data.
Two alternative NN uses are presented in Huang et al. [103] and
Roh et al. [193]. In the ﬁrst case paper, authors use a training back-
propagation NN for generating association rules that are mined
from a transactional database; in the second paper, authors pro-
pose a model that combines a CF algorithm with two machine
learning processes: SOM and Case Based Reasoning (CBR) by
changing an unsupervised clustering problem into a supervised
user preference reasoning problem.
Neuro-fuzzy inference has been used in Sevarac et al. [207] to
create pedagogical rules in e-learning. A new cold-start similarity
measure has been perfected in Bobadilla et al. [36] using optimiza-
tion based on neural learning.
Artiﬁcial immune systems are distributed and adaptive systems
using the models and principles derived form the human immune
system. They model the defence system which can protect our
body against infections. In order to tackle the RS sparsity problem
and to make algorithms more scalable, Acilar and Arslan [2] pres-
ent a new CF model based on the AIN Algorithm (aiNet). AIN were
previously proposed to general recommendations [49] and to rec-
ommend web sites [161].
8. Related works and original contributions of the paper
As CF has become more complex, different survey papers have
been published in this area. Schafer et al. [203] introduces the core
concepts of CF: the theory and practice, the rating systems and
their acquisition, evaluation, interaction interfaces and privacy is-
sues. Candillier et al. [51] review the main CF ﬁltering methods
and compare their results.
Su and Khoshgoftaar [212] presents a survey of CF techniques.
Authors introduce the theory on CF and concisely deal with the
main challenges: sparsity, scalability, synonymy, gray sheep, shil-
ling attacks, privacy, etc. They also expose an overview table of
CF techniques.
Park et al. [171] review 210 papers on RS and classiﬁes them by
the year and journal of the publication, their application ﬁelds, and
their data mining techniques. Additionaly, they categorized the pa-
pers into eight application ﬁelds (ﬁlms, music, etc.).
A review in RS algorithms is presented in [141]. This paper fo-
cuses on explaining carefully how the most used algorithms in
RS work. The paper presents also the basic concepts of CF and their
evaluation metrics, dimensionality reduction techniques, diffu-
sion-based methods, social ﬁltering and meta approaches.
Our survey tries to include the most novel issues that have not
been dealt carefully in the previous papers. Next, we will stand out
the most outstanding features of this survey:
 Uses a methodology for selecting the most suitable papers in
the RS, standing out the latest and most cited papers in the area
of RS.
 Provides an updated overview table of the most used RS public
databases, including tags and friend relations information.
 Studies the cold-start problem inherent to all the RS.
 Presents a novel overview table informing both the classical
similarity measure and those which have recently been pro-
posed. It includes both the tailored metrics for cold-start users
and the general-purpose metrics. Besides, we show the quality
measures obtained when evaluating such metrics.
 Includes the recent quality measurements, beyond accuracy, to
evaluate RS: novelty, diversity and stability. Additionaly, we
include a reliability measure associated to predictions and
recommendations.
 Provides a comprehensive survey on social ﬁltering, presenting
a novel overview table on trust, reputation and credibility.
 Introduces the content-based ﬁltering from a modern perspec-
tive standing out its application for dealing with social informa-
tion, such as social tagging.
 Presents a summary of the most relevant contributions in the RS
for group of users. We will show a novel classiﬁcation for the
existing methods.
 Deals with a fast growing RS ﬁeld: the location-aware RS, based
on geographic information. This section is estructured with the
help of a novel geographic RS classiﬁcation table.
 Summarizes the most relevant contributions on the use of bio-
inspired approaches.
 Describes the RS trends to implicitally collect data (specially
those derived from the use of Internet of things).
 Provides an RS taxonomy for classifying the RS through three
factors: source of data (traditional web, social web 2.0, Internet
of things/web 3.0); target of data (users, items); method for
extracting data (explicit, implicit).
9. Conclusions
Recommender systems are proving to be a useful tool for
addressing a portion of the information overload phenomenon
from the Internet. Its evolution has accompanied the evolution of
the web. The ﬁrst generation of recommender systems used tradi-
tional websites to collect information from the following three
sources: (a) content-based data from purchased or used products,
(b) demographic data collected in users’ records, and (c) mem-
ory-based data collected from users’ item preferences. The second
generation of recommender systems, extensively use the web 2.0
by gathering social information (e.g., friends, followers, followed,
trusted users, untrusted users). The third generation of recom-
mender systems will use the web 3.0 through information pro-
vided by the integrated devices on the Internet. The use of
location information already incorporated in many recommender
systems will be followed by data from devices and sensors, which
will be widely used (e.g., real-time health signals, RFID, food habits,
online
local
weather
parameters
such
as
temperature
and
pressure).
The ﬁrsts recommender systems were focused on improving
recommendation accuracy through ﬁltering. Most memory-based
methods and algorithms were developed and optimized in this
context (e.g., kNN metrics, aggregation approaches, singular value
decomposition, diffusion-based methods, etc.). At this stage, hybrid
approaches (primarily collaborative–demographic and collabora-
tive–content ﬁltering) improved the quality of the recommenda-
tions.
In
the
second
stage,
algorithms
that
included
social
information with previous hybrid approaches were adapted and
developed
(e.g.,
trust-aware
algorithms,
social
adaptive
ap-
proaches, social networks analysis, etc.). Currently, the hybrid
ensemble algorithms incorporate location information into exist-
ing recommendation algorithms.
Evaluation of the predictions and recommendations has evolved
since the origins of recommender systems, which weighted
prediction errors (accuracy) heavily. They also recognized the
Table 4
Geographic collaborative ﬁltering recommender systems classiﬁcation.
Rating stage
Recommendation stage
User GI
Item
Itemg
Item
Itemg
User
RS/GRS
–
RS
RS + G
Not
Userg
–
GRS+
–
GRS/GRS+
Yes
Item GI
Not
Yes
Not
Yes
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
127

convenience of evaluating the quality of the top n recommenda-
tions as a set; evaluation of the top n recommendations as a ranked
list was then incorporated. Currently, there is a tendency to assess
new evaluation measures, such as diversity and novelty.
Future research will concentrate on advancing the existing
methods and algorithms to improve the quality of recommender
systems predictions and recommendations. Simultaneously, new
lines of research will be developed for ﬁelds and aims, such as
on: (1) proper combination of existing recommendation methods
that use different types of available information, (2) to get the
maximum use of the individual potential of various sensors and
devices on the Internet of things, (3) acquisition and integration
of trends related to the habits, consumption and tastes of individ-
ual users in the recommendation process, (4) data mining from RS
databases for non-recommendation uses (e.g., market research,
general trends, visualization of differential characteristics of demo-
graphic groups), (5) enabling security and privacy for recom-
mender systems processes, (6) new evaluation measures and
developing a standard for non-standardized evaluation measures,
and (7) designing ﬂexible frameworks for automated analysis of
heterogeneous data.
References
[1] S. Abbar, M. Bouzeghoub, S. Lopez, Context-aware recommender systems: a
service oriented approach, in: Proceedings of the 3rd International Workshop
on Personalized Access, Proﬁle Management and Context Awareness in
Databases, 2009.
[2] A.M. Acilar, A. Arslan, A collaborative ﬁltering method based on artiﬁcial
immune network, Expert Systems with Applications 36 (4) (2008) 8324–
8332.
[3] G. Adomavicius, A. Tuzhilin, Toward the next generation of recommender
systems: a survey of the state-of-the-art and possible extensions, IEEE
Transactions on Knowledge and Data Engineering 17 (6) (2005) 734–749.
[4] G. Adomavicius, J. Zhang, On the stability of recommendations algorithms, in:
ACM Conference on Recommender Systems, 2010, pp. 47–54.
[5] G. Adomavicius, A. Tuzhilin, Context-Aware recommender Systems, in: F.
Ricci, et al. (Ed.), Recommender Systems Handbook, 2011, pp. 217–253.
[6] H.J. Ahn, A new similarity measure for collaborative ﬁltering to alleviate the
new user cold-starting problem, Information Sciences 178 (2008) 37–51.
[7] M.Y.H. Al-Shamri, K.K. Bharadwaj, Fuzzy-genetic approach to recommender
systems based
on
a novel hybrid
user model, Expert
Systems
with
Applications 35 (3) (2008) 1386–1399.
[8] J. Al-Sharawneh, M.A. Williams, Credibility-aware Web-based social network
recommender: follow the leader, in: Proceedings of the 2010 ACM Conference
on Recommender Systems, 2010, pp. 1–8.
[9] S. Alonso, F.J. Cabrerizo, F. Chiclana, F. Herrera, E. Herrera-Viedma, Group
decision making with incomplete fuzzy linguistic preference relations,
International Journal of Intelligent Systems 24 (2009) 201–222.
[10] A. Ansari, S. Essegaier, R. Kohli, Internet recommendation systems, Journal of
Marketing Research 37 (3) (2000) 363–375.
[11] N. Antonopoulus, J. Salter, Cinema screen recommender agent: combining
collaborative and content-based ﬁltering, IEEE Intelligent Systems (2006) 35–
41.
[12] P. Antunes, V. Herskovic, S.F. Ochoa, J.A. Pino, Structuring dimensions for
collaborative systems evaluation, ACM Computing Surveys 44 (2) (2012).
Article 8.
[13] O. Arazy, N. Kumar, B. Shapira, Improving Social Recommender Systems,
Journal IT Professional 11 (4) (2009) 31–37.
[14] L.
Ardissono,
A.
Goy,
G.
Petrone,
M.
Segnan,
P.
Torasso,
INTRIGUE:
Personalized
recommendation
of
tourist
attractions
for
desktop
and
handset devices, Applied Artiﬁcial Intelligence 17 (8-9) (2003) 687–714.
[15] R. Baeza-Yates, B. Ribeiro-Neto, Modern Information Retrieval, Addison-
Wesley, 1999.
[16] M. Balabanovic, Y. Shoham, Content-based, collaborative recommendation,
Communications of the ACM 40 (3) (1997) 66–72.
[17] L. Baltrunas, T. Makcinskas, F. Ricci, Group recommendation with rank
aggregation and collaborative ﬁltering, in: Proceedings of the 2010 ACM
Conference on Recommender Systems, 2010, pp. 119–126.
[18] A.B. Barragáns-Martı´nez, E. Costa-Montenegro, J.C. Burguillo, M. Rey-López,
F.A. Mikic-Fonte, A. Peleteiro, A hybrid content-based and item-based
collaborative ﬁltering approach to recommend TV programs enhanced with
singular value decomposition, Information Sciences 180 (22) (2010) 4290–
4311.
[19] C. Basu, H. Hirsh, W. Cohen, Recommendation as classiﬁcation: using social
and content-based information in recommendation, in: Proceedings of the
Fifteenth National Conference on Artiﬁcial Intelligence, 1998, pp. 714–720.
[20] P. Bedi, R. Sharma, Trust based recommender system using ant colony for
trust computation, Expert Systems with Applications 39 (1) (2012) 1183–
1190.
[21] Y. Bengio, Y. Grandvalet, No umbiased estimator of the variance of k-fold
cross-validation, Journal of Machine Learning Research 5 (2004) 1089–1105.
[22] S. Berkovsky, J. Freyne, Group-based recipe recommendations: analysis of
data aggregation strategies, in: Proceedings of the 2010 ACM Conference on
Recommender Systems, 2010, pp. 111–118.
[23] A. Bilge, H. Polat, An improved privacy-preserving DWT-based collaborative
ﬁltering scheme, Experts Systems with Applications 39 (3) (2012) 3654–
3841.
[24] M. Bilgic, R. Mooney, Explanation for recommender systems: satisfaction vs.
promotion, in: Next Stage of Recommender Systems Research Workshop (IUI
conference), 2005, pp. 13–18.
[25] D. Billsus, M. Pazzani, A personal news agent that talks, learns and explains,
in: Proc. Auton. Agents Conf., 1999, pp. 268–275.
[26] D. Billsus, M. Pazzani, User modeling for adaptive news access, User Modeling
and User-Adapted Interaction 10 (2–3) (2000) 147–180.
[27] D. Billsus, M. Pazzani, J. Chen, A learning agent for wireless news access, in:
Proceedings of the International Conference on Intelligent User Interfaces,
2002, pp. 33–36.
[28] R.P. Biuk-Aghai, S. Fong, S. Yain-Whar, Design of a recommender system for
mobile tourism multimedia selection, in: 2nd International Conference on
Internet Multimedia Services Architecture and Applications (IMSAA), 2008,
pp. 1–6.
[29] J. Bobadilla, F. Serradilla, The effect of sparsity on collaborative ﬁltering
metrics, in: Australian Database Conference, 2009, pp. 9–17.
[30] J. Bobadilla, F. Serradilla, A. Hernando, Collaborative ﬁltering adapted to
recommender systems of e-learning, Knowledge Based Systems 22 (2009)
261–265.
[31] J. Bobadilla, F. Serradilla, J. Bernal, A new collaborative ﬁltering metric that
improves the behavior of recommender systems, Knowledge Based Systems
23 (2010) 520–528.
[32] J. Bobadilla, A. Hernando, F. Ortega, J. Bernal, A framework for collaborative
ﬁltering recommender systems, Expert Systems with Applications 38 (12)
(2011) 14609–14623.
[33] J. Bobadilla, F. Ortega, A. Hernando, J. Alcalá, Improving collaborative ﬁltering
recommender systems results and performance using genetic algorithms,
Knowledge Based Systems 24 (8) (2011) 1310–1316.
[34] J. Bobadilla, A. Hernando, F. Ortega, A. Gutiérrez, Collaborative ﬁltering based
on signiﬁcances, Information Sciences 185 (1) (2012) 1–17.
[35] J. Bobadilla, F. Ortega, A. Hernando, A collaborative ﬁltering similarity
measure based on singularities, Information Processing and Management
48 (2) (2012) 204–217.
[36] J. Bobadilla, F. Ortega, A. Hernando, J. Bernal, A collaborative ﬁltering
approach to mitigate the new user cold start problem, Knowledge Based
Systems 26 (2012) 225–238.
[37] J.
Bobadilla,
F.
Ortega,
A.
Hernando,
J.
Bernal,
Generalization
of
recommender systems: collaborative ﬁltering extended to groups of users
and restricted to groups of items, Expert Systems with Applications 39 (2012)
172–186.
[38] J. Bobadilla, F. Ortega, A. Hernando, A. Arroyo, A balanced memory-based
collaborative ﬁltering similarity measure, International Journal of Intelligent
Systems 27 (10) (2013) 939–946.
[39] T. Bogers, A. Van Den Bosch, Collaborative and content-based ﬁltering for
item recommendation on social bookmarking websites, in: Proceedings of the
2009 ACM Conference on Recommender Systems, 2009, pp. 9–16.
[40] P. Bonhard, Who do trust? Combining recommender systems and social
networking for better advice, in: International Conference on Intelligent User
Interfaces, 2005.
[41] P. Bonhard, M.A. Sasse, ‘Knowing me, knowing you’—Using proﬁles and social
networking to improve recommender Systems, BT Technology Journal 24 (3)
(2006) 84–98.
[42] P. Borzymek, M. Sydow, A. Wierbicki, Enriching trust prediction model in
social network with user rating similarity, in: Proceedings of the 2009
International Conference on Computational Aspects of Social Network, 2009,
pp. 40–47.
[43] J.S.
Breese,
D.
Heckerman,
C.
Kadie,
Empirical
analysis
of
predictive
algorithms for collaborative ﬁltering, in: 14th Conference on Uncertainty in
Artiﬁcial Intelligence, 1998, pp. 43–52.
[44] D. Bridge, M.H. Goker, L. McGinty, B. Smyth, Case-based recommender
systems, The Knowledge Engineering Review 20 (3) (2005) 315–320.
[45] R. Burke, Encyclopedia of library and information systems, in: A. Kent (Ed.),
vol.
69(Suppl.
32),
Marcel
Dekker,
2000
(Chapter:
Knowledge-Based
Recommender Systems).
[46] R. Burke, A case-based reasoning approach to collaborative ﬁltering, in:
EWCBR 2000, 2000, pp. 370–379.
[47] R. Burke, Hybrid recommender systems: survey and experiments, User
Modeling and User-Adapted Interaction 12 (4) (2002) 331–370.
[48] F.
Cacheda,
V.
Carneiro,
D.
Fernández,
V.
Formoso,
Comparison
of
collaborative ﬁltering algorithms: limitations of current techniques and
proposals
for
scalable,
high-performance
recommender
Systems,
ACM
Transactions on the Web 5 (1) (2011). Article 2.
[49] S. Caizer, U. Aickelin, A recommender system based on idiotypic artiﬁcial
immune networks, Journal of Mathematics, Models and Algorithms 4 (2)
(2005) 181–198.
128
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

[50] L.M.
Campos,
J.M.
Fernández-Luna,
J.F.
Huete,
M.A.
Rueda-Morales,
Combining content-based and collaborative recommendations: a hybrid
approach
based
on
Bayesian
Networks,
International
Journal
of
Approximate Reasoning 51 (7) (2010) 785–799.
[51] L. Candillier, F. Meyer, M. Boullé, Comparing state-of-the-art collaborative
ﬁltering systems, Lecture Notes in Computer Sciece 4571 (2007) 548–562.
[52] F. Carmagnola, F. Vernero, P. Grillo, SoNARS: a social networks-based
algorithm for social recommender systems, in: Proceedings of the 17th
International Conference on User Modeling, Adaptation, and Personalization:
Formerly UM and AH, 2009, pp. 223–234.
[53] W. Carrer-Neto, M.L. Hernández-Alcaraz, R. Valencia-Garcı´a, F. Garcı´a-
Sánchez, Social knowledge-based recommender system, Application to the
movies domain. Expert Systems with Applications 39 (12) (2012) 10990–
11000.
[54] J.J. Castro-Sanchez, R. Miguel, D. Vallejo, L.M. López-López, A highly adaptive
recommender system based on fuzzy logic for B2C e-commerce portals,
Expert Systems with Applications 38 (3) (2011) 2441–2454.
[55] D. Chao, J. Balthrop, S. Forrest, Adaptive radio: achieving consensus using
negative preferences, in: International ACM SIGGROUP Conference on
Supporting Group Work, 2005, pp. 120–123.
[56] T. Chen, L. He, Collaborative ﬁltering based on demographic attribute vector,
in: Proceedings of the International Conference on Future Computer and
Communication, 2009, pp. 225–229.
[57] P.A. Chirita, W. Nejdl, C. Zamﬁr, Preventing shilling attacks in online
recommender
systems,
in:
Workshop
on
Web
Information
and
Data
Management, 2005, pp. 67–74.
[58] J. Cho, K. Kwon, Y. Park, Q-rater: a collaborative reputation system based on
source credibility theory, Expert Systems with Applications 36 (2009) 3751–
3760.
[59] S.B. Cho, J.H. Hong, M.H. Park, Location-based recommendation system using
Bayesian user’s preference model in mobile devices, Lecture Notes in
Computer Science 4611 (2007) 1130–1139.
[60] K. Choi, D. Yoo, G. Kim, Y. Suh, A hybrid online-product recommendation
system:
combining
implicit
rating-based
collaborative
ﬁltering
and
sequential
pattern
analysis.
Electronic
Commerce
Research
and
Applications, in press, doi: 10.1016/j.elerap.2012.02.004.
[61] K. Choi, Y. Suh, A new similarity fuction for selecting neighbors for each target
item in collaborative ﬁltering, Knowledge Based Systems 37 (2013) 146–153.
[62] C. Christakou, A. Stafylopatis, A hybrid movie recommender system based on
neural networks, in: International Conference on Intelligent Systems Design
and Applications, 2005, pp. 500–505.
[63] I.A. Christensen, S. Schiafﬁno, Entertainment recommender systems for group
of users, Expert Systems with Applications 38 (2011) 14127–14135.
[64] M. Claypool, A. Gokhale, T. Miranda, P. Murnikov, D. Netes, M. Sartin,
Combining content-based and collaborative ﬁlters in an online newspaper,
in: Proceedings of ACM SIGIR Workshop on Recommender Systems, 1999, pp.
40–48.
[65] W. Cohen, Fast effective rule induction, in: Proceedings of the Twelfth
International Conference on Machine Learning, 1995, pp. 115–123.
[66] M. Condliff, D. Lewis, D. Madigan, C. Posse, Bayesian mixed-effects models for
recommender systems, in: ACM SIGIR ’99 Workshop on Recommender
Systems: Algorithms and Evaluation, 1999, pp. 23–30.
[67] E. Costa-Montenegro, A.B. Barragáns-Martı´ nez, M. Rey-López, Which App? A
recommender system of applications in markets: implementation of the
service for monitoring users’ interaction, Expert Systems with Applications
39 (10) (2012) 9367–9375.
[68] T.H. Dao, S.R. Jeong, H. Ahn, A novel recommendation model of location-based
advertising: context-aware collaborative ﬁltering using GA approach, Expert
Systems with Applications 39 (3) (2012) 3731–3739.
[69] M. Dell’amico, L. Capra, SOFIA: social ﬁltering for robust recommendation,
IFIP Advances in Information and Communication Technology 263 (2008)
135–150.
[70] T. Dubois, J. Golbeck, J. Kleint, A. Srinivasan, Improving recommendation
accuracy by clustering social networks with trust, in: Proceedings of the 2009
ACM Conference on Recommender Systems, 2009, pp. 1–8.
[71] M. Ekström, H. Björnsson, C. Nass, A reputation mechanism for business-to-
business electronic commerce that accounts for rater credibility, Journal of
Organizational Computing and Electronic Commerce 15 (1) (2005) 1–18.
[72] I. Esslimani, A. Brun, A. Boyer, From social networks to behavioral networks in
recommender systems, in: Proceedings of the 2009 International Conference
on Advances in Social Network Analysis and Mining, 2009, pp. 143–148.
[73] Y. Fahri, A Framework for Organizing Justiﬁcations for Strategic use in
Adaptive Iteraction Contexts, ECIS, 2008. Article 250.
[74] A. Felfernig, R. Burke, Constraint-based recommender systems: technologies
and
research
issues,
in:
10th
International
Conference
on
Electronic
Commerce, 2008 (Article No. 3).
[75] L. Fengkun, J.L. Hong, Use of social network information to enhance
collaborative ﬁltering performance, Expert Systems with Applications 37
(7) (2010) 4772–4778.
[76] L.Q. Gao, C. Li, Hybrid personalizad recommended model based on genetic
algorithm,
in:
International
Conference
on
Wireless
Communication,
Networks and Mobile Computing, 2008, pp. 9215–9218.
[77] M. Gao, Z. Wu, F. Jiang, Userrank for item-based collaborative ﬁltering
recommendation, Information Processing Letters 111 (9) (2011) 440–446.
[78] I. Garcı´ a, L. Sebastia, E. Onaindia, On the design of individual and group
recommender systems for tourism, Expert Systems with Applications 38
(2011) 7683–7692.
[79] R. Garcı´a, X. Amatriain, Weighted content based methods for recommending
connections in online social networks, in: Proceedings of the 2010 ACM
conference on Recommender Systems, 2010, pp. 68–71.
[80] D. Gavalas, M. Kenteris, A web-based pervasive recommendation system for
mobile tourist guides, Personal and Ubiquitous Computing 15 (7) (2011)
759–770.
[81] F. Gedikli, D. Jannach, Rating items by rating tags, in: Proceedings of the 2010
ACM Conference on Recommender Systems, 2010, pp. 25–32.
[82] J. Gemmell, T. Schimoler, B. Mobasher, R. Burke, Resource recommendation
for social tagging: a multi-channel hybrid approach, in: Proceedings of the
2010 ACM Conference on Recommender Systems, 2010, pp. 60–67.
[83] J. Gemmell, T. Schimoler, M. Ramezani, L. Christiansen, B. Mobasher,
Improving FolkRank with item-based collaborative ﬁltering, in: Proceedings
of the 2009 ACM conference on Recommender Systems, 2009, pp. 17–24.
[84] M. Gemmis, P. Lops, G. Semeraro, P. Basile, Integrating tags in a semantic
content-based recommender, in: Proceedings of the 2008 ACM conference on
Recommender Systems, 2008, pp. 163–170.
[85] T. George, S. Meregu, A scalable collaborative ﬁltering Framework base don
co-clustering, in: IEEE International Conference on Data Mining (ICDM), 2005,
pp. 625–628.
[86] J. Golbeck, U. Kuter, The ripple effect: change in trust and its impact over a
social
network,
in:
Computing
with
Social
Trust,
Human–Computer
Interaction Series, Part II, 2009, pp. 169–181 (Chapter 7).
[87] K. Goldberg, T. Roeder, D. Gupta, C. Perkins, Eigentaste: a constant time
collaborative ﬁltering algorithm, Information Retrieval 4 (2) (2001) 133–151.
[88] R. González-Crespo, O. Sanjuán-Martı´nez, J. Manuel-Cueva, B. Cristina-Pelayo,
J.E.
Labra-Gayo,
P.
Ordoñez,
Recommendation
system
based
on
user
interaction data applied to intelligent electronic books, Computers in
Human Behavior 27 (4) (2011) 1445–1449.
[89] N. Good, J.B. Schafer, J.A. Konstan, A. Borchers, B. Sarwar, J.L. Herlocker, J.
Riedl, in: Proceedings of the Sixteenth National Conference on Artiﬁcial
Intelligence and the Eleventh Innovative Applications of Artiﬁcial Intelligence
Conference Innovative Applications of Artiﬁcial Intelligence, 1999, pp. 439–
446.
[90] A. Gunawardana, G. Shani, A survey of accuracy evaluation metrics of
recommender tasks, Journal of Machine Learning Reearch 10 (2009) 2935–
2962.
[91] J.L.
Herlocker,
J.A.
Konstan,
J.
Riedl,
Explaining
collaborative
ﬁltering
recommendations,
in:
ACM
Conference
on
Computer
Supported
Cooperative Work (CSCW), 2000, pp. 241–250.
[92] J.L. Herlocker, J.A. Konstan, A.L. Borchers, J.T. Riedl, An algorithmic framework
for performing collaborative ﬁltering, in: Proceedings of the 22nd Annual
International ACM SIGIR Conference on Research and Development in
Information Retrieval, 1999, pp. 230–237.
[93] J.L. Herlocker, J.A. Konstan, J.T. Riedl, An empirical analysis of design choices
in
neighborhood-based
collaborative
ﬁltering
algorithms,
Information
Retrieval 5 (2002) 287–310.
[94] J.L. Herlocker, J.A. Konstan, J.T. Riedl, L.G. Terveen, Evaluating collaborative
ﬁltering recommender systems, ACM Transactions on Information Systems
22 (1) (2004) 5–53.
[95] F. Hernández, E. Gaudioso, Evaluation of recommender systems: a new
approach, Expert Systems with Applications 35 (2008) 790–804.
[96] A. Hernando, J. Bobadilla, F. Ortega, J. Tejedor, Incorporating reliability
measurements into the predictions of a recommender systems. Information
Sciences, in press, doi: 10.1016/j.ins.2013.03.018.
[97] A. Hernando, J. Bobadilla, F. Ortega, A. Gutiérrez, Trees for explaining
recommendations
made
through
collaborative
ﬁltering,
Information
Sciences 218 (2013) 1–16.
[98] K. Heung-Nam, E.S. Abdulmotaleb, J. Geun-Sik, Collaborative error-reﬂected
models for cold-start recommender systems, Decision Support Systems 51 (3)
(2011) 519–531.
[99] Y. Ho, S. Fong, Z. Yan, A hybrid ga-based collaborative ﬁltering model for
online recommenders, in: International Conference on e-Business, 2007, pp.
200–203.
[100] L. Hossain, D. Fazio, The social networks of collaborative process, The Journal
of High Technology Management Research 20 (2) (2009) 119–130.
[101] H.R. Hu, P. Pu, Using personality information in collaborative ﬁltering for new
users, in: Proceedings of the 2010 ACM Conference on Recommender
Systems, 2010, pp. 17–24.
[102] Y. Hu, Y. Koren, C.H. Volinsky, Collaborative ﬁltering for implicit feedback
datasets, in: IEEE International Conference on Data Mining (ICDM), 2008, pp.
263–272.
[103] Y.P. Huang, W.P. Chuang, Y.H. KE, F.E. Sandnes, Using back-propagation to
learn association rules for service personalization, Expert Systems with
Applications 35 (2008) 245–253.
[104] Z. Huang, D. Zeng, H. Chen, A comparison of collaborative ﬁltering
recommendation algorithms for e-commerce, IEEE Intelligent Systems 22
(5) (2007) 68–78.
[105] N. Hurley, M. Zhang, Novelty and diversity in top-N recommendations-
analysis and evaluation, ACM Transactions on Internet Technology 10 (4)
(2011) 1–29.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
129

[106] CH.S.
Hwang,
Y.CH.
Su,
K.CH.
Tseng,
Using
genetic
algorithms
for
personalized recommendation, Lecture Notes in Computer Science 6422
(2010) 104–112.
[107] H. Ingoo, J.O. Kyong, H.R. Tae, The collaborative ﬁltering recommendation
based on SOM cluster-indexing CBR, Expert Systems with Applications 25
(2003) 413–423.
[108] A. Jameson, B. Smyth, Recommendation to groups, in: P. Brusilovsky, A.
Kobsa, W. Nejdl (Eds.), The Adaptive Web, 2007, pp. 596–627 (Chapter 20).
[109] D. Jannach, Fast computation of query relaxations for knowledge-based
recommenders, AI Communications 22 (4) (2009) 235–248.
[110] R. Jäschke, L. Marinho, A. Hotho, L. Schmidt-Thieme, G. Stumme, Tag
Recommendations in Folksonomies, in: Proceedings of the 11th European
Conference on Principles and Practice of Knowledge Discovery in Databases,
2007, pp. 506–514.
[111] J.J.
Jason,
Contextualized
mobile
recommendation
service
based
on
interactive social network discovered from mobile users, Expert Systems
with Applications 36 (9) (2009) 11950–11956.
[112] B. Jeong, J. Lee, H. Cho, User credit based collaborative ﬁltering, Expert
Systems with Applications 36 (2009) 7309–7312.
[113] T. Joachims, Text categorization with support vector machines: learning with
many relevant features, in: European Conference on Machine Learning, 1998,
pp. 137–142.
[114] A. Jøsang, R. Ismail, C. Boyd, A survey of trust and reputation systems for
online service provision, Decision Support Systems 43 (2) (2007) 618–644.
[115] CH.W. Jyun, CH.CH. Chui, Recommending trusted online auction sellers using
social network analysis, Expert Systems with Applications 34 (3) (2008)
1666–1679.
[116] C.
Kaleli,
H.
Polat,
Privacy-preserving
SOM-based
recommendations
on horizontally distributed data, Knowledge Based Systems 33 (2012)
124–135.
[117] H.N. Kim, A. Alkhaldi, A.E. Saddik, G.S. Jo, Collaborative user modeling with
user-generated tags for social recommender Systems, Expert Systems with
Applications 38 (7) (2011) 8488–8496.
[118] H.N. Kim, A.T. Ji, I. Ha, G.S. Jo, Collaborative ﬁltering based on collaborative
tagging for enhancing the quality of recommendations, Electronic Commerce
Research and Applications 9 (1) (2010) 73–83.
[119] J. Kim, B. Lee, M. Shaw, H. Chang, W. Nelson, Application of decision-tree
induction techniques to personalized advertisements on internet storefronts,
International Journal of Electronic Commerce 5 (3) (2001) 45–62.
[120] K. Kim, H. Ahn, Using a clustering genetic algorithm to support customer
segmentation for personalizad recommender systems, in: Proceedings of the
13th International Conference on AI, Simulation, and Planning in High
Autonomy Systems, 2004, pp. 409-415.
[121] K. Kim, H. Ahn, A recommender system using GA K-means clustering in an
online Shopping market, Expert Systems with Applications 34 (2) (2008)
1200–1209.
[122] S. Kitisin, C. Neuman, Reputation-based trust-aware recommender system,
in: Securecomm and Workshops, 2009, pp. 1–7.
[123] F. Kong, X. Sun, S. Ye, A comparison of several algorithms for collaborative
ﬁltering in startup stage, IEEE Transactions on Networks, Sensing and Control
(2005) 25–28.
[124] Y.
Koren,
R.
Bell,
CH.
Volinsky,
Matrix
factorization
techniques
dor
recommender systems, IEEE Computer 42 (8) (2009) 42–49.
[125] G. Koutrika, B. Bercovitz, H. Garcia, FlexRecs: expressing and combining
ﬂexible recommendations, in: Proceedings of the 35th SIGMOD International
Conference on Management of Data, 2009, pp. 745–757.
[126] B.
Krulwich,
Lifestyle
ﬁnder:
intelligent
user
proﬁling
using
large-scale demographic data, Artiﬁcial Intelligence Magazine 18 (2) (1997)
37–45.
[127] K. Kwon, J. Cho, Y. Park, Multidimensional credibility model for neighbor
selection
in
collaborative
recommendation,
Expert
Systems
with
Applications 36 (2009) 7114–7122.
[128] S.K. Lam, J. Riedl, Shilling recommender systems for fun and proﬁt, in:
International Conference on World Wide Web, 2004, pp. 393–402.
[129] X.N. Lam, T. Vu, T.D. Le, A.D. Duong, Addressing cold-start problem in
recommendation
systems,
in:
Conference
On
Ubiquitous
Information
Management And Communication, 2008, pp. 208–211.
[130] N. Landia, S.S. Anand, Personalised tag recommendation, in: Proceedings of
the 2009 ACM Conference on Recommender Systems, 2009, pp. 83–36.
[131] K. Lang, NewsWeeder: learning to ﬁlter netnews, in: Proceedings 12th
International Conference on Machine Learning, 1995, pp. 331–339.
[132] D.H. Lee, P. Brusilovsky, Does trust inﬂuence information similarity? in:
Proceedings of the 2009 ACM Conference on Recommender Systems, 2009,
pp. 71–74.
[133] M. Lee, Y. Woo, A hybrid recommender system combining collaborative
ﬁltering with neural network, Lecture Notes on Computer Sciences 2347
(2002) 531–534.
[134] S.K. Lee, Y.H. Cho, S.H. Kim, Collaborative ﬁltering with ordinal scale-based
implicit ratings for mobile music recommendations, Information Sciences
180 (11) (2010) 2142–2155.
[135] C.W. Leung, S.C. Chan, F.L. Chung, An empirical study of a cross-level
association rule mining approach to cold-start recommendations, Knowledge
Based Systems 21 (7) (2008) 515–529.
[136] Q. Li, Clustering approach for hybrid recommender system, in: Proceedings of
the 2003 IEEE/WIC International Conference on Web Intelligence, 2003, pp.
33–38.
[137] Y.M. Li, CH.W. Chen, A synthetical approach for blog recommendation:
combining trust, social relation, and semantic análisis, Expert Systems with
Applications 36 (3) (2009) 6536–6547.
[138] Y.M. Li, CH.P. Kao, TREPPS: a trust-based recommender system for peer
production services, Expert Systems with Applications 36 (2) (2009) 3263–
3277.
[139] Y.M. Li, T.F. Liao, CH.Y. Lai, A social recommender mechanism for improving
knowledge
sharing
in
online
forums,
Information
Processing
and
Management, in press, doi: 10.106/j.ipm.2011.10.004.
[140] S. Loh, F. Lorenzi, R. Granada, D. Lichtnow, L.K. Wives, J.P. Oliveira, Identifying
similar users by their scientiﬁc publications to reduce cold start in
recommender systems, in: Proceedings of the 5th International Conference
on Web Information Systems and Technologies (WEBIST2009), 2009, pp.
593–600.
[141] L. Lü, M. Medo, Ch.H. Yeung, Y.Ch. Zhang, Z.K. Zhang, T. Zhou, Recommender
systems, Physics Reports 519 (2012) 1–49.
[142] X. Luo, Y. Xia, Q. Zhu, Incremental collaborative ﬁltering recommender based
on regularizad matrix factorization, Knowledge-Based Systems 27 (2012)
271–280.
[143] X. Luo, Y. Xia, Q. Zhu, Applying the learning rate adaptation to the matrix
factorization based collaborative ﬁltering, Knowledge Based Systems 37
(2013) 154–164.
[144] H. Ma, I. King, M.R. Lyu, Learning to recommend with explicit and implicit
social relations, ACM Transactions on Intelligent Systems and Technology 2
(3) (2011). Article 29.
[145] H. Ma, T. CH. Zhou, M.R. Lyu, I. King, Improving recommender systems by
incorporating
social
contextual
information,
ACM
Transactions
on
Information Systems 29 (2) (2011). Article 9.
[146] A.
Machanavajjhala,
A.
Korolova,
A.D.
Sharma,
Personalized
social
recommendations:
accurate
or
private,
in:
Proceedings
of
the
VLDB
Endowment, vol. 4, issue 7, 2011, pp. 440–450.
[147] L.B. Marinho, L. Schmidt-Thieme, Collaborative tag recommendations, in:
Proceedings of the 31st Annual Conference of the German Classiﬁcation
Society, 2008, pp. 533–540.
[148] L. Martinez, L.G. Perez, M.J. Barranco, Incomplete preference relations to
smooth out the cold-start in collaborative recommender systems, in:
Proceedings of the 28th North American Fuzzy Information Processing
Society Annual Conference (NAFIPS2009), 2009a, pp. 1–6.
[149] L. Martinez, R.M. Rodriguez, M. Espinilla, REJA: a georeferenced hybrid
recommender system for restaurants, in: IEEE/WIC/ACM International Joint
Conference on Web Intelligence and Intelligent Agent Technonolgy (WI-IAT
3), 2009b, pp. 187–190.
[150] P. Massa, P. Avesani, Trust-aware collaborative ﬁltering for recommender
systems, Lecture Notes in Computer Science 3290 (2004) 492–508.
[151] P. Massa, P. Avesani, Trust-aware recommender Systems, in: Proceedings of
the 2007 ACM conference on Recommender Systems, 2007, pp. 17–24.
[152] C. Matyas, C. Schlieder, A spatial user similarity measure for geographic
recommender systems, in: Proceedings of the 3rd International Conference
on GeoSpatial Semantics, 2009, pp. 122–139.
[153] K. Mccarthy, J. Reilly, L. Mcginty, B. Smyth, Thinking positively-explanatory
feedback for conversational recommender systems, in: European Conference
on Case-based reasoning (ECCBR), 2004, pp. 115–124.
[154] K. Mcnally, M.P. O’mahony, M. Coyle, P. Briggs, B. Smyth, A case study of
collaboration and reputation in social web search, ACM Transactions on
Intelligent Systems and Technology 3 (1) (2011). Article 4.
[155] D. Mcsherry, Explanation in recommender systems, Artiﬁcial Intelligence
Review 24 (2) (2005) 179–197.
[156] F. Mcsherry, I. Mironov,
Differentially Private recommender
systems:
building privacy into the netﬂix prize contenders, in: Proceedings of the
15th ACM SIGKDD International Conference on Knowledge Discovery and
Data Mining (KDD), 2009, pp. 627–636.
[157] P. Melville, R.J. Mooney, R. Nagarajan, Content-boosted collaborative ﬁltering
for
improved
recommendations,
in:
Proceeding
Eighteenth
National
Conference on Artiﬁcial Intelligence, 2002, pp. 187–192.
[158] R. Meteren, M. Someren, Using content-based ﬁltering for recommendation,
in: Proceedings of ECML 2000 Workshop: Maching Learning in Information
Age, 2000, pp. 47–56.
[159] S.E. Middleton, N.R. Shadbolt, D.C. De Roure, Ontological user proﬁling in
recommender systems, ACM Transactions on Information Systems (TOIS) 22
(1) (2004) 54–88.
[160] R.J. Mooney, L. Roy, Content-based book recommending using learning for
text categorization, in: Proceedings of the Fifth ACM Conference on Digital
Libraries, 2000, pp. 195–204.
[161] T. Morrison, U. Aickelin, An artiﬁcial immune system as a recommender for
Web sites, in: International Conference on Artiﬁcial Immune Systems, 2002,
pp. 161–169.
[162] A. Nanolopoulus, D. Rafailidis, P. Symeonidis, Y. Manolopoulus, Music Box:
personalizad music recommendation based on cubic analysis of social tags,
IEEE Transactions on Audio, Speech and Language Processing 18 (2) (2010)
407–412.
[163] K. Nehring, C. Puppe, A theory of diversity, Econometrica 70 (3) (2002) 1155–
1198.
[164] E.R. Núñez-Valdéz, J.M. Cueva-Lovelle, O. Sanjuán-Martı´nez, V. Garcı´ a-Dı´az, P.
Ordoñez,
C.E.
Montenegro-Marı´ n,
Implicit
feedback
techniques
on
recommender systems applied to electronic books, Computers in Human
Behavior 28 (4) (2012) 1186–1193.
130
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132

[165] J. O’donovan, Capturing trust in social web applications, in: J. Golbeck (Ed.),
Computing with Social Trust, 2009, pp. 213–257.
[166] J. O’donovan, B. Smyth, Trust in recommender systems, in: International
Conference on Intelligent user Interfaces, 2005, pp. 167–174.
[167] K. Oku, R. Kotera, K. Sumiya, Geographical recommender system based on
interaction between map operation and category selection, in: Workshop on
Information Heterogeneity and Fusion in Recommender Systems, 2010, pp.
71–74.
[168] J. Ortega, J. Bobadilla, A. Hernando, A. Gutiérrez, Incorporating group
recommendations to recommender systems: alternatives and performance,
Information Processing and Management (2013), http://dx.doi.org/10.1016/
j.ipm.2013.02.003.
[169] J. Ortega, J.L. Sánchez, J. Bobadilla, A. Gutiérrez, Improving collaborative
ﬁltering-based recommender systems results using Pareto dominance,
Information Sciences (2013), http://dx.doi.org/10.1016/j.ins.2013.03.011.
[170] A. Papadimitriou, P. Symeonidid, Y. Manolopoulus, A generalized taxonomy
of explanations styles for traditional and social recommender systems, Data
Minning Knowledge Discovery 24 (3) (2012) 555–583.
[171] D.H. Park, H.K. Kim, I.Y. Choi, J.K. Kim, A literature review and classiﬁcation of
recommender Systems research, Expert Systems with Applications 39 (2012)
10059–10072.
[172] S.T.
Park,
W.
Chu,
Pairwise
preference
regression
for
cold-start
recommendation,
in:
Proceedings
of
the
2009
ACM
Conference
on
Recommender Systems, 2009, pp. 21–28.
[173] S.T. Park, D.M. Pennock, O. Madani, N. Good, D. Coste, Naı¨ve ﬁlterbots for
robust cold-start recommendations, in: Proceedings of Knowledge Discovery
and Data Mining (KDD2006), 2006, pp. 699–705.
[174] Y.J. Park, A. Tuzhilin, The long tail of recommender systems and how to
leverage it, in: Proceedings of the 2008 ACM Conference on Recommender
Systems, 2008, pp. 11–18.
[175] M. Pazzani, D. Billsus, Learning and revising user proﬁles: the identiﬁcation of
interesting web sites, Machine Learning 27 (3) (1997) 313–331.
[176] M.J.
Pazzani,
D.
Billsus,
Content-based
recommender
systems,
in:
P.
Brusilovsky, A. Kobsa, W. Nejdl (Eds.), The Adaptive Web, 2007, pp. 291–
324 (Chapter 10).
[177] M. Pazzani, A framework for collaborative, content-based, and demographic
ﬁltering, Artiﬁcial Intelligence Review-Special Issue on Data Mining on the
Internet 13 (5-6) (1999) 393–408.
[178] S. Perugini, M.A. Gonçalves, E.A. Fox, Recomender systems research: a
connection-centric Surrey, Journal of Intelligen Information Systems 23 (2)
(2004) 107–143.
[179] M.C. Pham, Y. Cao, R. Klamma, M. Jarke, A clustering approach for
collaborative
ﬁltering
recommendation
using
social
network
analysis,
Journal of Universal Computer Science 17 (4) (2011) 583–604.
[180] G. Pitsilis, S.J. Knapskog, Socila trust as a solution to address sparsity-inherent
problems of recommender Systems, in: Proceedings of the 2009 ACM
Conference on Recommender Systems, 2009, pp. 33–40.
[181] G. Pitsilis, X. Zhang, W. Wang, Clustering recommenders in collaborative
ﬁltering using explicit trust information, Advances in Information and
Communication Technology 358 (2011) 82–97.
[182] A. Popescul, L.H. Ungar, D.M. Pennock, S. Lawrence, Probabilistic models for
uniﬁed collaborative and content-based recommendation in sparse-data
environments, in: Proceeding UAI ’01 Proceedings of the 17th Conference in
Uncertainty in Artiﬁcial Intelligence, 2001, pp. 437–444.
[183] C. Porcel, E. Herrera-Viedma, Dealing with incomplete information in a fuzzy
linguistic recommender system to disseminate information in university
digital libraries, Knowledge-Based Systems 23 (1) (2010) 32–39.
[184] C. Porcel, J.M. Moreno, E. Herrera-Viedma, A multi-disciplinar recommender
system to advice research resources in university digital libraries, Expert
Systems with Applications 36 (10) (2009) 12520–12528.
[185] C. Porcel, A. Tejeda-Lorente, M.A. Martı´ nez, E. Herrera-Viedma, A hybrid
recommender system for the selective dissemination of research resources in
a technology transfer ofﬁce, Information Sciences 184 (1) (2012) 1–19.
[186] J. Preece, B. Shneiderman, The reader to leader framework: motivating
technology-mediated social participation, AIS Transactions on Human–
Computer Interaction 1 (1) (2009) 13–32.
[187] P. Pu, L. Chen, Trust-inspiring explanation interfaces for recommender
systems, Knowledge Based Systems 20 (2007) 542–556.
[188] W. Qin, L. Xin, H. Liang, Unifying user-based and item-based algorithm to
improve collaborative ﬁltering accuracy, Energy Procedia 13 (2011) 8231–
8239.
[189] L. Ramaswamy, P. Deepak, R. Polavarapu, K. Gunasekera, D. Garg, K.
Visweswariah,
S.
Kalyanaraman,
CAESAR:
a
context-aware,
social
recommender
system
for
low-end
mobile
devices,
in:
International
Conference
on
Mobile
Data
Management:
Systems,
Services
and
Middleware, 2009, pp. 338–347.
[190] A.M. Rashid, G. Karypis, J. Riedl, Learning preferences of new users in
recommender systems: an information theoretic approach, in: ACM SIGKDD
Explorations Newsletter, vol. 10, issue 2, 2008, pp. 90–100.
[191] S.
Ray,
A.
Mahanti,
Strategies
for
effective
shilling
attacks
against
recommender systems, Lecture Notes in Computer Science 5456 (2009)
111–125.
[192] L. Ren, L. HE, J. Gu, W. Xia, F. Wu, A hybrid recommender approach based on
Widrow–Hoff learning, in: International Conference on Future Generation
Communication and Networking, 2008, pp. 40–45.
[193] T.H. Roh, K.J. Oh, I. Han, The collaborative ﬁltering recommendation based on
SOM cluster-indexing CBR, Expert Systems with Applications 25 (2003) 413–
423.
[194] J.A. Rodrigues, L.F. Cardoso, J. Moreira, G. Xexeo, Bringing knowledge into
recommender systems, The Journal of Systems and Software, in press, http://
dx.doi.org/10.1016/j.jss.2012.10.002.
[195] S.B. Roy, S. Amer-Yahia, A. Chala, G. Das, C. Yu, Space efﬁciency in group
recommendation, The International Journal on Very Large Data Bases 19 (6)
(2010) 877–900.
[196] G. Ruffo, R. Schifanella, A peer-to-peer recommender system base don
spontaneous afﬁnities, ACM Transactions on Internet Technology 9 (1) (2009)
1–34.
[197] P.B. Ryan, D. Bridge, Collaborative recommending using formal concept
analysis, Knowledge Based Systems 19 (5) (2006) 309–315.
[198] G. Salton, Automatic Text Processing: The Transformation, Analysis, and
Retrieval of Information by Computer, Addison-Wesley, Reading, MA, 1989.
[199] M. Saranya, T. Atsuhiro, Hybrid recommender systems using latent features,
in: Proceedings of the International Conference on Advanced Information
Networking and Applications Workshops, 2009, pp. 661–666.
[200] B. Sarwar, G. Karypis, J.A. Konstan, J. Riedl, Item-based collaborative ﬁltering
recommendation algorithms, in: 10th International Conference on World
Wide Web, 2001, pp. 285–295.
[201] B. Sarwar, G. Karypis, J. Konstan, J. Riedl, Analysis of recommendation
algorithms for e-commerce, in: ACM Conference on Electronic Commerce,
2000a, pp. 158–167.
[202] B. Sarwar, G. Karypis, J. Konstan, J. Riedl, Application of dimensionality
reduction in recommender system – a case study, in: ACM WebKDD
Workshop, 2000b, pp. 264–272.
[203] J.B. Schafer, D. Frankowski, J. Herlocker, S. Sen, Collaborative ﬁlltering
recommender systems, in: P. Brusilovsky, A. Kobsa, W. Nejdl (Eds.), The
Adaptive Web, 2007, pp. 291–324 (Chapter 9).
[204] A.I. Schein, A. Popescul, L.H. Ungar, D.M. Pennock, Methods and metrics for
cold-start recommendations, in: Proceeding SIGIR ’02 Proceedings of the 25th
Annual International ACM SIGIR Conference on Research and Development in
Information Retrieval, 2002, pp. 253–260.
[205] C.
Schlieder,
Modeling
collaborative
semantics
with
a
geographic
recommender,
in:
Workshop
on
Semantic
and
Conceptual
Issues
in
Geographic Information Systems, 2007, pp. 336–347.
[206] J. Serrano-Guerrero, E. Herrera-Viedma, J.A. Olivas, A. Cerezo, F.P. Romero, A
google wave-based fuzzy recommender system to disseminate information
in University Digital Libraries 2.0., Information Sciences 181 (9) (2011) 1503–
1516.
[207] Z. Severac, V. Devedzic, J. Jovanovic, Adaptive neuro-fuzzy pedagogical
recommender, Expert Systems with Applications 39 (10) (2012) 9797–9806.
[208] A.
Shepitsen,
J.
Gemmell,
B.
Mobasher,
R.
Burke,
Personalized
recommendation in social tagging systems using hierarchical clustering, in:
Proceedings of the 2008 ACM Conference on Recommender Systems, 2008,
pp. 259–266.
[209] S.K. Shinde, U. Kulkami, Hybrid personalizad recommender system using
centering–bunching
based
clustering
algorithm,
Expert
Systems
with
Applications 39 (1) (2012) 1381–1387.
[210] S.
Siersdorfer,
S.
Sergei,
Social
recommender
systems
for
web
2.0
folksonomies, in: 20th ACM conference on Hypertext and hipermedia,
2009, pp. 261–269.
[211] I. Soboroff, C. Nicholas, Combining content and collaboration in text ﬁltering,
in:
Proceedings
of
the
IJCAI’99
Workshop
on
Machine
Learning
for
Information Filtering, 1999, pp. 86–91.
[212] X. Su, T.M. Khoshgoftaar, A survey of collaborative ﬁltering techniques,
Advance in Artiﬁcial Intelligence 2009 (2009) 1–19.
[213] P. Symeonidis, A. Nanopoulus, Y. Manolopoulus, Providing justiﬁcations in
recommender systems, IEEE Transactions on Systems, Man and Cybernet 38
(6) (2008) 1262–1272.
[214] P.
Symeonidis,
A.
Nanopoulus,
Y.
Manolopoulus,
MovieExplain:
a
recommender system with explanations, in: Proceedings of the 2009 ACM
Conference on Recommender Systems, 2009, pp. 317–320.
[215] G. Takács, I. Pilászy, B. Németh, D. Tikk, Scalable collaborative ﬁltering
approaches for large recommender systems, Journal of Machine Learning
Research 10 (2009) 623–656.
[216] S. Tan, J. Bu, CH. Chen, X. He, Using rich social media information for music
recommendation via hypergraph model, ACM Transactions on Multimedia
Computing, Communications, and Applications 7 (1) (2011). Article 7.
[217] N. Tintarev, J. Masthoff, A survey of explanations in recommender systems,
in: IEEE 23rd International Conference on Data Engineering Workshop, 2007,
801–810.
[218] T. Tran, R. Cohen, Hybrid recommender systems for electronic commerce, in:
Proceedings of the 17th National Conference on Artiﬁcial Intelligence, AAAI,
2000, pp. 78–84.
[219] K.H.L. Tso-Sutter, L.B. Marinho, L. Schmidt-Thieme, Tag-aware recommender
systems by fusion of collaborative ﬁltering algorithms, in: Proceedings of the
2008 ACM Symposium on Applied Computing, 2008, pp. 1995–1999.
[220] S. Vargas, P. Castells, Rank and relevance in novelty and diversity metrics for
recommender systems, in: Proceedings of the 2011 ACM Conference on
Recommender Systems, 2011, pp. 109–116.
[221] P. Victor, CH. Cornelis, M. De-Cock, Trust Networks for Recommender
Systems, Antalis Press, 2011.
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
131

[222] J. Vig, S. Sen, J. Riedle, Tagsplanations: Explaining recommendations using
tags, Proceedings of the 13th international conference on Intelligent user
interfaces, 2009, pp. 47-56.
[223] P. Victor, CH. Cornelis, M. De-Cock, P.P. DA-SILVA, Gradual tust and distrust in
recommender systems, Fuzzy Sets and Systems 160 (10) (2009) 1367–1382.
[224] M.G. Vozalis, K.G. Margaritis, Using SVD and demographic data for the
enhancement of generalized collaborative ﬁltering, Information Sciences 177
(2007) 3017–3037.
[225] Y. Wan-Shiou, CH. Hung-Chi, D. Jia-Ben, A location-aware recommender
system for mobile shopping environments, Expert Systems with Applications
34 (1) (2008) 437–445.
[226] J.
Wang, A. Vries, M. Reinders,
Unifying user-based
and item-based
collaborative ﬁltering approaches by similarity fusion, in: Proc. SIGIR Conf.,
2006, pp. 501–508.
[227] J. Wang, A.P. Vries, M.J. Reinders, Uniﬁed relevance models for rating
prediction in collaborative ﬁltering, ACM Transactions in Information
Systems 26 (3) (2008) 1–42.
[228] L.T. Weng, Y. Xu, Y. Li, R. Nayak, Exploiting item taxonomy for solving cold-
start problem in recommendation making, in: Proceedings of the 20th IEEE
International Conference on Tools with Artiﬁcial Intelligence (ICTAI2008),
2008, pp. 113–120.
[229] B. Widrow, M.E. Hoff, Adaptive switching circuits, in: Convention Record, IRE
WESCON, 1960, pp. 96–104.
[230] P. Winoto, T.Y. Tang, The role of user mood in movie recommendations,
Expert Systems with Applications 37 (8) (2010) 6086–6092.
[231] W. Woerndl, G. Groh, Utilizing physical and social context to improve
recommender systems, in: IEEE/WIC/ACM International Conferences on Web
Intelligence and Intelligent Agent Technology, 2007, pp. 123–128.
[232] B. Xie, P. Han, F. Yang, R.M. Shen, H.J. Zeng, Z. Chen, DCFLA: a distributed
collaborative-ﬁltering neighbor-locating algorithm, Information Sciences 177
(6) (2007) 1349–1363.
[233] W. Xin, Q. Jamaliding, T. Okamoto, Discovering social network to improve
recommender
system
for
group
learning
support,
in:
International
Conference on Computational Intelligence and Software Engineering, 2009,
pp. 1–4.
[234] R.R. Yager, Fuzzy logic methods in recommender systems, Fuzzy Sets and
Systems 136 (2) (2003) 133–149.
[235] W.S. Yang, H.CH. Cheng, J.B. Dia, A location-aware recommender system for
mobile shopping environments, Expert Systems With Applications 34 (1)
(2008) 437–445.
[236] Y. Yang, An evaluation of statistical approaches to text categorization,
Information Retrieval 1 (1) (1999) 67–88.
[237] Z. Yao, Q. Zhang, Item-based clustering collaborative ﬁltering algorithm
under high dimensional sparse data, in: International Joint Confeence on
Computational Sciences and Optimization, 2009, pp. 787–790.
[238] Z. Yu, X. Zhou, Y. Hao, J. Gu, TV program recommendation for multiple
viewers based on user proﬁle merging, User Modeling and User-Adapted
Interaction 16 (1) (2006) 63–82.
[239] W. Yuan, D.
Guan, Y.K.
Lee,
S.
Lee, S.J.
Hur, Improved
trust-aware
recommender system using small-worldness of trust networks, Knowledge
Based Systems 23 (3) (2010) 232–238.
[240] G. Zacharia, A. Moukas, P. Maes, Collaborative reputation mechanisms for
electronic marketplaces, Decision Support Systems 29 (2000) 371–388.
[241] O. Zaiane, Building a recommender agent for e-learning systems, in:
Proceedings of the International Conference on Computers Education
(ICCE’02), vol. 1, 2002, pp. 55–59.
[242] J.
Zhan,
Privacy-preserving
collaborative
recommender
systems,
IEEE
Transactions on Systems, Man and Cybernetics 40 (4) (2010) 472–476.
[243] F. Zhang, H.Y. Chang, A collaborative ﬁltering algorithm employing genetic
clustering
to
ameliorate
the
scalability
issue,
in:
IEEE
International
Conference on e-Business Engineering, 2006, pp. 331–338.
[244] S. Zhang, W. Wang, J. Ford, F. Makedon, Using singular value decomposition
approximation for collaborative ﬁltering, in: IEEE International Conference
on E-Commerce Technology, 2005, pp. 1–8.
[245] L. Zhen, G.Q. Huang, Z. Jiang, Recommender systems based on workﬂow,
Decision Support Systems 48 (2009) 237–245.
[246] L. Zhen, G.Q. Huang, Z. Jiang, Collaborative ﬁltering based on workﬂow space,
Expert Systems with Applications 36 (2009) 7873–7881.
[247] L. Zhen, Z. Jiang, H. Song, Distributed recommender for peer-to-peer
knowledge sharing, Information Sciences 210 (2010) 3546–3561.
[248] N. Zheng, Q. Li, A recommender system based on tag and time information for
social tagging systems, Expert Systems with Applications 38 (4) (2011)
4575–4587.
[249] Y. Zheng, X. Xie, Learning travel recommendations from user-generated GPS
traces, ACM Transactions on Intelligent Systems and Technology 2 (2011) 1.
Article 2.
[250] Y. Zheng, L. Zhang, Z. Ma, X. Xie, W.Y. Ma, Recommending friends and
locations based on individual location history, ACM Transactions on the Web
5 (2011) 1. Article 5.
[251] J. Zhong, X. Li, Uniﬁed collaborative ﬁltering model based on combination of
latent features, Expert Systems with Applications 37 (2010) 5666–5672.
[252] R.L. Zhu, S.J. Gong, Analyzing of collaborative ﬁltering using clustering
technology, international colloquium on computing, in: ISECS International
Colloquium on Computing, Communication, Control, and Management, 2009,
pp. 57–59.
[253] C.N. Ziegler, S.M. Mcnee, J.A. Konstan, G. Lausen, Improving recommendation
lists through topic diversiﬁcation, in: Proceedings of the 14th International
Conference on World Wide Web, 2005, pp. 22–32.
132
J. Bobadilla et al. / Knowledge-Based Systems 46 (2013) 109–132
"
Creating synthetic datasets for collaborative filtering recommender systems using generative adversarial networks.pdf,"Knowledge-Based Systems 280 (2023) 111016
Available online 23 September 2023
0950-7051/© 2023 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Contents lists available at ScienceDirect
Knowledge-Based Systems
journal homepage: www.elsevier.com/locate/knosys
Creating synthetic datasets for collaborative filtering recommender systems
using generative adversarial networks
Jesús Bobadilla a, Abraham Gutiérrez a, Raciel Yera b, Luis Martínez b,∗
a Departamento de Sistemas Informáticos, ETSI Sistemas Informáticos, Universidad Politécnica de Madrid, C/ Alan Turing s/n, 28031, Madrid, Spain
b Departamento de Informática, Universidad of Jaén, Jaén, Spain
A R T I C L E
I N F O
Keywords:
Recommender systems
Generative adversarial networks
Deep learning
Collaborative filtering
A B S T R A C T
Research and education in machine learning requires diverse, representative, and open datasets that contain
sufficient samples to handle the necessary training, validation, and testing tasks. Currently, the Recommender
Systems area includes a large number of subfields in which accuracy and beyond-accuracy quality measures
are continuously being improved. To feed this research variety, it is both necessary and convenient to reinforce
the existing datasets with synthetic ones. This paper proposes a Generative Adversarial Network (GAN)-based
method to generate collaborative filtering datasets in a parameterized way by selecting their preferred number
of users, items, samples, and stochastic variability. This parameterization cannot be performed using regular
GANs. Our GAN model is fed with dense, short, and continuous embedding representations of items and users,
instead of sparse, large, and discrete vectors, to ensure fast and accurate learning, as compared to the traditional
approach based on large and sparse input vectors. The proposed architecture includes a DeepMF model to
extract the dense user and item embeddings and a clustering process to convert the dense GAN generated
samples to the discrete and sparse samples necessary to create each required synthetic dataset. The results
from three different source datasets show adequate distributions and expected quality values and evolutions
in the generated datasets compared to the source datasets. Synthetic datasets and source codes are available
to researchers.
1. Introduction
Recommender systems (RS) are a relevant area in artificial intel-
ligence due to the growing popularity of social networks. The big
companies that extensively use RSs are TripAdvisor, Netflix, Spotify,
YouTube Music, TikTok, YouTube and Amazon [1]. These companies
make use of the RS models to recommend to users similar items (music,
videos, trips, news) to those that they have already consumed; some
other companies, such as Facebook, work hard to collect customer
activity to provide personalized advertising rather than personalized
products or services. RSs are usually classified according to their filter-
ing approach [2]; content-based RSs select the recommended items by
looking for similar content [3]; since most item contents is text, natural
language processing models are used. Reviews [4] and tweets [5]
are two common types of content-based filtered data. Product images
can also be processed to make recommendations; convolutional neural
networks are the most commonly used models to perform this task [6].
Social filtering has been extensively used to improve social-based rec-
ommendations. This type of filtering uses data such as tags, followers,
∗Corresponding author.
E-mail addresses: jesus.bobadilla@upm.es (J. Bobadilla), abraham.gutierrez@upm.es (A. Gutiérrez), ryera@ujaen.es (R. Yera), martin@ujaen.es
(L. Martínez).
and being followed, and makes use of the concepts of reputation and
trust [7]. Geographic information, such as GPS coordinates and POI,
is mainly used to support context-aware filtering [8]. Demographic
filtering (age, gender, country, etc.) is commonly combined with other
types of filtering, implementing recommendation ensembles [9]. Be-
yond the previous filtering strategies, collaborative filtering (CF) [10] is the
most important approach for implementing RSs, since it provides superior
accuracy, particularly when combined with some other types of filtering.
Effective RS research makes use of innovative models, adequate quality
measures, and representative datasets.
The historical evolution of CF begins with the use of memory-based
models, mainly the K-Nearest Neighbors algorithm [11]. Memory-
based approaches were replaced by model-based machine learning
approaches due to their overall performance: they are superior in accu-
racy of results, also in time to obtain predictions (once the model has
learned); and their output is capable of being explained through post-
hoc techniques [12]. Matrix Factorization (MF) [13] is the most widely
used machine learning model to implement collaborative filtering; it
performs a dimensional reduction of users and items, capturing the
https://doi.org/10.1016/j.knosys.2023.111016
Received 17 December 2022; Received in revised form 12 September 2023; Accepted 15 September 2023

Knowledge-Based Systems 280 (2023) 111016
2
J. Bobadilla et al.
main patterns that relate them to the votes cast. Additionally, by using
Non-Negative Matrix Factorization (NFM) [14], semantic meanings can
be assigned to latent factors. Bayesian NMF [15] allows clustering
users and making predictions simultaneously, which opens the door to
effective recommendations to user groups and social clustering appli-
cations [16]. Nowadays, CF research is mainly developed by deep learning
models, where DeepMF [17] is the basis for modern approaches. DeepMF is
the model that we use in this paper, in which users are coded in a latent
space by means of an embedding layer, whereas items are coded in a
different latent space by means of a second embedding layer; finally,
predictions are made by making the dot product of both, item and user
embeddings. DeepMF improves MF due to the inherent competence
of neural networks to capture the non-linear relations hips between
samples. Neural Collaborative Filtering (NCF) [18] is extensively used
to implement CF; this model replaces the DeepMF dot layer with a
Multi-Layer Perceptron (MLP) and outperforms DeepMF when applied
to large and complex datasets. Beyond accuracy, deep learning models
are emerging to perform some innovative tasks, such as improving
fairness, where the DeepFair model [19] achieves a trade-off between
equity and precision; green computing [20]; results explanation via
latent space visualization [21] and efficient neighborhood identifica-
tion [22]. The adversarial network-based recommendation has recently
been introduced in the RS area [23] and we will focus on it in the
’Related work’ section. Generative Adversarial Networks (GAN) [24]
are responsible for the popular fake faces and fake videos that flood
social networks. Their architecture has two separate neural networks
that compete against each other (’adversarial’), such as an art forger
competing against an art expert, ensuring that both improve their work.
The GAN ‘forger’ is a generator model that creates fake samples from
random noise vectors, while the GAN ‘expert’ is a discriminator model
implemented as a simple binary classifier: fake, non-fake. However,
while RS research is mainly focused on proposing novel recommendation
models, this paper tries to make progress in CF datasets.
In this respect, it is essential to identify quality measures as a
key element to carry out adequate research, since they allow the
baselines of the state of the art to be compared with the proposed
algorithms, methods and models. Beyond the usual prediction and
recommendation quality measures (MAE, MSD, precision, recall, F1,
NDCG, etc.), some other measures, such as novelty and diversity [25],
have recently acquired growing importance. Of these, diversity is cur-
rently the main focus of researchers’ attention, due to the risks of
inappropriate recommendations in social networks, such as those that
exhibit a lack of variability and promote prefixed ideas and behaviors.
Diversity and reliability in RS have been improved by introducing
diversity-enhancing constraints in the MF model [26]; additionally,
a deep learning classification model [27] is proposed to obtain the
recommendation reliability values from the softmax output layer of the
neural network. Quality values are obtained when a model or method is
tested on balanced CF datasets. To obtain balanced training and testing
sets, with respect to their user and item distributions, deterministic
strategies are proposed in [28]. Most of the RS research makes use
of popular CF datasets such as MovieLens, FilmTrust, MyAnimeList or
CiteSeer; CF datasets include different domains such as music, movies,
POIs, tourism, news, research papers, tagged data, etc. Some of these
datasets have been filled with explicit votes from users, while others
contain implicit interactions between users and systems. There are also
datasets filled with crawled Web pages or academic PDFs [29] and
some others are enriched with social tags that researchers add to the ar-
ticles [30]. A selection of relevant social CF datasets is provided in [31]
and related to some articles using them. Recently, an educational news
dataset [32] was released; which included contextualized information:
time and location. Finally, an RS dataset has also been provided that
contains artificial intelligence research data [33] to obtain segmented
information, clustering, and geographical locations. Beyond these works,
it is particularly relevant that parameterized synthetic datasets have not
yet been used, so consequently the CF research does not benefit from the
flexibility that parameterization provides in the experiment design: different
dataset sizes, number of users and items, and so on. This paper aims to
fill the gap by proposing a procedure, coined as GANRS, which focuses
on the use of GANs to generate collaborative filtering recommender
systems datasets in a parameterized way. Please note that current RS
GAN-based models cannot simultaneously set the number of generated
users, items, and rating distributions.
Regarding our contribution, two main overall approaches can be
identified in the state-of-art: statistical and generative. The main ad-
vantage of the statistical approach is that several relevant parameters
can be simultaneously set: number of users, number of items, dataset
size, etc. The main drawback of this approach is its poor accuracy. On
the other hand, current model-based generative approaches improve
accuracy compared to statistical frameworks, but they lack flexibility,
since parameterization is very limited. In fact, current GAN designs
are focused on user profiles, and they can generate as many new fake
users as required, but other relevant parameters cannot be set, such
as the number of items that is fixed in the source set of user vectors
and then, also, in the fake generated set of user vectors. This can be
explained with an example: when we run a regular GAN to generate
fake images, the synthetic images have the same shape (resolution
and number of channels) as the source images. In the RS field, the
synthetic user vectors contain the same number of items as the real user
vectors. Following the example, there are some specific GAN designs
that return ‘super-resolution’ images (they can increase resolution), but
to our knowledge there are no RS GANs designed to generate fake
users containing more items (or fewer items). Our proposed method
is designed to simultaneously set some CF relevant parameters, such as
the number of users and items.
The rest of the paper has been structured as follows: related work
is introduced in Section 2, focusing on the most recent uses of the GAN
models applied to RS. Section 3 explains the proposed model and its
formalization. Section 4 presents the design, result, and discussion of
the experiments. Finally, Section 5 contains the main conclusions of the
article and discusses future work.
2. Background
2.1. Basics on generative adversarial networks
GANs are designed to generate data from scratch [24]. They have
been commonly used to create fake images, although their use has been
spreading to many other domains: music, medicine, financial data, etc.
The GAN architecture composes of two deep network models: generator
and discriminator. The generator model learns to create samples as
similar as possible to those in a dataset (e.g., a dataset with human faces
images), whereas the discriminator model learns to detect fake samples
(those samples created by the generator). To better understand the
GANs we can consider the example of a painting forger and a forgery
expert: the more imitations the forger paints, the better their results,
and the better the expert’s ability to detect fake paintings. Both people
successively improve their abilities. When the learning begins, the GAN
discriminator (the forgery expert, in our example) has an easy job,
since the generator does not have the painting patterns. After thousands
of learning epochs, the generator has learnt the patterns well enough
to confuse the discriminator, who is forced to tune their weights. If
the learning loop iterates enough times, both the generator and the
discriminator models are well designed and the painting in the dataset
contains suitable patterns, the generator will be able to create synthetic
(fake) samples that are difficult to distinguish from the originals.
Fig. 1 shows the GAN architecture [24]; the discriminator model
makes a binary classification between fake and real samples. The
generator model updates its weights (learns) when the discriminator
correctly classifies a fake sample. The discriminator model updates its
weights when it incorrectly classifies a sample. Note that the generator
takes a random noise distribution as input to generate samples; then,

Knowledge-Based Systems 280 (2023) 111016
3
J. Bobadilla et al.
Fig. 1. Generative Adversarial Networks architecture.
once it has learnt, for each input random noise vector that feeds the
generator a sample is created with the patterns of the dataset samples.
For this reason, by providing random noise vectors we can create as
many samples as required, which means that, in our context, we can
create fake CF datasets of any size by creating fake profiles.
To measure GAN loss, we use cross-entropy. The discriminator (𝐷)
loss can be expressed as the sum of the expectations:
𝑚𝑎𝑥𝐷𝑉(𝐷) = 𝐸𝑥∼𝑝𝑑𝑎𝑡𝑎(𝑥)[𝑙𝑜𝑔𝐷(𝑥)] + 𝐸𝑧∼𝑝𝑧(𝑧)[𝑙𝑜𝑔(1 −𝐷(𝐺(𝑧)))]
(1)
Where the first term of the equation is used to recognize real images,
and the second term recognizes generated images. 𝑍represents the
noisy vector, 𝐺is the generator, and 𝐺(𝑍) is the generated sample.
𝐷(𝐺(𝑧)) is the classification result of the discriminator when its input
is a fake sample. 𝐷(𝑥) is the classification result of the discriminator
when its input is a real sample.
The generator loss is designed to learn when the discriminator
correctly classifies (the true label is 1, and the fake label is 0). Its
equation is:
𝑚𝑖𝑛𝐺𝑉(𝐺) = 𝐸𝑥∼𝑝𝑑𝑎𝑡𝑎(𝑥)[𝑙𝑜𝑔(1 −𝐷(𝐺(𝑧)))]
(2)
The GAN is a minimax in which 𝐺wants to minimize 𝑉while 𝐷
wants to maximize it:
𝑚𝑖𝑛𝐺𝑚𝑎𝑥𝐷𝑉(𝐷, 𝐺) = 𝐸𝑥∼𝑝𝑑𝑎𝑡𝑎(𝑥)[𝑙𝑜𝑔𝐷(𝑥)] + 𝐸𝑧∼𝑝𝑧(𝑧)[𝑙𝑜𝑔(1 −𝐷(𝐺(𝑧)))] (3)
As in the previous example, GAN models act on non sparse values
(e.g., pixels in a picture), but they are not designed to work with
sparse vectors or matrices. Our problem here is that CF datasets contain
extraordinarily sparse matrices of ratings (users only vote or consume
a very limited number of the available items). Using the regular GAN
architecture is not an adequate approach to addressing CF-based RS.
This paper proposes an extended GAN architecture where embeddings
are introduced to code the sparse and discrete vectors of votes to
dense and continuous vectors. This innovation makes it possible to use
a regular GAN to generate dense and continuous vectors efficiently
and accurately. This compression stage forces us to design the corre-
sponding stage to decompress the generated dense vectors. The adopted
solution makes it possible to set both the number of users and items in
the generated dataset, which is a relevant innovation in the state-of-art.
2.2. Related works
Generative deep learning is an innovative field in the CF RS area.
Although some variational autoencoder approaches have been pub-
lished [34,35], current research is mainly focused on GAN models [36].
A CF subfield where GANs are used is the attack/defense strate-
gies [37], where these models can reinforce security in RS. Neverthe-
less, the most extended uses of CF GANs are: (a) to solve the issue of
noisy data, and (b) to tackle the data sparsity problem, and implement
a data augmentation framework by capturing the distribution of real
data. CFGAN [38] is a model that generates purchase vectors rather
than the IDs of items and then uses the generated fake purchase vectors
to augment the real vectors. The Wasserstein version of CFGAN is
the unified GAN (UGAN) [39] and reports improvements compared
to CFGAN. To prioritize long and short-term RS information (inter-
actions between users and items that change quickly or slowly), the
PLASTIC [40] model trains a generator and uses it as a reinforcement
learning agent. The recurrent GAN: RecGAN [41], learns temporal
patterns in ratings; combining GAN and recurrent neural networks
(RNNs) models. To capture negative sampling information in the CF
datasets, IPGAN [42] implements two different generative models:
one for positive instances and another for negative instances. IPGAN
considers the relations between the positive ratings sampled and the
negative ones selected.
Currently, the DCGAN model [43] combines GAN and reinforcement
learning models to catch the information of the RS sessions, rather than
the traditional historical matrices of votes from users to items. Session
information includes the responses of users to current recommenda-
tions. The user’s immediate feedback is managed by the reinforcement
learning model combined with the GAN. The NCGAN [44] incorporates
a neural network to extract nonlinear features from users, and a GAN
to guide the recommendation training; the generator model makes user
recommendations, whereas the discriminator model measures distances
between real and generated distributions. An innovative method to
improve the information flow from generator to discriminator [45]
reduces the discrepancies between both models in the CF GAN. A
regularization Wasserstein GAN model is used in [46], combined with
an autoencoder acting as a generator, reporting accuracy improvement
when applied to high-dimensional and sparse CF matrices. A CGAN
(Conditional GAN) is used [47] to improve CF recommendations, and
the sizes of the rating vectors can be set, simplifying the generator
and discriminator tasks. Additionally, it allows conditional rating gen-
eration to be established. For datasets that do not follow standard
Gaussian distributions, a missing data imputation based on GAN [48] is
proposed; results show improved quality in several representative clas-
sification data sets. Trust information is used in [49] to make effective
recommendations. They propose a GAN where the discriminator is an
MLP model, and the generator is a long-short term memory network
(LSTM) model [50]. Finally, CF datasets are usually imbalanced due to
their social data collection (e.g: more young people than old people).
To address this limitation, [51] proposes a Wasserstein GAN model in
the generator, and the PacGAN concept in the discriminator [52], to
minimize the mode collapse problem.
A platform for multi-agent RS simulation is the probabilistic-based
RecSim [53], which generates synthetic profiles of users and items,
and uses Markov chains and recurrent neural networks. The Virtual-
Taobao [54] is a multiagent reinforcement learning system designed
to improve search in the social Taobao website; it makes use of a

Knowledge-Based Systems 280 (2023) 111016
4
J. Bobadilla et al.
GAN to simulate internal distributions. A simple matrix factorization is
used [55] to inject topic diversification into the recommendation pro-
cess. The DataGenCars [56] is a Java-based generator of RS synthetic
data; it contains a statistical basement that provides flexibility, but it
returns low accuracy compared to deep learning generative models.
Finally, the SynEvaRec framework [57] provides the generation of
synthetic RS datasets using the Synthetic Data Vault (SVD) library.
This library models multivariate distributions using copula functions;
its CTGAN sub-library includes GAN models. The main advantage
of SynEvaRec, compared to previous frameworks, is that it can use
different RSs as a source; its main drawbacks are the poor quality of the
results in most of the cases, and the excessive time it takes to perform
the training stage.
Previous research mainly focuses on improving different objectives
such as noise reduction, recommendation quality, prediction values,
defense against attacks, or balancing data. To make this happen, many
different approaches and information sources have been combined:
the use of GAN, CGAN, Wasserstein GAN, etc. GAN models have
been combined with Recurrent Neural Networks [58] and LSTM net-
works [50], and reinforcement learning has been introduced in the
GAN-based architectures. Long and short data have been introduced
to the proposed models, in addition to trust information, session logs,
including responses of the users to previous recommendations, and
inferred negative votes. The pure generation of synthetic datasets does
not seem to be a goal in this novel field of GAN applied to CF RS, which
is currently focused on improving prediction and recommendation
quality results by means of data augmentation based on the inherent
ability of the GAN model to capture the complex nonlinear patterns
of high-dimensional and sparse CF datasets. The innovation of our
proposal is to generate representative and useful CF synthetic datasets,
rather than to improve the existing results that are of varying quality.
Additionally, it allows representative parameters to be set and a whole
‘family’ of synthetic datasets to be obtained, taking real datasets as a
source, such as Movielens, Netflix, or MyAnimeList. These parameters
are the number of users, the number of items, the number of samples
and the variability of the generated data. By varying the parameter
values, we can generate different versions of the same CF pattern, such
as a Movielens-based dataset containing 8000 users and 3000 items, or
another that contains 2000 users and 1000 items, among others. In this
way, we can test the accuracy and performance impact of the dataset
size, its sparsity, its number of users and items, as well as check the
improvement of the MAE when the number of users increases. As far
as we know, there are no published methods or models for creating, in
a parameterized deep learning model, accurate and scalable synthetic
datasets from diverse sources.
3. The generative adversarial networks-based approach for data-
sets building in collaborative filtering
As previously mentioned in the Introduction section, our research
problem is defined as obtaining a larger, scalable synthetic dataset
from an original RS dataset that synthesizes similar user behavior
and valuation patterns in relation to the original dataset. In addition,
it is desirable that such generation be parameterized, allowing the
number of users, items, samples and variability of the distribution to
be controlled.
Next, this section proposes the GANRS method, which uses a GAN
network to generate synthetic CF datasets; the GAN is fed with a real CF
dataset and the model learns its internal patterns. The most innovative
contribution is to feed the GAN with dense and small embedding
representations of users and items, instead of the traditional approach
where the GAN inputs are large and comprise sparse vectors containing
the votes cast for each user. The main advantage of the GANRS method
is that it greatly reduces the complexity of the GAN architecture, its
convergence speed, and its performance.
The traditional and sparse-based GAN architectures deal with very
large input vectors: as large as the number of items in the dataset,
which can be in the tens of thousands, and require a very large dense
layer in the model to hold this huge amount of data. What is more,
between 97% to 99% of the data is usually missing, since users only
vote for or consume a tiny proportion of the available products or
services, hence the extraordinary sparsity in the CF datasets. Following
the huge dense layer, in classical GAN architectures, it is necessary
to stack a large multilayer perceptron to reduce dimensionality. By
comparison, the proposed model replaces the large dense layer with
two embeddings, one to code users and the other to code items (bor-
rowed from the DeepMF model in the first stage of the proposed
method). Embedding layers are specifically designed to deal with sparse
data; they receive integer values (user and item IDs, in our case),
and they provide small embedding representations (typically 5 to 15
float values in the CF scenarios). Related users or items share similar
embedding representations, and this feature allows for extraordinarily
simplification of the model. Overall, the proposed architecture is much
smaller than traditional architectures, it contains far fewer parameters,
and consequently, learns faster. Additionally, it better captures the
complex nonlinear relations between items and users, in the same way
that non-GAN RS models do to improve predictions.
The formalization of the GANRS method is presented and structured
according to the following seven stages, also illustrated in Fig. 2:
• Stage 0. CF definitions
1 Let 𝑈be the set of users who make use of a CF RS.
2 Let 𝐼be the set of items available for voting in the CF RS.
3 Let 𝑉be the range of allowed votes; usually 𝑉= {1, 2, 3,
4, 5}.
4 Let 𝑆be the set of samples contained in the CF dataset; in
which 𝑁= |𝑆| = 𝑡ℎ𝑒𝑡𝑜𝑡𝑎𝑙𝑛𝑢𝑚𝑏𝑒𝑟𝑜𝑓𝑣𝑜𝑡𝑒𝑠𝑐𝑎𝑠𝑡.
5 𝑆
=
{⟨𝑢, 𝑖, 𝑣⟩1, ⟨𝑢, 𝑖, 𝑣⟩2, … ., ⟨𝑢, 𝑖, 𝑣⟩𝑁}; where each 𝑢
∈
{1, … , |𝑈|}, each 𝑖∈{1, … , |𝐼|}, and each 𝑣∈{1, … , |𝑉|}.
• Stage 1. DeepMF training
6 Let E be the size of two neural layer embeddings used to
vectorize each user and each item belonging to 𝑈and 𝐼,
respectively.
7 Let 𝑓𝑒𝑢(𝑢) = [𝑒𝑢
0, 𝑒𝑢
1, … , 𝑒𝑢
𝐸], where 𝑓𝑒𝑢is the embedding
layer output of the users, where 𝑢∈{1, … , |𝑈|}.
8 Let 𝑓𝑒𝑖(𝑖) = [𝑒𝑖
0, 𝑒𝑖
1, … , 𝑒𝑖
𝐸], where 𝑓𝑒𝑖is the embedding
layer output of the items, where 𝑖∈{1, … , |𝐼|}. By com-
bining both dense vectors of user and item embeddings:
([𝑒𝑢
0, 𝑒𝑢
1, … , 𝑒𝑢
𝐸] and [𝑒𝑖
0, 𝑒𝑖
1, … , 𝑒𝑖
𝐸]), we can make rating pre-
dictions in the DeepMF training stage. The dot product
of the user embedding and the item embedding in each
⟨𝑢, 𝑖, 𝑣⟩𝑗∈𝑆provides its rating prediction:
9 ̂𝑦𝑗= 𝑓𝑒𝑢(𝑢) ⋅𝑓𝑒𝑖(𝑖) = [𝑒𝑢
0, 𝑒𝑢
1, … , 𝑒𝑢
𝐸] ⋅[𝑒𝑖
0, 𝑒𝑖
1, … , 𝑒𝑖
𝐸]
10
1
2 (𝑦𝑗−̂𝑦𝑗)2 is the output error used in the DeepMF neural
network to start the backpropagation algorithm, where the
neural weights are iteratively improved from the 𝛿𝑗values:
▵𝑤𝑗𝑖= 𝛼𝑦𝑗𝑓′(𝑁𝑒𝑡𝑖) ∑
𝑘𝑤𝑖𝑘𝛿𝑘, when 𝑘is a hidden layer,
and ▵𝑤𝑗𝑖= 𝛼𝑦𝑖𝑓′(𝑁𝑒𝑡𝑖) 1
2(𝑦𝑘−̂𝑦𝑘)2, if 𝑘is the output
layer. i, j, and k are successive sequential layers. 𝑁𝑒𝑡𝑖
represents the cumulative input received for an artificial
neuron, 𝑁𝑒𝑡𝑖= ∑
𝑗𝑦𝑗∗𝑤𝑗, where 𝑗is the index of the
neurons in the layer preceding the current neuron.
• Stage 2. DeepFM feedforward
Once the DeepMF has learned, we can collect the embedding
representation of each user and each item in the CF RS.
11 Let 𝐸∗
=
{⟨𝑢, [𝑒𝑢
0, 𝑒𝑢
1, … , 𝑒𝑢
𝐸]⟩, ∀𝑢
∈
𝑈}, be the set of
embeddings for all the RS users. (𝑢∈[1...#𝑈], one to u)
12 Let 𝐸∗(𝑢) = [𝑒𝑢
0, 𝑒𝑢
1, … , 𝑒𝑢
𝐸]

Knowledge-Based Systems 280 (2023) 111016
5
J. Bobadilla et al.
Fig. 2. The stages of the proposed GANRS method.
13 Let 𝐸∗∗= {⟨𝑖, [𝑒𝑖
0, 𝑒𝑖
1, … , 𝑒𝑖
𝐸]⟩, ∀𝑖∈𝐼}, be the set of embed-
dings for all the RS items. (𝑖∈[1...#𝐼], one to i)
14 Let 𝐸∗∗(𝑖) = [𝑒𝑖
0, 𝑒𝑖
1, … , 𝑒𝑖
𝐸]
• Stage 3. Setting the dataset of embeddings
15 Let 𝑅= [⟨𝐸∗(𝑢), 𝐸∗∗(𝑖), 𝑣⟩], ∀⟨𝑢, 𝑖, 𝑣⟩𝑗∈𝑆be the embedding-
based dataset of real samples.
• Stage 4. GAN training
16 Let 𝑓𝐷be the discriminator D model belonging to a GAN
model.
17 Let 𝑓𝐺be the generator G model belonging to a GAN
model.
18 Let 𝑓𝐺𝐷be the optimization function of the GAN model;
𝑓𝐺𝐷= 𝑀𝑖𝑛𝐺𝑀𝑎𝑥𝐷𝑓(𝐷, 𝐺) = 𝐸𝑅[𝑙𝑜𝑔(𝐷(𝑅))] + 𝐸𝑧[𝑙𝑜𝑔(1 −
𝐷(𝐺(𝑧)))], where 𝐸𝑅is the expected value for real samples,
𝑧is the random noise that feeds the generator 𝐺, and 𝐸𝑧
is the expected value for the generated fake profiles 𝐺(𝑧).
Note that 𝑅refers to [15].
• Stage 5. GAN generation
19 Let 𝐹= 𝑓𝐺(𝑧) be the generated dataset of fake samples
from different random noise vectors 𝑧.
• Stage 6. Clustering of items and users.
20 Let 𝐾∗be the number of clusters used to group the embed-
dings of the users.
21 Let 𝐾∗∗be the number of clusters used to group the
embeddings of the items.
22 Let ℎ∗(𝑢) = 𝑐|𝑐∈{1, … , 𝐾∗}, be the clustering operation
that assigns a centroid to each user.
23 Let ℎ∗∗(𝑖) = 𝑐|𝑐∈{1, … , 𝐾∗∗}, be the clustering operation
that assigns a centroid to each item.
• Stage 7. Setting dataset of item IDs and user IDs
24 Let H be the item IDs and users IDs discrete dataset ob-
tained from the embedding-based dataset F of fake sam-
ples. 𝐻= {⟨ℎ∗(𝑢), ℎ∗∗(𝑖), 𝑣⟩|∀⟨𝐸∗(𝑢), 𝐸∗∗(𝑖), 𝑣⟩∈𝐹}
25 Let 𝑆= {𝐻} be the synthetic generated dataset version of
H where duplicated samples are removed.
26 Let 𝐺′ = {⟨ℎ∗(𝑢), ℎ∗∗(𝑖), 𝑣⟩∈𝐻|∄⟨ℎ∗(𝑢′), ℎ∗∗(𝑖′), 𝑣′⟩∈𝐻
where ℎ∗(𝑢) = ℎ∗(𝑢′) ∧ℎ∗(𝑖) = ℎ∗∗(𝑖) ∧𝑣≠𝑣′}
Fig. 2 shows the seven designed stages to generate different syn-
thetic datasets from real datasets (Movielens, Netflix, etc.). Stage 1 (top
left graph in Fig. 2) shows the training of a DeepMF model used to set
both the embedding layer of users and the embedding layer of items.
Basically, embedding layers in a neural network efficiently convert an
input from a sparse representation into an output dense representation.
For each input sample ⟨𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚, 𝑟𝑎𝑡𝑖𝑛𝑔⟩in the training set, the output
dot layer combines the embedding layer values to predict the rating
value and to obtain the output error ‘‘(rating - prediction)’’ that will
we backpropagated to update the learning parameters. Steps 6 to 10
formalize these concepts. Once the DeepMF model has learned, Stage
2 (top right graph in Fig. 2) shows the DeepMF feedforward process
where each item ID (from one to the number of items in the dataset,
range [1...#𝐼]) feeds the item embedding, which outputs the item ID
dense representation; usually, CF embedding vectors have a size from
5 to 10. The same applies to user IDs as input and their output dense
representations. Please note that the number of items in the dataset
will be different from the number of users. Steps 11 to 14 explain this
second stage.
The purpose of the third stage is to convert the source sparse CF
dataset into its dense representation. To accomplish the task, for each
source ⟨𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚, 𝑟𝑎𝑡𝑖𝑛𝑔⟩sample in the dataset (e.g.: ⟨8920, 345, 4⟩) we
replace the user ID (8920 in this example) with its related dense
representation; the same applies for the item ID. Using embeddings of
size 5, the result in the example could be such as:

Knowledge-Based Systems 280 (2023) 111016
6
J. Bobadilla et al.
Table 1
Example of samples representation.
Sparse
Dense
< 𝟖𝟗𝟎, 47, 5 >
< [𝟎.𝟎𝟑, 𝟎.𝟗𝟒, 𝟏.𝟎𝟐, 𝟎.𝟖𝟕, −𝟎.𝟕𝟖], [−1.23, 0.99, 1.02, 0.65, −0.48], 5 >
< 𝟖𝟗𝟎, 𝟑𝟏, 4 >
< [𝟎.𝟎𝟐, 𝟎.𝟗𝟓, 𝟎.𝟗𝟗, 𝟎.𝟖𝟏, −𝟎.𝟔𝟗], [𝟎.𝟒𝟓, −𝟎.𝟕𝟖, 𝟎.𝟖𝟑, −𝟎.𝟏𝟓, 𝟎.𝟎𝟗], 4 >
< 968, 𝟑𝟏, 4 >
< [−1.04, 0.04, 0.66, −0.67, 0.11], [𝟎.𝟒𝟐, −𝟎.𝟕𝟏, 𝟎.𝟖𝟎, −𝟎.𝟏𝟎, 𝟎.𝟏𝟒], 4 >
< 123, 𝟑𝟏, 2 >
< [1.56, −1.12, 0.33, 1.22, −0.87], [𝟎.𝟒𝟑, −𝟎.𝟕𝟓, 𝟎.𝟖𝟎, −𝟎.𝟏𝟏, 𝟎.𝟎𝟔], 2 >
⟨[0.03, 0.94, 1.02, 0.87, −0.78], [−1.23, 0.99, 1.02, 0.65, −0.48], 4⟩.
Stage 3 in Fig. 2 shows an illustrative example. Step 15 formalizes
the operation. The dense dataset obtained will be used in Stage 4 to
train a GAN capable of generating fake user and item profiles, as well
as their associated rating values, which will be, even at this stage,
the ratings that will be in the dataset generated at the end of the
proposal. Our GAN will use the Stage 3 dense dataset to train the
discriminator by providing it with the necessary real samples. The GAN
generator takes Gaussian random noise as input and iteratively learns
how to generate increasingly good fake profiles capable of cheating the
discriminator model. Once the generator and the discriminator have
learnt, the generator can convert input noise vectors into dense samples
that mimic the patterns of the real dataset provided in stage 3. Stage 4
is formalized in steps 16 to 18.
The last stage in Fig. 2 (bottom left graph) uses the trained GAN
generator model (Stage 4) to generate as many fake samples as desired.
We feed the generator with successive vectors of random noise values
following a Gaussian distribution, and the generator outputs successive
fake dense samples following the patterns of the real dataset (obtained
in Stage 3). The higher the standard deviation of the Gaussian distribu-
tion, the higher the variety of individual values in the generated dense
fake samples. As an example, a low standard deviation value in the ran-
dom noise Gaussian distribution leads to a higher proportion of votes ‘3’
(ranging from 1 to 5), while choosing a high standard deviation value
will produce a higher density of votes ‘5’ and ‘1’. Ratings are generated
in the same way as items and users: they are coded in the dense
embedding generated by the GAN. Synthetic ratings are continuous
values, whereas real ratings are discrete, usually in the range {1, … , 5}.
To make this conversion, a function assigns the maximum value in the
range (usually ‘5’) to the synthetic continuous values greater than it;
analogously the function assigns the minimum value (usually ‘1’) to the
continuous values lower than it. Finally, a round function is performed
to ensure discrete values. Step 19 formalizes the generation of fake
samples.
Although the GANRS method could be considered complete, this is
not the case because our goal is to generate fake datasets of sparse
samples (such as Movielens or Netflix); it is then necessary to convert
from the obtained dense representation in Stage 5 to the usual sparse
representation seen in Stage 1. The process is not straightforward, since
all the dense representations of the fake samples are different from
each other; this will be better explained using the example in Table 1:
it can be observed that user 890 (two first rows) has very similar
dense embedding values, but there are not identical, since the GAN
generator is not able to create the same exact values from the noise
input vectors. The same situation occurs in Table 1 for the item with ID
31. Consequently, the GANRS method provides a way to ‘group’ similar
dense embeddings into a unique ID; that is, to convert the dense bold
vectors of the user in Table 1 into a unique user ID (need not be 890),
and the dense bold vectors of the item into a unique item ID (need not
be 31, either).
To group similar dense embeddings into a unique ID, a K-Means
clustering [59] has been chosen. This algorithm has the relevant feature
that a number K of clusters must be chosen a priori, and it is very con-
venient in this context, since, in this way, we will have the opportunity
to establish the number of users and the number of items in the GANRS
synthetic generated dataset. Stage 6 of Fig. 2 shows this concept, where
Table 2
Main parameter values of the tested datasets.
Dataset
#users
#items
#ratings
Scores
Sparsity
Movielens 100K
943
1682
99,831
1 to 5
93.71
Netflix*
23,012
1,750
535,421
1 to 5
98.68
MyAnimeList
19,179
2,692
548,967
1 to 10
98.94
𝐾∗has been selected as number of users and 𝐾∗∗has been selected as
number of items. Two separate K-Means processes are run: one to group
user embeddings, and the other to group item embeddings. Steps 20 to
23 formalize these two clustering processes. To better understand this
stage, we can consider an example where one million fake samples have
been generated and we want to create a synthetic dataset containing
two thousand fake users and one thousand fake items. To accomplish
this task, we should obtain two thousand groups collected from the one
million user vectors (the same for the one thousand item groups). On
average, five hundred user vectors could be assigned to each user group
(and, analogously, one thousand item vectors to each item group), but
we know that this depends on the user and item vector patterns. To
adequately accomplish the grouping task, machine learning provides
us with clustering algorithms, of which the k-means allow us to set the
number of desired groups (two thousand for users and one thousand
for items, in our example). Running both clustering processes (one for
users and the other for items) we can assign a fake user ID to all the
fake user vectors in each cluster. Please note that the ID number can
be assigned at random to each of the two thousand clusters (the same
for the one thousand item IDs).
Fig. 3 illustrates the concept where graphs at the top show the two
k-means clustering processing performed in the proposed model: one
to group item vectors (yellow circles), and the other one to group user
vectors (orange circles). Gray ellipses represent the k-means clustering
groups. All the fake user vectors in each cluster collapse into the same
user vector, which codes a sample representative of its group, and is
different from the samples in the rest of the clusters (same for items).
In this way, we obtain the selected representative K* users and K**
items. The graphs at the bottom in Fig. 3 show the final stages of the
proposed method; Stage 6 draws the K* clusters of users and the K**
clusters of items, from the previous clustering with blue circles. Each
of the K* clusters (of users) groups a set of user vectors (columns of
orange squares), and each of the K** clusters (of items) groups a set of
item vectors (columns of yellow squares). Each cluster of user vectors
collapses into a representative user: at the bottom of Stage 6 graph (the
same for items). Once the representative users and items are set, we can
generate the fake dataset of embeddings by translating each generated
embedding sample (bottom-right graph) to its equivalent representative
concatenated embedding of representative (collapsed) users and items
(at the bottom of the Stage 6 graph). Previously, we illustrated a case
where a generated sample collapses its item vector in the ‘‘item 3’’
representative code (vector of red squares), and it collapses its user
vector in the ‘‘user 1’’ representative code (vector of brown squares).
In stage 7 the complete embedding, and the translation to the ⟨1, 3, 5⟩
sparse tuple codification can be seem. This is also true for the following
fake embedding, which collapses in the ‘‘item 1’’ (green) and ‘‘user
3’’ (blue), generating the sparse tuple < 3,2,4>. Note that the GAN-
generated profiles (bottom-right in Fig. 3) are not limited to a fixed
number of users and items, whereas their Stage 7 version (bottom-left in
Fig. 3) are limited to the ranges {1, … , 𝐾∗}, and {1, … , 𝐾∗∗}, making
it possible to preset the number of users and items of the synthetic
dataset.
The seventh stage in Fig. 2 converts dense fake samples (coming
from Stage 5) into sparse samples ⟨𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚, 𝑟𝑎𝑡𝑖𝑛𝑔⟩. To accomplish this
task, for each sample in the dense representation we replace its user
vector with its centroid number (from 1 to 𝐾∗) and its item vector
with its centroid number (from 1 to 𝐾∗∗); the rating value remains
the same as that already generated by the framework in Stages 4–5.

Knowledge-Based Systems 280 (2023) 111016
7
J. Bobadilla et al.
Fig. 3. Top graphs: clustering process to collapse user and item similar vectors into their representative user and item representations. Bottom graphs: translation from unlimited
fake limited profiles to profiles in the range ⟨{1 … 𝐾∗}, {1..𝐾∗∗}, 𝑟𝑎𝑡𝑖𝑛𝑔⟩.
Fig. 2 shows an example of this operation, formalized in Step 24. Please
note that repeated samples will appear in the previous discretization
process, since the GAN generator can create very similar dense samples
that will be converted to the same discrete encoding. There are several
factors that modulate the number of repeated samples, such as the
number of generated samples, the embedding size, the size of the noise
vector and the standard deviation of the Gaussian distribution, but the
most relevant factor is the number of chosen users or items (𝐾∗and
𝐾∗∗): the lower the 𝐾, the higher the number of repeated samples.
When the number of users or items is low, the average number of
samples grouped in each cluster is high. Step 25 formalizes the process
of removing repeated discrete samples. Finally, the GANRS method can
generate a small proportion of samples in which different votes are cast
from the same user to the same item; e.g.: ⟨879, 56, 4⟩, ⟨879, 56, 5⟩. This
could be considered as a convenient behavior: code a higher range of
votes (4.5 in the example) or express a change in the user’s opinion.
These cases can be unchanged, changed, or removed. Step 26 formalizes
their removal operation.
Overall, it is important to keep in mind that new rating values are
initially calculated in the context of Stage 4 of the proposal, where
GAN is used for generating the fake samples of pairs ⟨𝑢𝑠𝑒𝑟, 𝑖𝑡𝑒𝑚, 𝑟𝑎𝑡𝑖𝑛𝑔⟩,
using the user and item embeddings obtained in the previous stages as
a base. Afterward, our methodology refines the obtained data to assure
consistency (Stages 5–7).
Appendix A (Table 4) shows the main parameter and hyperparam-
eter values used to design both the models DeepMF and GAN involved
in the proposed GANRS method.
4. Experiments and results
Evaluating the quality of the generated datasets and comparing
them with state-of-art synthetic datasets is not straightforward, since
the traditional measures only cover distribution probabilities. This is
the case of the Kullback–Leibler (KL) divergence 𝐷𝐾𝐿(𝑃∥𝑄), where
𝑃and 𝑄are two probability distributions. In our context, we face
two main drawbacks to applying the KL divergence or any similar
divergence measure: 1) 𝑃and 𝑄are not distribution probabilities; they
are datasets, and (2) a low 𝐷𝐾𝐿value does not mean that 𝑄(the
generated dataset) is a good synthetic dataset obtained from 𝑃(the
source dataset). In fact, if 𝐷𝐾𝐿= 0, usually 𝑃= 𝑄, which is not a
useful result. Of course, each CF dataset contains a reduced number
of representative distribution probabilities, including: rating, user, and
item distributions (𝑄𝑢, 𝑄𝑖, 𝑄𝑟); but comparing each distribution of the
generated dataset with the corresponding distribution of the source
dataset has the same intrinsic problem as explained above: 𝐷𝐾𝐿(𝑃𝑢∥
𝑄𝑢) = 0, 𝐷𝐾𝐿(𝑃𝑖∥𝑄𝑖) = 0, 𝐷𝐾𝐿(𝑃𝑟∥𝑄𝑟) = 0, does not mean
that 𝑄is a suitable synthetic dataset with regard to 𝑃, indeed 𝑄must
have a certain degree of variability regard to 𝑃. A common alternative
approach that is used to deal with these situations is testing the quality
results in the specific domain; in our case MAE, precision, recall,
etc. Results should be interpreted according to graph trends rather
than absolute values, since better results just mean that 𝑄patterns
are less complicated than 𝑃ones, and worse results tell us that 𝑄
patterns are more complicated than 𝑃ones. Which scenario is better?
It depends on the objectives of the scientist that generates the synthetic
datasets. Addressing the concerns explained, we provide a complete set
of comparative graphs between the source (𝑃) and generated datasets
(𝑄), including probability distributions of the user, item, rating, and
precision and recall trends. Designing specific quality measures that
maximize each scientist’s objectives (required distribution variability,
required complexity in the resulting patterns, etc.) is challenging re-
search, and would help compare state-of-art generative approaches, but
this is out of the scope of this paper.
In this paper we evaluate the suitability of the presented procedure
focused on building synthetic datasets. First, the traditional data sets to
be used as a starting point for the present procedure are presented, as
well as a description of the experiments to be performed. Subsequently,
the obtained results are presented and discussed.
4.1. Experiments
To test the behavior of the proposed GANRS method, we will use
three representative and open datasets in the CF field: Movielens [60],
Netflix and MyAnimeList. We have chosen the 100K version of Movie-
lens and a reduced version of the complete Netflix dataset: Netflix*,
available in [61]. Table 2 shows the main parameter values for these
datasets. A complete set of experiments has been run using Netflix*,
whereas only a subset of these experiments is shown for Movielens
and MyAnimeList, to reduce the size of the paper. Results from the
Movielens and MyAnimeList tests are summarized at the end of this
section. Each of the three source datasets is used to generate its corre-
sponding synthetic version: setting different numbers of users, items,
and samples, and changing the standard deviation of the Gaussian
random noise.

Knowledge-Based Systems 280 (2023) 111016
8
J. Bobadilla et al.
Table 3
Parameter values of the synthetic datasets generated by GAN.
Source: Netflix*.
#
std
#users
#items
#
std
#users
#items
#
std
#users
#items
#samples
1
2.0
100
4000
6
2.5
100
4000
11
3.0
100
4000
1.5M
2
2.0
1000
4000
7
2.5
1000
4000
12
3.0
1000
4000
3
2.0
2000
4000
8
2.5
2000
4000
13
3.0
2000
4000
4
2.0
4000
4000
9
2.5
4000
4000
14
3.0
4000
4000
5
2.0
8000
4000
10
2.5
8000
4000
15
3.0
8000
4000
16
1.5
4000
2000
17
1.5
4000
8000
18
1.2
2000
4000
150K
19
1.2
2000
4000
500K
20
1.2
2000
4000
1M
21
1.2
2000
4000
3M
Experiments have been carried out using the neural DeepMF model.
Training, validation, and testing sets have been obtained for all the
real datasets (Netflix*, MyAnimeList, and Movielens 100K), and their
corresponding synthetic datasets. The source code to train the model
and test the results is the same for both the real and generated datasets;
ensuring the consistency of the graphs in the comparative figures
(Figs. 4a, 4b, 7b, 7e, 8b and 7e).
Table 3 shows the GAN generated synthetic datasets used to test
the proposed GANRS method, using Netflix* as source data. The ’#’
columns show the number of the generated datasets; ‘std’ is the stan-
dard deviation used in the random noise Gaussian distribution; #users
and #items are the total number of users and items chosen to generate
each dataset; #samples is the number of fake samples created by the
GAN generator. Please note that the final number of samples contained
in each of the datasets is lower than #samples, due to the removing
process of repeated samples. Cases 1 to 15 in Table 3 are used to test
the effect of changing standard deviation and number of users. Cases
16 and 17 test the consequences of increasing the number of items.
Finally, cases 18, 19 and 20 test the behavior of the synthetic datasets
when they have different sizes (number of samples). All generated
datasets and the source code of the proposed GANRS method are fully
available in http://suleiman.ujaen.es:8061/gitlab-instance-981c80cc/
ganrs. Additionally, Appendix B (Fig. 9) shows an example of the dis-
tribution graphs obtained for each of the synthetic datasets. Following
the link provided, each generated dataset is located in its specific
directory where a ’readme.txt’ file is provided along the synthetic
dataset distribution graphs.
Using the parameter values of Table 3, a variety of experiments have
been conducted. The classification of the experiments is as follows:
1. Number of users
(a) Distribution of users versus ratings
(b) Distribution of the user ratings
(c) Number of repeated samples
(d) Proportion of samples with the same user and item
(e) MAE and accuracy of the data set
(f) Users’ precision and recall
2. Number of items
(a) MAE and accuracy of the dataset
(b) Item’s precision and recall
3. Number of samples
(a) Number of samples generated
(b) Precision and Recall
These experiments refer to well-known metrics in collaborative
filtering.
Precision is focused on measuring the proportion of relevant rec-
ommendations (i.e. the user rated the item with a rating value equal or
greater than a threshold 𝜃) among the top 𝑁items recommended to the
user 𝑢, collected in the list 𝑇𝑁
𝑢
(Eq. (4)). On the other hand, Recall mea-
sures the proportion of correctly predicted relevant recommendations
among the total number of relevant votes of each user; therefore, recall
is sensitive to the existing proportions of relevant ratings (Eq. (5)).
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛=
1
#𝑈
∑
𝑢∈𝑈
|{𝑖∈𝑇𝑁
𝑢|𝑟𝑢𝑖≥𝜃}|
𝑁
(4)
𝑅𝑒𝑐𝑎𝑙𝑙=
1
#𝑈
∑
𝑢∈𝑈
|{𝑖∈𝑇𝑁
𝑢|𝑟𝑢𝑖≥𝜃}|
|{𝑖∈𝑇𝑁
𝑢|𝑟𝑢𝑖≥𝜃}| + |{𝑖∉𝑇𝑁
𝑢|𝑟𝑢𝑖≥𝜃}|
(5)
Where 𝑈is the set of training users, 𝑟𝑢𝑖is the rating of the training
user 𝑢for the item 𝑖, 𝑁is the number of recommendations, and 𝑇𝑁
𝑢
is
the set of 𝑁recommendations for the test user 𝑢: 𝑁highest predictions
of the user 𝑢above the relevancy threshold 𝜃.
Please note that Precision measures the proportion of recommenda-
tion hits (hits with respect to number of recommendations), whereas
Recall measures the proportion of recommendation hits with respect to
the total number of relevant items. Precision takes into consideration
the number of true positives, whereas Recall combines both the true
positives and the false negatives. The importance of the precision
and the recall quality measures largely depends on the scenario in
which they are applied, e.g. Recall seems to be crucial in medicine,
where a false negative is a serious mistake (i.e. not detecting cancer).
Nevertheless, Recall is less important in RS since missing a relevant film
(false negative) is not serious; the objective is to maximize a correctly
recommended film (true positives). The F1 quality measure combines
both Precision and Recall (Eq. (6)).
𝐹1 = 2 ∗𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛∗𝑅𝑒𝑐𝑎𝑙𝑙
𝑃𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛+ 𝑅𝑒𝑐𝑎𝑙𝑙
(6)
Finally, this paper also tests accuracy (Eq. (7)), where true negatives
are also considered. In this case both the positive and the negative hits
contribute to the results (to positively recommend relevant items and
to negatively recommend non relevant items).
𝐴𝑐𝑐𝑢𝑟𝑎𝑐𝑦= |{𝑖∈𝑆𝑡|𝑝𝑢𝑖≥𝜃∧𝑟𝑢𝑖≥𝜃}| + |{𝑖∈𝑆𝑡|𝑝𝑢𝑖< 𝜃∧𝑟𝑢𝑖< 𝜃}|
|𝑆𝑡|
(7)
Where 𝑆𝑡is the set of test samples, and each sample is the tuple
⟨𝑢, 𝑖, 𝑟⟩containing the user ID, item ID and rating of the user u for the
item 𝑖(𝑟𝑢𝑖). The model prediction of the rating is 𝑝𝑢𝑖.
Two values of the threshold 𝜃will be explored across the experimen-
tal scenario when precision, recall, F1, and accuracy quality measures
are tested
Note that the accuracy quality measure does not use the term
𝑇𝑁
𝑢
since the typical RS does not include negative recommendations.
Accordingly, this accuracy formulation does not average users’ results
and acts on the entire training data, such as we have done with the
Mean Absolute Error (Eq. (8)).
𝑀𝐴𝐸=
1
|𝑆𝑡|
∑
𝑠∈𝑆𝑡
|𝑝𝑢𝑖−𝑟𝑢𝑖|, 𝑢, 𝑖∈𝑆
(8)

Knowledge-Based Systems 280 (2023) 111016
9
J. Bobadilla et al.
Fig. 4. (a) Distribution of users versus ratings. Number of items: 4000. Datasets 8 and 10 in Table 3;(b) Distribution of user ratings. Number of users: 2000; number of items:
4000. Datasets 3, 8 and 13 in Table 3;(c) Number of samples remaining after removing the repeated ones. items: 4000. Datasets 6 to 10 in Table 3;(d) Proportion of samples in
which the same user has cast different votes for the same item. items: 4000. Datasets 6 to 10 in Table 3;(e) MAE and accuracy. Number of users: 2000; number of items: 4000;
‘std’ is the standard deviation of the Gaussian random noise distribution. Datasets 1 to 15 in Table 3;(f) Precision, recall, and F1. Standard deviation of the random noise Gaussian
distribution: 2.5. Number of recommendations 𝑁= [2,4,6,8,10]. Datasets 6 to 10 in Table 3.
4.2. Results
This subsection shows the graphs obtained when the designed ex-
periments (previous subsection) are run. The synthetic datasets de-
scribed in Table 3 are used to obtain results that allow us: (1) to
compare the distributions of users, items and ratings belonging to
the source datasets, in relation to those obtained using the synthetic
datasets, (2) to measure the number of repeated samples returned in
the clustering stage, and (3) to test the prediction and recommendation
qualities and trends obtained by running the proposed RSGAN method
and comparing them to those shown by the source datasets.
4.2.1. Experiment 1a. Number of users: Distribution of users versus ratings
Fig. 4a shows the density of users (y-axis) that have cast different
numbers of votes (x-axis). (Selected datasets: 8 and 10 in Table 3). As
expected, for a fixed number of ratings in the dataset, we can observe
that the higher the number of users, the lower the number of ratings. If
the fixed number of samples in the dataset is distributed among a high
number of users, each user centroid in the clustering stage receives a
lower number of samples. Please, note that Netflix* contains around
23,000 users.
4.2.2. Experiment 1b. Distribution of the user ratings
Fig. 4b shows the proportion of each rating 1,...,5 (x-axis) when
different random noise Gaussian distributions are applied. (Selected
datasets: 3, 8 and 13). It can be observed that the standard deviation 2.5
generates a more similar distribution of votes, compared to the Netflix*
original, than the adjacent standard distributions 2 and 2.5. Fig. 4b also
shows the impact of the Gaussian standard deviation in the layout of
the individual values of the GAN-generated samples.
4.2.3. Experiment 1c. Number of repeated samples
As explained in the ‘Method’ section, the trained GAN generator
predicts from random noise vectors as many dense samples as we want;
all these samples are then converted from continuous dense values to
discrete sparse ones. In the discretization process, repeated samples will
appear that must be removed (Table 1 contains an example). Fig. 4c
shows the number of samples remaining in the dataset after the removal
process. The lower the number of users, the higher the number of
samples assigned to each user (to its centroid in the clustering process),
and therefore the higher the probability of repeating discrete samples.
Overall, the smaller the number of users, the smaller the number of
remaining samples. Selected datasets: 6 to 10 in Table 3.
4.2.4. Experiment 1d. Proportion of samples with the same user and item
The GANRS generated datasets possess one attribute that does not
exist in the source datasets (Movielens, etc.): they contain a proportion
of samples where the same user has cast different votes for the same
item; e.g.: ⟨348, 90, 5⟩, ⟨348, 90, 4⟩, as explained in the ‘Method’ section.
This can be seen as a mechanism to allow intermediate votes (4.5 in
the example) or to allow users to change their minds. This makes sense
if, the number of repeated votes is two or three. The rare cases of four
or five repeated votes should be removed, just as we have done in all
the generated datasets.
From the standard quality metrics to measure the accuracy of
predictions: Mean Absolute Error (MAE) and Root Mean Square Error
(RMSE), we have chosen the former since it is the most widely used
in RS state-of-art research. Some papers provide both measures, but
experimental research shows that in the CF field, results for RMSE and
MAE are very similar. This is because the distribution of the errors in
the CF field usually has little variance. The MAE returns the absolute
difference between the predicted values and the real values in the
testing set: 𝑀𝐴𝐸= 1
𝑛
∑
𝑖|𝑦𝑖−̂𝑦𝑖|. The lower the MAE, the better the
model fits a dataset. The RMSE uses the square of the error instead of
the absolute value: 𝑀𝐴𝐸= 1
𝑛
∑
𝑖(𝑦𝑖−̂𝑦𝑖)2; therefore, the RMSE is more
sensitive to observations that are further from the mean, and this is not
the case in CF.
Fig. 4d shows that for regular CF RS (1000 or more users), the
proportion of four or five repetitions is not significant, and as the
number of users increases, the proportion of repetitions drops very fast.
4.2.5. Experiment 1e. MAE and accuracy of the dataset
Whereas the previous experiments analyze the internal composition
and distribution of the synthetic datasets, this experiment and the

Knowledge-Based Systems 280 (2023) 111016
10
J. Bobadilla et al.
following experiment test the behavior of the generated datasets on
the prediction and recommendation tasks. Fig. 4e shows the prediction
quality (MAE) and the accuracy of the recommendation obtained from
each set of individual samples in Datasets 1 to 15 in Table 3. Please
note that these measures are not obtained by analyzing and averaging
the results of users. The graphs in Fig. 4e show an improvement in
accuracy (and its corresponding decrease in MAE error) as the number
of users increases. This behavior is expected in the CF RS, where a high
number of users leads to better predictions, and it tells us that the GAN-
generated samples follow a CF convenient pattern. The MAE values in
the top graph of Fig. 4e are closely related to the distribution of ratings
for each of the standard deviations 2.0, 2.5 and 3.0. MAE/accuracy
results can be used to select the most appropriate standard deviation;
in this case: std = 2.5.
4.2.6. Experiment 1f. Users’ precision and recall
This experiment provides the most significant results to test the
generated datasets: we extract the values and evolutions of two repre-
sentative recommendation quality measures: precision and recall. The
top graphs in Fig. 4f show the quality values obtained testing several
numbers of recommendations N: [2,4,6,8,10] (x-axis), two different
relevancy thresholds 𝜃: [4,5], and two number of users: 2000 (green
lines), and 8000 (blue lines). The standard deviation of the Gaussian
random noise has been set to 2.5. Selected datasets: 6 to 10 in Table 3.
The values and evolutions obtained from the synthetic datasets fit with
the source dataset: Netflix* (black lines). Additionally, as expected, the
overall results of the dataset generated by 8000 users outperform those
of the 2000 users and are closer to the Netflix* reference (please note
that Netflix* contains around 23,000 users). The two bottom graphs in
Fig. 4f represent the F1 combination of precision and recall; they clearly
show the similarity in the behavior of the generated datasets compared
to the source dataset.
4.2.7. Experiment 2a. MAE and accuracy when the number of items varies.
Experiment 1e tested MAE and accuracy quality measures on
datasets with different numbers of users. Now we will test both quality
measures on datasets with different numbers of items: [100, 1K, 2K,
4K, 8K]. The results in Fig. 5 show adequate values for both MAE and
accuracy, and consistent evolutions where accuracy increases and MAE
decreases as the number of items (x-axis) increases. Thus, the higher
the number of items, the better the accuracy: this shows that the GAN
generator can enrich the data. The Netflix * source dataset contains
1750 items and we can observe in Fig. 5a how the improvement
slows down around this value (x-axis). Selected datasets: 16 and 17 in
Table 3, and the 100, 1000, 4000 user versions not included in Table 3.
4.2.8. Experiment 2b. Items’ precision and recall
Experiment 2b is similar to Experiment 1f; now we will test the
behavior of datasets that contain different numbers of items (instead of
different numbers of users). Fig. 5b shows the performance of Netflix*
(1750 items), represented using black lines, and compares it with the
2000 item dataset (green lines) and the 8000 item dataset (blue lines).
We can observe that evolutions and values are consistent with the
source datasets (black lines); furthermore, both the 2K and 4K items
versions perform well: the first one conveniently captures the Neflix*
patterns of items, since both contain a similar number of items. The
dataset generated second (8K items) can enrich the data and show
better accuracy than the 2K items version. Selected datasets: 16 and
17 in Table 3, and the 100, 1000, 4000 user versions not included in
Table 3.
4.2.9. Experiment 3a. Number of samples generated in datasets with differ-
ent sizes
Here we will test the number of samples that the GANRS method
obtains when different numbers of
generated samples and different
numbers of users have been set. For this purpose, we define four
different numbers of samples: 150K, 500K, 1M and 3M (Datasets 18
to 21 in Table 3, and their equivalent datasets for 100, 1000, 4000
and 8000 users) in the GAN generation process. The number of items
is fixed at 4K for all experiments. In Fig. 6a we can observe that
the smaller the number of users, the smaller the number of generated
samples; this is due to the fact that the smaller the number of users, the
higher the number of samples assigned to each user (to each centroid
in the clustering stage), and therefore the higher the probability of
repeated samples that will be removed. As an example, Fig. 6 shows
that the 8K user dataset preserves, approximately, 1M samples from
the GAN generated (version 3M), and 600K in version 1M.
4.2.10. Experiment 3b. Precision and recall on datasets with different sizes
This experiment shows the impact of increasing the number of
samples in datasets with fixed parameters, in this case: 2000 users,
4000 items, and a standard deviation of 1.2 ( Table 3; Datasets 18, 19
and 21). It is important to realize that we are using the same source
dataset Netflix* to generate the three cases shown in Fig. 6b: 150K
samples (yellow lines), 500K samples (magenta lines), and 3M samples
(red lines). Please note that 150K, 500K and 3M samples refer to the
dense and continuously generated samples, prior to the removal stage
to convert them into their sparse, discrete version. Fig. 6a shows the
final sizes of the datasets in the 2000 user data (x-axis).
Fig. 6b compares the precision and recall values obtained in the
Netflix* dataset (black lines) with the generated values. Overall: (1)
precision increases and recall decreases; (2) the bigger the generated
dataset, the better its precision; (3) the higher the dataset, the lower
its recall. Precision results improve when using large datasets, as there
are more relevant samples to choose from, and therefore it is easier to
succeed in the fixed number 𝑁of recommended predictions. On the
other hand, recall gets worse using large datasets because they contain
more variability in the samples, particularly when large standard devi-
ations have been chosen for the random noise Gaussian distribution.
Unlike precision, whose denominator is the constant 𝑁(number of
recommendations), the recall quality measure depends on the variable:
‘number of relevant votes’ in the set of test items for each user tested.
As the number of samples increases, the number of user votes also
increases (and, from them, the number of relevant votes); this is the
reason why recall is lower in the 3M synthetic dataset in Fig. 6b, and
higher in the 150K version.
Figs. 7 and 8 show, respectively, the results obtained from the
MyAnimeList and Movielens 100K test datasets. Graph ’(a)’ compares
the rating distribution of each source dataset (in blue) with the gen-
erated rating distributions obtained by setting different values of the
Gaussian random noise standard deviation. We have chosen the stan-
dard deviation value of 1.2 for MyAnimeList, and the standard devia-
tion value of 2.5 for Movielens 100K, since the obtained distributions
of ratings are closest to their respective baselines. Results ‘(b)’, ‘(c)’
and ‘(e)’ are obtained using the selected standard deviation values.
Graph ‘(b)’ shows the distribution of users according to their number
of casted ratings (x-axis). As expected, they follow the same pattern as
the one in Netflix*. To compare results, please note that MyAnimelist
dataset contains 19,179 users, and Movielens 100K contains 943 users.
Graph ‘(c)’ shows the number of samples left after removing repeated
instances. The higher the number of users, the lower the probability of
generating samples containing the same user ID, item ID, and rating. In
the MyAnimeList case, we started with 1.5 million generated samples,
whereas for Movielens we selected 1 million generated samples. Graph
‘(d)’ refers to MAE error and accuracy values obtained by processing
the individual samples contained in each dataset. As usual in the CF
context, the higher the number of users, the lower the error, and the
higher the accuracy. Finally, Graphs ‘(e)’ tests the recommendations
obtained by processing the users in each dataset. As is with Netflix*,
compared to baselines, precision improves and recall gets worse.

Knowledge-Based Systems 280 (2023) 111016
11
J. Bobadilla et al.
Fig. 5. (a) MAE and accuracy obtained from the dataset samples when the number of items varies. Number of users: 4000. Standard deviation of the Gaussian random noise:
1.5. Datasets 16 and 17 in Table 3;(b) Precision, recall, and F1 when the number of items varies. Standard deviation of the random noise Gaussian distribution: 1.5. Number of
recommendations 𝑁= [2, 4, 6, 8, 10]. Datasets 16 and 17 in Table 3.
Fig. 6. (a) Number of generated samples using different number of users (x axis) and different number of GAN generated samples (legend). Standard deviation of the random
noise Gaussian distribution: 1.2. Number of items: 4000. Datasets 18 to 21 in Table 3;(b) Precision and recall using a different number of recommendations (x axis) and a different
number of GAN generated samples (legend). Standard deviation of the random noise Gaussian distribution: 1.2. Datasets 18, 19 and 21 in Table 3.
The results obtained in this section highlight the importance of
those that test the performance of the synthetic datasets against the
source datasets, particularly when specific RS metrics are used. To
check the consistency between synthetic and real data, two types of

Knowledge-Based Systems 280 (2023) 111016
12
J. Bobadilla et al.
Fig. 7. MyAnimeList. 1.5 million generated samples, (a) distribution of the MyAnimeList ratings 1 to 10, (b) distribution of users according to their number of casted ratings, (c)
number of samples after the removing process of the repeated ones, (d) error and accuracy by processing the samples of the dataset, (e) CF precision and recall (by testing the
dataset users). The GANRS std = 1.2 value has been set to test experiments (b) to (e).
experiments have been conducted: direct and indirect. In direct com-
parisons, rating distributions have been obtained and compared from
both source datasets and their synthetic versions. Figs. 4b and 4c show
the Netflix* results by varying the number of generated users and the
standard deviation of the random Gaussian distribution used to feed the
proposed GAN. Figs. 7b and 8b show, respectively, comparison of the
MyAnimeList and the Movielens 100K datasets, in this case by varying
the number of users in the synthetic datasets versus their equivalent
source counterparts. Indirect experiments tested and compared the
recommendation performance on both the synthetic and the source
datasets. We have chosen the recommendation quality measures of pre-
cision, recall and F1, obtained using the classical neural model DeepMF.
Results can be found in Fig. 4f (Netflix* vs. its synthetic version), Fig. 7e
(MyAnimeList vs. its synthetic version), and Fig. 8e (Movielens 100K
vs. its synthetic version). Overall, as expected, the results show that
synthetic datasets behave like their source datasets. The more similar
the results are, the more suitable the generated datasets will be, as this
means that the original datasets can be effectively replaced by synthetic
ones.
4.3. Comparison of the proposed framework with previous work
The Related Works section identifies some previous work focused on
data generation methods for recommender systems. In this subsection, a
brief analysis will be performed, which will focus on showing how this
previous work is not truly comparable with our current proposal in a
fair way, since it is focused on different objectives and also generates
data of a different nature.
• Mladenov et al. [53] presented RecSim NG, an architecture cen-
tered on the generation of synthetic profiles of users and items
as part of the recommendation environment. Overall, the goal
of the work is the development of a configurable platform for
both authoring and learning RS simulation environments. The
aim of this simulation is to evaluate existing RS policies, or
generate data to train new policies (in either a tightly coupled
online fashion, or in batch mode). Furthermore, this paper lacks
information about the presented method, and therefore does not
allow reproducibility.
• Shi et al. [54] introduce a multi-agent reinforcement learning
architecture tailored to Taobao-specific website search improve-
ment, and uses a GAN to simulate the internal rating distribu-
tion. Therefore, considering that it is focused on data generation
for a specific context, it is not comparable with the framework
proposed in the current paper.
• Del Carmen et al. [56] introduce DataGenCars, a Java-based
generator of RS synthetic data. Here it is important to remark
that this work is specifically focused on the context-aware recom-
mendation scenario. In this sense, even though the proposed tool
supports the generation of synthetic datasets of users, items, con-
texts, and ratings; this generation always relies on context-related
characteristics through criteria introduced throughout the work,
such as the uncertainty of the content, the user’s expectations or
the item’s attributes. As result, this work is not comparable with
the methodology presented in our current paper, which mainly
uses rating values as input and does not consider datasets with
contextual information.

Knowledge-Based Systems 280 (2023) 111016
13
J. Bobadilla et al.
Fig. 8. Movielens 100K results. 1 million generated samples, (a) Distribution of the Movielens 100K ratings 1 to 5, (b) Distribution of users according to their number of casted
ratings, (c) Number of samples after the removal process of the repeated ones, (d) error and accuracy by processing the samples of the dataset, (e) CF precision and recall (by
testing the dataset users). The GANRS std = 2.5 value has been set to test experiments (b) to (e).
• Provalov et al. [57] introduce the SynEvaRec framework, focused
on the presentation of a novel paradigm for evaluating recom-
mendations based on the generation of synthetic RS datasets. In
contrast to our current paper, this approach is mainly focused
on generating synthetic user and item profiles that are internally
used by SynEvaRec to guarantee user privacy protection, mitigate
the data insufficiency problem, and measure the effect of the
no-free-lunch problem. Regarding the aim of the architecture
proposed in Provalov et al. [57] is not the proper retrieval of the
whole synthetic rating datasets to be used in further evaluations
(i.e. an evaluation protocol is presented rather than a dataset
generation method), a major transformation of their work is
needed to make it comparable with this paper. A fair comparison
is then not possible at this stage.
4.4. Overall discussion
A large number of synthetic datasets have been generated to test the
performance of the proposed GANRS method. These datasets have been
created setting different values for the main parameters of the method:
number of users, number of items, Gaussian random noise variation,
and number of generated samples. To generalize the conclusions of this
paper, three open and representative CF datasets have been used as
sources for the generative process. Finally, a variety of quality measures
have been tested on the generated datasets; of these, precision and
recall are the most relevant. A key issue is that we are not able to
visually test the quality of the generated samples, as can be done, for
example, with the popular fake faces; in fact, in the CF context, we
only can adequately test the generated datasets by comparing their CF
quality results with those typically obtained in real CF datasets. For
this reason, we have focused on the designed experiments in which
the quality measures of precision and recall are tested: using datasets
containing different numbers of users, different numbers of items, and
different numbers of samples (sizes). In all cases, comparatively, we
obtain excellent precision results and moderate recall values. Overall, it
can be considered positive in the CF context, where precision errors are
serious and recall errors are less important: it is worse to recommend a
trip you will not like (sorry, no refunds!) than not to recommend a trip
that you probably would enjoy. Please note that it is the opposite for
a deep learning model detecting malignant tumors: it is worse to make
precision errors (no early detection of the tumor) than making recall
errors (to erroneously detect a tumor).
Additionally, experiments show the relevant impact of the standard
deviation on the quality of the results. The GAN network learning has
been based on a vector containing noise values that serves as a seed
to generate the different samples in the synthetic dataset. Each ‘fake’
sample is generated from the list of random values in the ‘noise’ vector.
As usual in the GAN context, random values have been created from

Knowledge-Based Systems 280 (2023) 111016
14
J. Bobadilla et al.
a Gaussian distribution with a mean of 0 and a standard deviation
of 1. Each generated sample contains a dense item representation,
a dense user representation, and an individual value that codes the
user’s rating of the item. Once the GAN has learned, its generative
model can be used, in a feedforward process, to generate as many
samples as we want, starting from a different random noise vector
for each sample generated. Experimental results show that using a
Gaussian distribution with a standard deviation of 1 leads to many
ratings in the middle of the voting range: rating 3 and closest in the
1 to 5 voting range (Movielens and Netflix), and rating 5 and closests
in 1 to 10 voting range (MyAnimeList). Several experiments in this
section demonstrate that we can modulate the standard deviation of
the Gaussian distribution of random noise to generate a wider range of
ratings using feedforward. As expected, when the standard deviation
increases, the range of ratings also increases proportionally.
Finally, the experiments presented include the existing relationship
between the number of fake samples generated for the GAN and
the number of samples that the dataset will eventually contain. As
explained in the ‘Method’ section, the conversion from dense and
continuous values to sparse and discrete values leads to a probability
of sample repetitions. Results show that, as expected, the larger the
number of users and items in the synthetic dataset, the lower the
number of repeated samples. It has also been shown that for a typical
number of users, say 4000 or more, the probability of more than two
different ratings from one user for the same item can be considered
negligible.
5. Conclusions
This paper provides an innovative method for generating synthetic
parameterized collaborative filtering datasets from real datasets. Syn-
thetic datasets can be generated by selecting different numbers of
users, items, samples, and distribution variability. This means that
comparative experiments can be designed on the basis of a whole
‘family’ of generated datasets, for example, to test the accuracy of a new
matrix factorization model when the number of users increases. A GAN
is used to obtain ‘fake’ samples from real samples, benefitting from the
inherent capacity of GAN networks to capture complex patterns in the
source datasets. The GAN learns from dense and continuous embedding
representations of items and users, rather than the sparse and discrete
representations of the collaborative filtering datasets. The effect is a
fast and accurate learning process.
The proposed GANRS method contains a clustering stage to convert
from the dense generated ‘fake’ samples to the sparse and discrete
values necessary to fill the generated dataset. This clustering stage
implements a k-means algorithm to group items and another k-means to
group users. In a natural way, both ‘k’ parameters set the chosen num-
ber of users and items in the dataset. A drawback of the discretization
process is the generation of identical samples that our method merely
removes. A complete set of experiments have been made using three
representative source datasets. We have tested the distribution values
and evolutions of the results, as well as prediction and recommendation
qualities. Although precision tends to improve, while recall tends to
worsen, overall accuracy can be considered correct, since precision is
more relevant than recall in the RS context. The results show that
the generated datasets conveniently mimic the behavior of the source
datasets Movielens, MyAnimeList, etc.
The source code for the proposed GANRS method is available to
ensure the reproducibility of the experiments. Similarly, a complete set
of generated datasets has been made available for research. This paper
and its related documentation open the door to address some future
work, such as designing alternative options to the clustering stage,
implementing the PacGAN concept in the GAN discriminator, testing
generated datasets using a complete range of machine learning and
deep learning collaborative filtering models, replacing the GAN model
with a CGAN one, generating demographically balanced datasets, and
performing an in-depth study of the impact of the random noise vector
variations in the generated set of samples.
CRediT authorship contribution statement
Jesús Bobadilla: Conceptualization, Validation, Formal analysis,
Investigation, Software, Writing – original draft, Writing – review &
editing, Visualization. Abraham Gutiérrez: Conceptualization, Valida-
tion, Formal analysis, Investigation, Software, Writing – original draft,
Visualization. Raciel Yera: Methodology, Validation, Formal analysis,
Writing – original draft , Visualization. Luis Martínez: Methodology,
Validation, Writing – original draft.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Data availability
The link to the data and code has been shared in the manuscript.
Acknowledgments
This work was partially supported by Ministerio de Ciencia e Inno-
vación of Spain under the project PID2019-106493RB-I00 (DL-CEMG);
the Comunidad de Madrid, Spain under Convenio Plurianual with
the Universidad Politécnica de Madrid, Spain in the actuation line of
Programa de Excelencia para el Profesorado Universitario; and the Plan
Andaluz de Investigación, Desarrollo e Innovación (PAIDI 2020), Spain
under the project PROYEXCEL_00257.
Appendix A
See Table 4.
Table 4
Main parameter and hyperparameter values set for the neural models involved in the
RSGAN method.
DeepMF
values
Embedding size (both for
users an items)
5
Optimizer
Adam
Loss function
Mean squared error
Epochs
20
GAN generator
Input shape, noise vector size
100
Block 1 dense layer #neurons
10
Block 1 activation function
LeakyRelu, alpha 0.2
Block 1 normalization
BatchNormalization, momentum 0.8
Block 2 dense layer #neurons
20
Block 2 activation function
LeakyRelu, alpha 0.2
Block 2 regularization
Dropout 0.2
Block 3 dense layer #neurons
2 ∗𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑠𝑖𝑧𝑒+ 1
Block 3 activation function
linear
GAN discriminator
Input: shape
2 ∗𝑒𝑚𝑏𝑒𝑑𝑑𝑖𝑛𝑔𝑠𝑖𝑧𝑒+ 1
Block 1 dense layer #neurons
6
Block 1 activation function
LeakyRelu, alpha 0.2
Block 2 dense layer
1
Block 2 activation function
Sigmoid
GAN train
Epochs
20
Batch size
64
Stochastic noise
Gaussian (0,1)
Loss function
(𝑟𝑒𝑎𝑙𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑙𝑜𝑠𝑠+ 𝑓𝑎𝑘𝑒𝑠𝑎𝑚𝑝𝑙𝑒𝑠𝑙𝑜𝑠𝑠)∕2

Knowledge-Based Systems 280 (2023) 111016
15
J. Bobadilla et al.
Appendix B
See Fig. 9.
Fig. 9. Main distributions of the data in the synthetic dataset generated from Movielens
100K compared to the distributions of the data in the source dataset. Number of users:
8000, number of items: 4000, initial number of samples: 800,000, standard deviation
of the Gaussian noise: 2.5. Graph (a) shows the distribution of the fake users (y axis)
versus the number of ratings belonging to each of the users (x axis). Graph (b) shows
the distribution of the fake items (y axis) versus the number of ratings belonging to
each of the items (x axis). Graph (c) shows the percentage of ratings (y axis) for each
of the available vote values 1, 2, 3, 4, 5 (x axis) in the dataset.
References
[1] Z. Fang, L. Zhang, K. Chen, A behavior mining based hybrid recommender
system, in: 2016 IEEE International Conference on Big Data Analysis, ICBDA,
IEEE, 2016, pp. 1–5.
[2] R. Yera, L. Martínez, Fuzzy tools in recommender systems: A survey, Int. J.
Comput. Intell. Syst. 10 (1) (2017) 776–803.
[3] R. Yera, A.A. Alzahrani, L. Martínez, A fuzzy content-based group recommender
system with dynamic selection of the aggregation functions, Internat. J. Approx.
Reason. 150 (2022) 273–296.
[4] L. Zheng, V. Noroozi, P.S. Yu, Joint deep modeling of users and items using
reviews for recommendation, in: Proceedings of the Tenth ACM International
Conference on Web Search and Data Mining, 2017, pp. 425–434.
[5] Y. Gong, Q. Zhang, Hashtag recommendation using attention-based convolutional
neural network., in: IJCAI, 2016, pp. 2782–2788.
[6] H. Kanwal, M. Assam, A. Jabbar, S. Khan, et al., Convolutional neural network
and topic modeling based hybrid recommender system, Int. J. Adv. Comput. Sci.
Appl. 11 (7) (2020).
[7] K. McNally, M.P. O’Mahony, B. Smyth, A comparative study of collaboration-
based
reputation
models
for
social
recommender
systems,
User
Model.
User-Adapt. Interact. 24 (3) (2014) 219–260.
[8] N.M. Villegas, C. Sánchez, J. Díaz-Cely, G. Tamura, Characterizing context-aware
recommender systems: A systematic literature review, Knowl.-Based Syst. 140
(2018) 173–200.
[9] M.
Moradi,
J.
Hamidzadeh,
Ensemble-based
top-k
recommender
system
considering incomplete data, J. AI Data Min. 7 (3) (2019) 393–402.
[10] M. Jalili, S. Ahmadian, M. Izadi, P. Moradi, M. Salehi, Evaluating collaborative
filtering recommender algorithms: a survey, IEEE Access 6 (2018) 74003–74024.
[11] B. Zhu, R. Hurtado, J. Bobadilla, F. Ortega, An efficient recommender system
method based on the numerical relevances and the non-numerical structures of
the ratings, IEEE Access 6 (2018) 49935–49954.
[12] R. Yera, A.A. Alzahrani, L. Martínez, Exploring post-hoc agnostic models for
explainable cooking recipe recommendations, Knowl.-Based Syst. 251 (2022)
109216.
[13] E. D’Amico, G. Gabbolini, C. Bernardis, P. Cremonesi, Analyzing and improving
stability of matrix factorization for recommender systems, J. Intell. Inf. Syst. 58
(2) (2022) 255–285.
[14] M.H. Aghdam, A novel constrained non-negative matrix factorization method
based on users and items pairwise relationship for recommender systems, Expert
Syst. Appl. 195 (2022) 116593.
[15] G. Ayci, A. Köksal, M.M. Mutlu, B. Suyunu, A.T. Cemgil, Active learning with
Bayesian nonnegative matrix factorization for recommender systems, in: 2019
27th Signal Processing and Communications Applications Conference, SIU, IEEE,
2019, pp. 1–4.
[16] J. Bobadilla, R. Bojorque, A.H. Esteban, R. Hurtado, Recommender systems
clustering using Bayesian non negative matrix factorization, IEEE Access 6 (2017)
3549–3564.
[17] H.-J. Xue, X. Dai, J. Zhang, S. Huang, J. Chen, Deep matrix factorization models
for recommender systems, in: IJCAI, Vol. 17, Melbourne, Australia, 2017, pp.
3203–3209.
[18] X. He, L. Liao, H. Zhang, L. Nie, X. Hu, T.-S. Chua, Neural collaborative filtering,
in: Proceedings of the 26th International Conference on World Wide Web, 2017,
pp. 173–182.
[19] J. Bobadilla, R. Lara-Cabrera, A. Gonzalez-Prieto, F. Ortega, DeepFair: Deep
learning for improving fairness in recommender systems., Int. J. Interact.
Multimed. Artif. Intell. 6 (6) (2021) 86–95.
[20] Y. Himeur, A. Alsalemi, A. Al-Kababji, F. Bensaali, A. Amira, C. Sardianos,
G. Dimitrakopoulos, I. Varlamis, A survey of recommender systems for energy
efficiency in buildings: Principles, challenges and prospects, Inf. Fusion 72 (2021)
1–21.
[21] J. Bobadilla, J. Dueñas, A. Gutiérrez, F. Ortega, Deep variational embedding
representation on neural collaborative filtering recommender systems, Appl. Sci.
12 (9) (2022) 4168.
[22] J. Bobadilla, Á. González-Prieto, F. Ortega, R. Lara-Cabrera, Deep learning
approach to obtain collaborative filtering neighborhoods, Neural Comput. Appl.
34 (4) (2022) 2939–2951.
[23] S. Zhang, L. Yao, A. Sun, Y. Tay, Deep learning based recommender system: A
survey and new perspectives, ACM Comput. Surv. 52 (1) (2019) 1–38.
[24] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A.
Courville, Y. Bengio, Generative adversarial networks, Commun. ACM 63 (11)
(2020) 139–144.
[25] D. Sacharidis, Diversity and novelty in social-based collaborative filtering, in:
Proceedings of the 27th ACM Conference on User Modeling, Adaptation and
Personalization, 2019, pp. 139–143.
[26] A. Gogna, A. Majumdar, DiABlO: Optimization based design for improving
diversity in recommender system, Inform. Sci. 378 (2017) 59–74.
[27] J. Bobadilla, A. Gutierrez, S. Alonso, Á. González-Prieto, Neural collaborative
filtering classification model to obtain prediction reliabilities, Int. J. Interact.
Multimed. Artif. Intell. 7 (4) (2022) 18–26.

Knowledge-Based Systems 280 (2023) 111016
16
J. Bobadilla et al.
[28] F. Pajuelo-Holguera, J.A. Gómez-Pulido, F. Ortega, Evaluating strategies for
selecting test datasets in recommender systems, in: International Conference on
Hybrid Artificial Intelligence Systems, Springer, 2019, pp. 243–253.
[29] K.D. Bollacker, S. Lawrence, C.L. Giles, CiteSeer: An autonomous web agent for
automatic retrieval and identification of interesting publications, in: Proceedings
of the Second International Conference on Autonomous Agents, 1998, pp.
116–123.
[30] W. Choochaiwattana, Usage of tagging for research paper recommendation,
in: 2010 3rd International Conference on Advanced Computer Theory and
Engineering, Vol. 2, ICACTE, IEEE, 2010, pp. V2–439.
[31] J. Shokeen, C. Rana, Social recommender systems: techniques, domains, metrics,
datasets and future scope, J. Intell. Inf. Syst. 54 (3) (2020) 633–667.
[32] Y. Xing, I. Mohallick, J.A. Gulla, Ö. Özgöbek, L. Zhang, An educational news
dataset for recommender systems, in: Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, Springer, 2020, pp. 562–570.
[33] F. Ortega, J. Bobadilla, A. Gutiérrez, R. Hurtado, X. Li, Artificial intelligence
scientific documentation dataset for recommender systems, IEEE Access 6 (2018)
48543–48555.
[34] D. Liang, R.G. Krishnan, M.D. Hoffman, T. Jebara, Variational autoencoders for
collaborative filtering, in: Proceedings of the 2018 World Wide Web Conference,
2018, pp. 689–698.
[35] S. Zamany, D. Li, H. Fei, P. Li, Towards deeper understanding of variational
auto-encoders for binary collaborative filtering, in: Proceedings of the 2022 ACM
SIGIR International Conference on Theory of Information Retrieval, 2022, pp.
254–263.
[36] M. Gao, J. Zhang, J. Yu, J. Li, J. Wen, Q. Xiong, Recommender systems based
on generative adversarial networks: A problem-driven perspective, Inform. Sci.
546 (2021) 1166–1185.
[37] Y. Deldjoo, T.D. Noia, F.A. Merra, A survey on adversarial recommender systems:
from attack/defense strategies to generative adversarial networks, ACM Comput.
Surv. 54 (2) (2021) 1–38.
[38] D.-K. Chae, J.-S. Kang, S.-W. Kim, J.-T. Lee, Cfgan: A generic collaborative
filtering framework based on generative adversarial networks, in: Proceedings
of the 27th ACM International Conference on Information and Knowledge
Management, 2018, pp. 137–146.
[39] Z. Wang, M. Gao, X. Wang, J. Yu, J. Wen, Q. Xiong, A minimax game for
generative and discriminative sample models for recommendation, in: Pacific-
Asia Conference on Knowledge Discovery and Data Mining, Springer, 2019, pp.
420–431.
[40] W. Zhao, B. Wang, J. Ye, Y. Gao, M. Yang, X. Chen, Plastic: Prioritize long and
short-term information in top-n recommendation using adversarial training, in:
Ijcai, 2018, pp. 3676–3682.
[41] H. Bharadhwaj, H. Park, B.Y. Lim, Recgan: recurrent generative adversarial net-
works for recommendation systems, in: Proceedings of the 12th ACM Conference
on Recommender Systems, 2018, pp. 372–376.
[42] G. Guo, H. Zhou, B. Chen, Z. Liu, X. Xu, X. Chen, Z. Dong, X. He, IPGAN:
Generating informative item pairs by adversarial sampling, IEEE Trans. Neural
Netw. Learn. Syst. (2020).
[43] J. Zhao, H. Li, L. Qu, Q. Zhang, Q. Sun, H. Huo, M. Gong, DCFGAN: An adver-
sarial deep reinforcement learning framework with improved negative sampling
for session-based recommender systems, Inform. Sci. 596 (2022) 222–235.
[44] J. Sun, B. Liu, H. Ren, W. Huang, NCGAN:: A neural adversarial collaborative
filtering for recommender system, J. Intell. Fuzzy Systems 42 (4) (2022)
2915–2923.
[45] Y. Lin, Z. Xie, B. Xu, K. Xu, H. Lin, Info-flow enhanced GANs for recommender,
in: Proceedings of the 44th International ACM SIGIR Conference on Research
and Development in Information Retrieval, 2021, pp. 1703–1707.
[46] Q. Wang, Q. Huang, K. Ma, X. Zhang, A recommender system based on
model regularization wasserstein generative adversarial network, in: 2021 IEEE
International Conference on Systems, Man, and Cybernetics, SMC, IEEE, 2021,
pp. 2043–2048.
[47] J. Wen, X.-R. Zhu, C.-D. Wang, Z. Tian, A framework for personalized recom-
mendation with conditional generative adversarial networks, Knowl. Inf. Syst. 64
(10) (2022) 2637–2660.
[48] G. Deng, C. Han, D.S. Matteson, Extended missing data imputation via GANs for
ranking applications, Data Min. Knowl. Discov. (2022) 1–23.
[49] H. Chen, S. Wang, N. Jiang, Z. Li, N. Yan, L. Shi, Trust-aware generative
adversarial network with recurrent neural network for recommender systems,
Int. J. Intell. Syst. 36 (2) (2021) 778–795.
[50] G. Van Houdt, C. Mosquera, G. Nápoles, A review on the long short-term memory
model, Artif. Intell. Rev. 53 (2020) 5929–5955.
[51] W. Shafqat, Y.-C. Byun, A hybrid GAN-based approach to solve imbalanced data
problem in recommendation systems, IEEE Access 10 (2022) 11036–11047.
[52] Z. Lin, A. Khetan, G. Fanti, S. Oh, Pacgan: the power of two samples in generative
adversarial networks, in: Proceedings of the 32nd International Conference on
Neural Information Processing Systems, 2018, pp. 1505–1514.
[53] M. Mladenov, C.-w. Hsu, V. Jain, E. Ie, C. Colby, N. Mayoraz, H. Pham, D.
Tran, I. Vendrov, C. Boutilier, Demonstrating principled uncertainty modeling
for recommender ecosystems with RecSim NG, in: Fourteenth ACM Conference
on Recommender Systems, 2020, pp. 591–593.
[54] J.-C. Shi, Y. Yu, Q. Da, S.-Y. Chen, A.-X. Zeng, Virtual-taobao: Virtualizing real-
world online retail environment for reinforcement learning, in: Proceedings of
the AAAI Conference on Artificial Intelligence, Vol. 33–01, 2019, pp. 4902–4909.
[55] C.-N. Ziegler, S.M. McNee, J.A. Konstan, G. Lausen, Improving recommendation
lists through topic diversification, in: Proceedings of the 14th International
Conference on World Wide Web, 2005, pp. 22–32.
[56] M. del Carmen Rodríguez-Hernández, S. Ilarri, R. Hermoso, R. Trillo-Lado,
DataGenCARS: A generator of synthetic data for the evaluation of context-aware
recommendation systems, Pervasive Mob. Comput. 38 (2017) 516–541.
[57] V. Provalov, E. Stavinova, P. Chunaev, SynEvaRec: A framework for evalu-
ating recommender systems on synthetic data classes, in: 2021 International
Conference on Data Mining Workshops, ICDMW, IEEE, 2021, pp. 55–64.
[58] A. Cossu, A. Carta, V. Lomonaco, D. Bacciu, Continual learning for recurrent
neural networks: an empirical evaluation, Neural Netw. 143 (2021) 607–627.
[59] M. Ahmed, R. Seraj, S.M.S. Islam, The k-means algorithm: A comprehensive
survey and performance evaluation, Electronics 9 (8) (2020) 1295.
[60] F.M. Harper, J.A. Konstan, The movielens datasets: History and context, ACM
Trans. Interact. Intell. Syst. (TIIS) 5 (4) (2015) 1–19.
[61] F. Ortega, B. Zhu, J. Bobadilla, A. Hernando, CF4j: Collaborative filtering for
java, Knowl.-Based Syst. 152 (2018) 94–99.
"
Deep variational models for collaborative filtering-based recommender systems.pdf,"ORIGINAL ARTICLE
Deep variational models for collaborative filtering-based
recommender systems
Jesu´s Bobadilla1,2 • Fernando Ortega1,2 • Abraham Gutie´rrez1,2 • A´ ngel Gonza´lez-Prieto2,3,4
Received: 16 September 2022 / Accepted: 22 November 2022 / Published online: 9 December 2022
 The Author(s) 2022
Abstract
Deep learning provides accurate collaborative ﬁltering models to improve recommender system results. Deep matrix
factorization and their related collaborative neural networks are the state of the art in the ﬁeld; nevertheless, both models
lack the necessary stochasticity to create the robust, continuous, and structured latent spaces that variational autoencoders
exhibit. On the other hand, data augmentation through variational autoencoder does not provide accurate results in the
collaborative ﬁltering ﬁeld due to the high sparsity of recommender systems. Our proposed models apply the variational
concept to inject stochasticity in the latent space of the deep architecture, introducing the variational technique in the neural
collaborative ﬁltering ﬁeld. This method does not depend on the particular model used to generate the latent representation.
In this way, this approach can be applied as a plugin to any current and future speciﬁc models. The proposed models have
been tested using four representative open datasets, three different quality measures, and state-of-the-art baselines. The
results show the superiority of the proposed approach in scenarios where the variational enrichment exceeds the injected
noise effect. Additionally, a framework is provided to enable the reproducibility of the conducted experiments.
Keywords Recommender systems  Collaborative ﬁltering  Variational enrichment  Deep learning
1 Introduction
Recommender Systems (RSs) are an artiﬁcial intelligence
ﬁeld that provides methods and models to predict and
recommend items to users (e.g., ﬁlms to persons, e-com-
merce products to costumers, services to companies,
Quality of Service (QoS) to Internet of Things (IoT)
devices, etc.) [1]. Current popular RSs are Spotify, Netﬂix,
TripAdvisor, Amazon, etc. RSs are usually categorized
attending
to
their
ﬁltering
strategy,
mainly
demo-
graphic [2],
content-based [3],
context-aware [4],
social [5], Collaborative Filtering (CF) [1, 6] and ﬁltering
ensembles [7, 8]. CF is the most accurate and widely used
ﬁltering approach to implement RSs. CF models have
evolved from the K-Nearest Neighbors (KNN) algorithm to
the Probabilistic Matrix Factorization (PMF) [9], the non-
Negative Matrix Factorization (NMF) [10] and the Baye-
sian non-Negative Matrix Factorization (BNMF) [11].
Currently, deep learning research approaches are growing
in strength: they provide improvement in accuracy com-
pared to the Machine Learning (ML)-based Matrix Fac-
torization (MF) models [12]. Additionally, deep learning
architectures are usually more ﬂexible than the MF-based
ones, introducing combined deep and shallow learn-
ing [13], integrated content-based ensembles [14], gener-
ative approaches [15, 16], among others.
Deep Matrix Factorization (DeepMF) [17] is a neural
network model that implements the popular MF concept.
DeepMF was designed to take as input a user-item matrix
with explicit ratings and nonpreference implicit feedback,
& A´ ngel Gonza´lez-Prieto
angelgonzalezprieto@ucm.es
1
Departamento de Sistemas Informa´ticos, ETSI Sistemas
Informa´ticos, Universidad Polite´cnica de Madrid, C. de Alan
Turing, s/n, Madrid, 28031 Madrid, Spain
2
KNODIS Research Group, Universidad Polite´cnica de
Madrid, C. de Alan Turing, s/n, Madrid, 28031 Madrid, Spain
3
Departamento de A´ lgebra, Geometrı´a y Topologı´a,
Universidad Complutense de Madrid, Plaza Ciencias 3,
Madrid, 28040 Madrid, Spain
4
Instituto de Ciencias Matema´ticas (CSIC-UAM-UCM-
UC3M), C/ Nicola´s Cabrera, 13-15, Madrid, 28049 Madrid,
Spain
123
Neural Computing and Applications (2023) 35:7817–7831
https://doi.org/10.1007/s00521-022-08088-2
(0123456789().,-volV)(0123456789().
,- volV)

although current implementations use two embedding
layers whose inputs are, respectively, user and items. The
experimental results evidence the DeepMF superiority over
the traditional approaches based on ML-focused RS, par-
ticularly the most used MF models: PMF, NMF, and
BNMF. Currently, DeepMF is a popular model that is
rapidly replacing the traditional MF models based on
classical ML. Additionally, DeepMF has been used in the
RS ﬁeld to combine social behaviors (clicks, ratings,...)
with images [18], and a social trust-aware RS has been
implemented by using DeepMF to extract features from the
user-item rating matrix for improving the initialization
accuracy [19]. QoS predictions have also been addressed
by using DeepMF [20]. To learn attribute representations, a
DeepMF model has been used that creates a low-dimen-
sional representation of a dataset that lends itself to a
clustering interpretation [21]. Finally, the classical matrix
completion task has been addressed by using the DeepMF
approach [22].
The not so widely spread Neural Collaborative Filtering
(NCF) model [13] may be seen as an augmented DeepMF
model, where deeper layers are added to the ‘Dot’ one.
Additionally, the ‘Dot’ layer can be replaced by a
‘Concatenate’ layer. Figure 1 shows the explained
concepts. NCF slightly outperforms the DeepMF accuracy
results, but it increases the required runtime to train the
model and to run the forward process: it is necessary to
execute the ‘extra’ Multi-Layer Perceptron (MLP) on top
of the ‘Dot’ or ‘Concatenate’ layers. Moreover,
compared to DeepMF, the NCF architecture adds new
hyper-parameters to set: mainly the number of hidden
layers (depth) and their size (number of neurons in each
layer) of the MLP architecture.
The hypothesis of the paper is that we can improve the
existing CF neural models by adding a variational stage
that borrows its operative from the Variational Autoen-
coders (VAE). VAEs not only improve latent factor-based
models, but they also manage nonlinear probabilistic
latent-variable models. While VAEs have been extensively
used in the image-generative area, they have rarely been
covered in the CF ﬁeld. Autoencoders perform a nonlinear
PCA, and VAEs improve their results by performing a
nonlinear factor analysis. Unfortunately, regular autoen-
coders do not ensure the regularity of the latent space; this
is the reason why, in image processing, they do not perform
ﬁne producing new content from random encodings:
Without explicit regularization, some combinations of the
latent space are meaningless once decoded. The VAEs
superiority comes from their ‘variational’ behavior, which
allows to make suitable regularizations such as in the
statistic variational method. Using VAEs, inputs are
encoded as distributions instead of single points, making it
possible to naturally express latent space regularization.
The CF improvement using VAEs is because the item and
the user latent factor distributions are regularized in the
training stage, ensuring that their latent spaces have good
properties and conveniently generalize RS predictions. The
VAEs regularization has two main properties: (1) com-
pleteness: points sampled in the latent space give mean-
ingful content once decoded, and (2) continuity: close
points in the latent space provide similar contents when
they are decoded. To accomplish these properties, usually
regularization is done by enforcing distributions to be close
to a centered and reduced standard normal distribution.
Regularization involves a higher reconstruction error that
can be balanced using the Kullback–Leibler divergence.
The use of VAE in the CF ﬁeld provides a better
Fig. 1 Deep Matrix Factorization (DeepMF) versus Neural Collaborative Filtering (NCF)
7818
Neural Computing and Applications (2023) 35:7817–7831
123

generalization; it not only can improve recommendations,
but it also makes easier to use the latent codiﬁcations of
items and users to make clustering, to explain recommen-
dations, and to generate augmented datasets. The com-
pleteness and continuity properties make possible these
additional beneﬁts of the VAEs in the CF area.
The rest of the paper has been structured as follows: In
Sect. 2, we describe the main ideas involved in our pro-
posal, as well as its differences with the related work in
variational CF-based recommender systems. In Sect. 3, the
proposed model is explained. Section 4 shows the experi-
ments’ design, results and their discussions. Finally, Sect. 5
contains the main conclusions of the paper and the future
works.
2 Fundamentals and related work
2.1 VAEs as generative models
Variational Autoencoders (VAEs) act as regular autoen-
coders; they aim to compress the input raw values into a
latent space representation by means of an encoder neural
network, whereas the decoder neural network makes the
opposite operation seeking to decompress from latent space
to output raw values. The main difference between clas-
sical autoencoders and VAEs is the latent space design,
meaning, and operation. Classical autoencoders do not
generate structured latent spaces, whereas VAEs introduce
a statistical process that forces them to learn continuous
and structured latent spaces. In this way, VAEs turn the
samples into parameters of a statistical distribution, usually
the means and variance of a Gaussian distribution. From
the parameters in the multivariate distribution, we draw a
random sample and a latent space sample is obtained for
each training input. This operation procedure is represented
in Fig. 2.
The stochasticity of the random sampling improves the
robustness and forces the encoding of continuous and
meaningful latent space representations, as it can be seen in
Fig. 3, where it is shown the VAE latent space represen-
tation and its cumulative normal distribution.
Due to their properties, VAEs have been used as gen-
erative deep learning models in the image processing ﬁeld.
Reconstruction of a multispectral image has been per-
formed by means of a VAE [23] that parameterizes the
latent space of Gaussian distribution parameters. VAEs
have been also used to create superresolution images as in
[24], where a model is proposed to encode low-resolution
images in a dense latent space vector that can be decoded
for target high resolution image denoising. The blur image
problem using VAE is tackled in [25] by adding a condi-
tional sampling mechanism that narrows down the latent
space, making it possible to reconstruct high resolution
images. Moreover, in [26], the authors propose a ﬂexible
autoencoder model able to adapt to varying data patterns
with time. By importing the VAE concept from image
processing, several papers have used these models to
improve RS results. For instance, denoising and variational
autoencoders are tested in [27], where the authors reported
the superiority of the VAE option against other models, or
in [28], where variational autoencoders are combined with
social
information
to
improve
the
quality
of
the
recommendations.
2.2 Our proposal: Deep variational models
The aim of this paper is to propose a neural architecture
that joins the best of the DeepMF and NCF models with the
VAE concept. This novel models will be called, respec-
tively, Variational Deep Matrix Factorization (VDeepMF)
and Variational Neural Collaborative Filtering (VNCF). In
contrast with the autoencoder and Generative Adversarial
Network (GAN) approaches in the CF ﬁeld [16, 29, 30], we
shall not use the generative decoder stage and we maintain
the regression output layer presented in the DeepMF and
the NCF models. The main advantage in the use of the
VAE operation is the robustness that it confers to the latent
representation. This robustness can be seen by observing
Fig. 3. If we consider each dot drawn as a train sample
representation in the latent space, then test samples are
most likely to be correctly classiﬁed in the VAE model
(right graph in Fig. 3) than being correctly classiﬁed in the
regular autoencoder model (left graph in Fig. 3). In short,
the variational approach stochastically ‘spreads’ the sam-
ples in the latent space, improving the chances of classi-
fying correctly the training samples.
In our proposed RS CF scenario, we expect that rating
values can be better predicted when a variational latent
space has been learnt, because this space covers a wider,
more robust, and more representative latent area. Whereas
with a traditional autoencoders each sample would be
coded as a value in the latent space (white circle in Fig. 4),
the VAE encodes the parameters of a multivariate distri-
bution (e.g., mean and variance of both the blue and the
orange Gaussian distributions in Fig. 4). From the learnt
distribution parameters, random sampling is carried out to
generate stochastic latent space values (gray circles in
Fig. 4). Each epoch in the learning process generates a new
set of latent space values. Once the proposed model has
been trained, when a huser, itemi tuple is presented to the
model, the obtained latent space value (green circle in
Fig. 4) can be better predicted in the VAE scenario than in
the regular autoencoder scenario: the random sampled
values (gray circles) of the enriched latent space will help
to associate the predicted sample (green circle) with their
Neural Computing and Applications (2023) 35:7817–7831
7819
123

associated training samples (white circle), making the
prediction process much more robust and accurate.
From the above explanations, the VAE operation can be
deﬁned following Fig. 2 in its ‘Variational layers’ stage:
ﬁrst, two dense layers code the normal distribution
parameters that set the mean and variance of the latent
factors. In the CF scenario, two dense layers are arranged
to code the normal distribution parameters of the items, and
two other different dense layers are used to code the normal
distribution parameters of the users. This variational
approach regularizes the latent factors and makes it pos-
sible to reach the explained completeness and continuity
goals. Once the distribution parameter layers are regular-
ized, it is necessary to obtain a single latent factor point to
code each user or item in the dataset; that is, for each user
and item in the input of the model we need to combine its
mean and variance. A normal random function is used to
generate the latent factor point, coding the item (or the
user) in the model input. Then, each latent factor point is
obtained by combining: the normal random value, its item
mean (or its user mean) and its item variance (or its user
variance). This operation is usually performed using a
neural ‘Lambda’ layer. Each Lambda layer result can be
seen as a regularized version of the DeepMF nonvariational
Fig. 2 Operation of a trained Variational Autoencoder (VAE) model.
When a new sample is presented to the encoder stage (the handwritten
digit ’2’ in this example), the model produces in the latent space a
probability distribution. Typically, this distribution belongs to a
known family (a multivariate normal distribution in this example), so
its shape is determined by some numerical parameters (mean and
standard deviation in our case). With this information, the decoder
stage generates an instance by sampling this distribution (getting a
slightly different digit ’2’ in this example). This introduces a
stochastic component in the generation procedure that enriches the
latent space and variability of the generative model
Fig. 3 Representation of a VAE latent space for the MNIST dataset (left side) and its cumulative normal distribution (right side)
7820
Neural Computing and Applications (2023) 35:7817–7831
123

approach. Finally, we obtain the prediction of the rating of
the user to the item by combining the ‘Lambda’ user and
item factors using a dot product. In short, our variational
approach incorporates the following substages: (1) Con-
verting the input embedding factors to normal distribution
values; and thus, making a regularization to generate
continuous and complete latent factor codes, (2) Combin-
ing the normal distribution latent factor codes to obtain
single latent factor values, and (3) Predicting ratings by
making the dot product of the regularized latent factor
values.
2.3 VAEs for recommender systems
Current CF-based variational autoencoders usually obtain
raw augmented data. One strategy is to synthetize ratings
from user to items or generated relevant versus not relevant
votes from users to items [16, 27, 31], and another
approach is to pre-train a VAE model to map data vectors
into the latent space, an idea that has been intensively
studied in several variants [32–36].
In any case, these strategies force practitioners to
sequentially run two separated models: the generative
model (GAN or VAE) that provides augmented data, and
the regression CF model that makes predictions and rec-
ommendations. This approach presents three main draw-
backs:
(1)
complexity,
as
two
separate
models
are
necessary, (2) large time consumption, and (3) sparsity
management. As we will explain deeper in the following
section, our proposed model does not generate raw aug-
mented data. On the contrary, its innovation is based on the
use of a single model to internally manage both augmen-
tation and prediction aims. Particularly signiﬁcant is the
way in which the proposed model addresses the sparsity
problem: we do not make augmentation on the sparse raw
data (ratings cast from users to item), but an internal
‘augmentation’ process in the dense latent space of the
model (Figs. 3 and 4). Each sample that is randomly gen-
erated from the latent space feeds the model regression
layers. Thereby, we propose a model that ﬁrst generates
stochastic variational samples in a dense latent space, and
then, these generated samples act as inputs of the regres-
sion stage of the model.
To test these ideas, the hypothesis considered in this
paper is that the augmented samples will be more accurate
and effective if they are generated in an inner and dense
latent space rather than in a very sparse input space. It is
important to realize that enriching the inner latent space
can improve the recommendation results, but it also injects
noise to the latent space that may potentially worsen the
results. It is expected that the proposed approach will work
better with poor latent spaces, whereas when it is applied to
rich spaces, the spurious entropy added by the variational
stage could worsen recommendations. Thus, medium-size
CF datasets, or large and complex ones are better candi-
dates to improve their results when the variational proposal
is applied, whereas large datasets with predictable data
distributions will probably not beneﬁt from the noise
injection of the variational architecture.
3 Proposed model
The proposed neural architecture will mix the VAE and the
DeepMF (or the NCF) models. From the VAE, we take the
encoder stage and its variational process, and from the
DeepMF or the NCF model, we use its regression layers.
This is an innovative approach in the RS ﬁeld, since the
VAE and GAN neural networks have only been used as a
posteriori stage to make data augmentation, i.e., to obtain
enriched input datasets to feed the CF DeepMF or NCF
models. Hence, the traditional approach needs to separately
train two models, ﬁrst the VAE and then the DeepMF/NCF
networks. As discussed in Sect. 2.3, these combined solu-
tions present important disadvantages in terms of model
complexity,
time
consumption
and
poor
sparsity
management.
In sharp contrast, our proposed approach efﬁciently
joins the VAE and the Deep CF regression concepts to
obtain improved predictions with a single training process.
In the learning stage, the training samples feed the model
(left hand side of Fig. 5). Each training sample consists of
the tuple huser, item, ratingi (rating casted by the user to
the item). In the DeepMF/NCF architecture, each user is
represented by his/her vector of voted ratings, and each
item is represented by its vector of received ratings. The
model learns the ratings (third element in the tuples) casted
by the users to the items (ﬁrst and second elements in the
Fig. 4 Latent space representation of the proposed variational model.
From the learnt means and variances of the multivariate Gaussian
distribution, a random sampling process is run to spread the latent
space sample values (gray circles) that will help to accurately predict
the unknown sample rating values (green circle)
Neural Computing and Applications (2023) 35:7817–7831
7821
123

tuples). In other words, the ratings are outputs of the neural
network (right hand side of Fig. 5).
Thanks to this architecture, the variational stage is nat-
urally embedded into the model, so it can be ﬂexibly used
to inject variability into the samples. It is worth mentioning
that this stage is also trained simultaneously to the pre-
dictive part of the model, mutually inﬂuencing each other,
which we expect will lead to better results than a simple
separate learning.
3.1 Formalization of the model
The architectural details of the proposed models are shown
in Fig. 6. For simplicity, only the Variational Deep Matrix
Factorization (VDeepMF) architecture is shown in this
ﬁgure. The corresponding model for NCF, named Varia-
tional Neural Collaborative Filtering (VNCF), is analogous
to the VDeepMF one: it has the same ‘Embedding’ and
‘Variational’ layers and we should only replace the
‘Dot’ layer of DeepMF by a ‘Concatenate’ layer fol-
lowed by a MLP.
To ﬁx the notation, let us suppose that our dataset
contains U users and I items. In general, the aim of any
deep learning model for CF-based prediction is to train a
(stochastic) neural network that implements a function
h : RU  RI ! R:
This function h operates as follows. Let us codify the u-th
user of the dataset (resp. the i-th item) using one-hot-en-
coding as the u-th canonical basis vector eu (resp. the i-th
canonical basis vector ei). Then, we have that the outcome
of the model is the following
hðeu; eiÞ 2 R
¼ Prediction of the score that the
u0th user would assign to thei0th item:
To train this function h, in the learning phase the neural
network is fed with a set
X ¼ hu; i; ri
f
g
of training tuples hu; i; ri of a user u that rated item i with a
score r. The function h is trained to minimize the error
EðhÞ ¼
X
hu;i;ri2X
dðhðeu; eiÞ; rÞ:
ð1Þ
Here, d : R  R ! R is any metric on R. Typical choices
are the so-called Mean Squared Error (MSE) and Mean
Absolute Error (MAE), respectively, given by
dMSEðx; yÞ ¼ ðx  yÞ2;
dMAEðx; yÞ ¼ jx  yj:
Our proposal for the VDeepMF consist on decomposing h
has a combination of a ‘Embedding’, followed by a
‘Variational’ stage and a ﬁnal ‘Dot’ layer, as shown
in Fig. 6), that is
h ¼ Dot  Variational  Embedding:
Notice that, at the end of the day, h is a deep leaning model
with novel customary layers designed for the RS problem.
In this way, h can be trained to reduce the error EðhÞ of
Eq. (1) with the standard Deep Learning (DL) methods,
such as the backpropagation algorithm.
3.2 The embedding layer
The ﬁrst ‘Embedding’ layer (left hand side of Fig. 6) is
borrowed
from
the
Natural
Language
Processing
(NLP) [13]. The idea is that this layers provides a fast
translation
of
users
and
items
into
their
respective
Fig. 5 Proposed VDeepMF/
NCF approach. CF samples are
encoded in the latent space by
means of a variational process
and then predictions are
obtained by using a regression
neural network
7822
Neural Computing and Applications (2023) 35:7817–7831
123

representations in the latent spaces. To be precise, this
layer implements a linear map
Embedding : RU  RI ! RL  RL;
that maps a pair ðeu; eiÞ into a pair of dense vectors
ðvu; wiÞ 2 RL  RL that represents the u-th user and the i-th
item, being L [ 0 the dimension of the representations.
For our purpose, we implement the ‘Embedding’ layer
as a regular MLP dense layer, in sharp contrast with other
approaches in NLP such as word2vec [37, 38], GloVe [39]
or ELMo [40], among others. The reason is that these later
approaches seek to ﬁnd an embedding preserving some
metric information of the words, typically, the likelihood of
ﬁnding two words together or their semantic similarity.
However, in our case, since we perform context-free CF
prediction, no a priori information about the similarity
between users or items is available. Indeed, this is precisely
the ultimate goal of the RS: to ﬁnd an appropriate repre-
sentations of these users and items in the latent space. For
this reason, we decided not to add any extra mechanism
that could bias this training process, so the ‘Embedding’
layer will act as a regular dense layer to be trained in
parallel during the learning process.
Finally, we would like to point out that, even though
from a conceptual point of view the ‘Embedding’ layer is
just a dense layer, to save time and space, these
‘Embedding’ layers are typically implemented through
lookup tables. In this way, instead of feeding the network
with the one-hot encoding of the user u (resp. the item i),
we input it via its ID as user (resp. as item). The lookup
table efﬁciently recovers the u-th (resp. i-th) column of the
embedding matrix that contains vu (resp. wi) so that the
translation can be conducted in a more efﬁcient way than
with a standard MLP layer by exploiting the sparsity of the
input.
3.3 The variational layer
The variational process is carried out by the ‘Varia-
tional’ stage (labeled as ‘variational layers’ at the
middle of Fig. 6). This is the core of our proposed model.
From the latent space representation ðvu; wiÞ 2 RL  RL
of the u-th user and the i-th item, two separated dense
layers return the mean and variance parameters of two
Gaussian multivariate distribution. In this way, if ﬁx a
latent space dimension K [ 0, the ﬁrst part of this
‘Variational’ stage (left part of the middle rectangle of
Fig. 6) computes a map
Sðvu; wiÞ ¼ ðl1ðvuÞ; r2
1ðvuÞ; l2ðwiÞ; r2ðwiÞÞ 2 R4K:
The outputs l1ðvuÞ; l2ðwiÞ of S will be interpreted as the
means of two Gaussian distributions to the user and the
item, respectively, whereas r2
1ðvuÞ; r2ðwiÞ will represent
variance.
The second part of the ‘Variational’ stage (left
right of the middle rectangle of Fig. 6) is ruled by a pair of
random vectors ðPl1ðvuÞ;r2
1ðvuÞ; Ql2ðwiÞ;r2ðwiÞÞ where
P  N ðl1ðvuÞ; diag r2
1ðvuÞÞ;
Q  N ðl2ðwuÞ; diag r2
2ðwiÞÞ:
Here, N ðl; RÞ denotes a K-dimensional multivariate nor-
mal distribution of mean vector l and diagonal covariance
matrix R, i.e., whose probability density function is
Fig. 6 Proposed VDeepMF
architecture. The NCF
architecture will have identical
‘Embedding’ and ‘Variational’
layers to the VDeepMF one; it
will just replace the ‘Dot’ layer
for a ‘Concatenate’ layer,
followed by an MLP
Neural Computing and Applications (2023) 35:7817–7831
7823
123

fðsÞ ¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2pÞK det R
q
exp  1
2 ðs  lÞtR1ðs  lÞ


:
Notice that, in our case, the covariance matrix is always
diagonal.
In this setting, the task of the ‘Variational’ stage is
just to sample P and Q. In this manner
Variationalðvu; wiÞ ¼ ðp; qÞ 2 RK  RK;
where p is a sample of P ¼ PðSðvu; wiÞÞ  N ðl1ðvuÞ;
diag r2
1ðvuÞÞ and q is a sample of Q ¼ QðSðvu; wiÞÞ 
N ðl2ðwuÞ; diag r2
2ðwiÞÞ. This pair represents the stochastic
latent representations associated with ðvu; wiÞ.
3.4 The join layer
This is the only layer that depends on the particular choice of
the architecture. In the case of the Variational Deep Matrix
Factorization (VDeepMF) architecture, this ﬁnal layer is a
‘Dot’ layer (labeled as ‘regression layer’ at right hand side of
Fig. 6). It is just a linear layer that simply computes the dot
product of the latent vectors p and q. Therefore
Dotðp; qÞ ¼ p  q:
In the case of VNCF, this simple layer is replaced by a
fully connected MLP
H : RK ! RK ! R
that extracts the nonlinear relations from p and q.
Therefore, summarizing the process, the proposed
VDeepMF model h computes
hðeu; eiÞ ¼ Dot  Variational  Embeddingðeu; eiÞ
¼ Pl1ðvuÞ;r2
1ðvuÞ  Ql2ðwiÞ;r2ðwiÞ:
Analogously, the VNCF model returns the proposed
VDeepMF model h computes
hðeu; eiÞ ¼ H  Variational  Embeddingðeu; eiÞ
¼ HðPl1ðvuÞ;r2
1ðvuÞ; Ql2ðwiÞ;r2ðwiÞÞ:
In both cases, hðeu; eiÞ is a random variable that, when
sampled, returns a natural number that should be inter-
preted as the predicted rating by h for the user u regarding
item i.
4 Empirical evaluation
In this section, we describe the empirical experiments
carried out to evaluate the performance of the variational
approach in the DeepMF and NCF models.
4.1 Experimental setup
The experimental evaluation has been performed over four
different datasets to measure the performance of the pro-
posed method over different environments. The selected
datasets are: FilmTrust [41], an small dataset that contains
the ratings of thousands of items to movies; MovieLens
1 M [42], the gold standard dataset in CF-based RS;
MyAnimeList [43], a dataset extracted from Kaggle1 that
contains the ratings of thousands of users to anime comics;
and Netﬂix [44], a popular dataset with hundred of millions
ratings used in the Netﬂix Prize competition. Table 1 shows
the main parameters of these datasets. The corpus of these
datasets has been randomly splitted into training ratings
(80% of the ratings) and test ratings (20% of the ratings).
The evaluation of the proposed method has been ana-
lyzed from three different points of view: the quality of the
predictions [45], the quality of the recommendations [46],
and the quality of the recommendation lists [47].
To measure the quality of the predictions, we have
compared the real rating ru;i of an user u to an item i of the
test split Rtest with the predicted one, ^ru;i. These compar-
isons have been carried out in three ways: using the MAE
as in Eq. (2), using the MSE as in Eq. (3) and computing
the proportion of the explained variance R2 as in Eq. (4).
Notice that, in Eq. (4), r denotes the mean of the ratings
contained in the test split.
MAE ¼
1
#Rtest
X
hu;ii2Rtest
j ru;i  ^ru;i j;
ð2Þ
MSE ¼
1
#Rtest
X
hu;ii2Rtest
ru;i  ^ru;i

2;
ð3Þ
R2 ¼ 1 
X
hu;ii2Rtest
ru;i  ^ru;i

2
X
hu;ii2Rtest
ru;i  r

2 :
ð4Þ
To measure the quality of the recommendations, we have
analyzed the impact of the top N recommended items to the
user u, collected in the list TN
u . Using precision Eq. (5), we
measure the proportion of relevant recommendations (i.e.,
the user rated the item with a rated equal or greater than a
threshold h) among the top N. Here, U denotes the set of
user in the test split. In a similar vein, using recall Eq. (6),
we measure the proportion of the test items rated by the
user u, Rtest
u , that were relevant to him or her and were
included into the recommended items TN
u . For the con-
ducted experiments, the used thresholds are h ¼ 3 for
FilmTrust, h ¼ 4 for MovieLens and Netﬂix, and h ¼ 8 for
MyAnimeList. These thresholds were chosen in agreement
1 www.kaggle.com.
7824
Neural Computing and Applications (2023) 35:7817–7831
123

with the results of [48], where it was shown that these
values represent a fair trade-off between provided coverage
of the dataset and prediction accuracy.
Precision ¼
1
#U
X
u2U
fi 2 TN
u j ru;i  hg
N
;
ð5Þ
Recall ¼
1
#U
X
u2U
fi 2 TN
u j ru;i  hg
fi 2 Rtest
u
j ru;i  hg :
ð6Þ
Additionally, we have measure the quality of the recom-
mendations using the harmonic mean of the precision and
the recall using F1 score Eq. (7).
F1 ¼ 2  Precision  Recall
Precision þ Recall
ð7Þ
However, evaluating the quality of recommendations based
solely on user ratings provides a biased view of the rec-
ommender’s performance. Therefore, we have also deter-
mined the novelty Eq. (8) of the recommendations. Novelty
[49] is calculated by assigning more weight to those items
that have received fewer ratings. In other words, the nov-
elty of an item is inversely proportional to the number of
ratings received for an item (#Ri) with respect to the total
number of votes in the recommender system (#R).
Novelty ¼
1
#U
X
u2U
P
i2TNu  log2
#Ri
#R


N
:
ð8Þ
Finally, to measure the quality of the recommendation lists,
we use the normalized Discounted Cumulative Gain
(nDCG). Suppose that the recommendation list of the user
u, TN
u , is sorted decreasingly so that the items predicted as
more relevant are placed in the ﬁrst positions. Given
i 2 TN
u , let posTNu ðiÞ be the position of the item i in the
recommendation list. Analogously, suppose that the real
top N recommendations to user u, RN
u , as sorted decreas-
ingly and denote by posRNu ðiÞ the position of the item i 2 RN
u
in the list. In this setting, the Discounted Cumulative Gain
(DCG) and the Ideal Cumulative Gain (IDCG) of the user
u 2 U are deﬁned as in Eq. (9).
DCGu ¼
X
i2TNu
2ru;i  1
log2 posTNu ðiÞ þ 1

 ;
IDCGu ¼
X
i2RNu
2ru;i  1
log2 posRNu ðiÞ þ 1

 :
ð9Þ
In this way, nDCG is given by the mean of the ratio
between DCG and IDCG as in Eq. (10).
nDCG ¼
1
#U
X
u2U
DCGu
IDCGu
:
ð10Þ
Due to the stochastic nature of the variational embedded
space of the proposed method, the test predictions used to
evaluate the proposed method have been computed as the
average of the 10 predictions performed for each pair of
user u and item i.
Overall, the proposed variational architecture ade-
quately improves simple models such as the DeepMF one,
approaching their results to larger models such as the NCF.
This tendency can be observed in both predictions and
recommendation quality measures. Additionally, shorter
running times are needed to train the proposed variational
approach compared to baselines. This is the expected
behavior in the hypothesis of the paper, but a remarkable
constraint must be considered: the variational stage works
particularly well when applied to not too large datasets,
whereas using large datasets, the variational approach
could not be necessary. The key idea is the ability of the
Table 1 Main parameters of the
datasets used in the experiments
Dataset
N users
N items
N ratings
Scores
Sparsity (%)
FilmTrust
1508
2071
35,494
0.5 to 4.0
98.86
MovieLens
6040
3706
1,000,209
1 to 5
95.53
MyAnimeList
69,600
9927
6,337,234
1 to 10
99.08
Netﬂix
480,189
17,770
100,480,507
1 to 5
98.82
Table 2 Quality of the predictions
FilmTrust
MovieLens
MyAnimeList
Netﬂix
(a) Mean Absolute Error. The lower the better.
VDeepMF
0.6567.
0.6827
0.8722
0.7176
DeepMF
0.7957
0.6993
0.9044
0.6830
VNCF
0.6410
0.7263
0.9281
0.7474
NCF
0.6361
0.7021
0.8874
0.6903
(b) Mean Squared Error. The lower the better.
VDeepMF
0.7324
0.7529
1.3453
0.8581
DeepMF
1.2046
0.7939
1.5017
0.7789
VNCF
0.6844
0.8179
1.4605
0.8952
NCF
0.6743
0.7908
1.3674.
0.7774
(c) R2 score. The higher the better.
VDeepMF
0.1438
0.3980
0.4549
0.2711
DeepMF
- 0.4082
0.3652
0.3916
0.3384
VNCF
0.1999
0.3460
0.4083
0.2396
NCF
0.2118
0.3677
0.4460.
0.3397
The best results for each quality measure are highlighted in bold
Neural Computing and Applications (2023) 35:7817–7831
7825
123

proposed model to deal with entropy: The variational stage
increases entropy by generating stochastic latent factors
and then enriching the latent space and making it more
robust to the input sample variability. The intrinsic com-
pleteness and continuity properties of the VAE are the
foundations on which the variational approach gets robust,
continuous, and structured latent spaces. These enriched
spaces provide the improved results obtained in the
experiments.
4.2 Experimental results
Table 2 includes the quality of the predictions performed
by the proposed model. Best values for each dataset are
highlighted in bold. Table 2a contains the MAE Eq. (2),
Table 2b contains the MSE Eq. (3), and Table 2c contains
the R2 score Eq. (4). We can observe that the proposed
variational approach improves the prediction capability of
DeepMF in all datasets except of Netﬂix and reports worse
predictions when it is applied to NCF.
We justify these results by taking into account the fea-
tures of the deep learning models used and the properties of
each dataset. On the one hand, the larger the size of the
dataset, the less necessary it is to enrich the votes with the
proposed variational approach. In other words, when the
dataset is small, the amount of Shannon entropy [50] that it
contains might be quite limited. By using a variational
method to generate new samples, we add some extra
entropy that enriches the dataset, giving the chance to the
regressive part of exploiting this extra data. However, large
datasets usually present a large entropy in such a way that
the regressive models can effectively extract very subtle
information from them. In this setting, if we add a varia-
tional stage, instead of adding new relevant variability to
the dataset, we only add noise that muddies the underlying
patterns. For this reason, the variational approach is of no
beneﬁt in huge datasets like Netﬂix.
On the other hand, the NCF model is more complex than
the DeepMF one, so data enrichment has less impact for
complex models that are able to ﬁnd more sophisticated
relationships between data than simpler models. In fact,
Fig. 7 Quality of the recommendations measured by precision and recall. The higher the better
7826
Neural Computing and Applications (2023) 35:7817–7831
123

based on these results, we can assert that including the
variational approach into a simple model such as DeepMF
is equivalent to using a more complex model such as NCF.
Furthermore, Figs. 7 and
8 show the quality of the
recommendations using precision Eq. (5), recall Eq. (6) and
F1 Eq. (7) quality measures. In FilmTrust (Figs. 7a and
8a), MovieLens (Figs. 7b and
8b) and MyAnimeList
(Figs. 7c and
8c), we can observe that the proposed
variational approach reports a beneﬁt for the DeepMF
model and it worsens the results of the NCF model. In
addition, VDeepMF model is the model that computes the
best recommendations for these datasets. In contrast, in
Netﬂix
(Figs. 7d
and
8d),
the
proposed
variational
approach does not improve the quality of the recommen-
dations, with NCF being the model that provides the best
recommendations for this dataset. These results are con-
sistent with those analyzed when measuring the quality of
the predictions. Consequently, it is evident that the pro-
posed variational approach works adequately when the
dataset is not too large and the model used is not too
complex.
Fig. 9 contains the quality of recommendations regard-
ing novelty Eq. (8). It is observed that, when the variational
stage is added to the DeepMF model, a signiﬁcant
improvement of the novelty of the recommendations in
small datasets is achieved. As the dataset becomes larger,
the impact of the variational step is detrimental to the
model. Thus, the variational stage has a positive impact on
the FilmTrust (Fig. 9a) and MovieLens (Fig. 9b) datasets
and a negative impact on the MyAnimeList (Fig. 9c) and
Netﬂix (Fig. 9d) datasets. On the contrary, when a varia-
tional stage is added to the NCF model, its impact on
novelty is practically zero regardless of the dataset size.
This experiment, like the previous ones, reafﬁrms the
conclusion that a variational step improves the results of
simple models on small datasets.
In addition, Fig. 10 contains the nDCG results. From it,
we can observe the same trends as those shown in previous
experiments: in FilmTrust (Fig. 10a), the quality of the
recommendation lists do not vary independently of whether
the variational approach is used or not; in MovieLens
(Fig. 10b) and MyAnimeList (Fig. 10c), the combination
of the variational approach with simple modeling such as
DeepMF,
provides
the
best
results;
and
in
Netﬂix
(Fig. 10d), the variational approach signiﬁcantly worsens
the quality of the recommendation lists.
Fig. 8 Quality of the recommendations measured by F1. The higher the better
Neural Computing and Applications (2023) 35:7817–7831
7827
123

5 Conclusions
In the latest trends, accuracy of RSs is being improved by
using deep learning models such as deep matrix factor-
ization and neural collaborative ﬁltering. However, these
models do not incorporate stochasticity in their design,
unlike variational autoencoders do. Variational random
sampling has been used to create augmented input raw data
in the collaborative ﬁltering context, but the inherent col-
laborative ﬁltering data sparsity makes it difﬁcult to get
accurate results. This paper applies the variational concept
not to generate augmented sparse data, but to create aug-
mented samples in the latent space codiﬁed at the dense
inner layers of the proposed neural network. This is an
innovative approach trying to combine the potential of the
variational stochasticity with the augmentation concept.
Augmented samples are generated in the dense latent space
of the neural network model. In this way, we avoid the
sparse scenario in the variational process.
Observe that the proposed model in this paper also
encodes the intrinsic locality of the users and items in the
latent space. Recall that regular MF models capture the
similarity of users and items in the latent space since pre-
dictions are constructed via inner product, a continuous
function. In the same spirit, our variational models also
preserve this locality since the output is still computed
through a continuous function: the feed-forward neural
network, a much more complicated function, but eventu-
ally continuous. Moreover, since the probability distribu-
tions representing each user and item in the latent space
depend on continuous parameters (the mean and standard
deviation of a Gaussian distribution), small variations in
these parameters, corresponding to similar items or users,
are also encoded as almost equal distributions, and thus,
their samples tend to be also close in the distributional
sense.
Thanks to these ideas, the results of the experimental
analyses conducted in this paper show an important
improvement when the proposed models are applied to
middle-size representative collaborative ﬁltering datasets,
compared to the state-of-the-art baselines, testing both
prediction and recommendation quality measures. In sharp
contrast, testing on the huge Netﬂix dataset not only leads
to no improvement, but the recommendation quality
Fig. 9 Quality of the recommendations measured by novelty. The higher the better
7828
Neural Computing and Applications (2023) 35:7817–7831
123

actually gets worse. In this manner, increasing the Shannon
entropy in rich latent spaces causes that the negative effect
of the introduced noise exceeds its beneﬁt. Therefore, the
proposed deep variational models should be applied to seek
to a fair balance between their positive enrichment and
their negative noise injection.
To emphasize this idea, in Table 3, we show the total
time and epochs required by each model to be ﬁtted to each
dataset using a Quadro RTX 8000 GPU. Best time for each
dataset is in bold. We can observe that including a varia-
tional layer to the model signiﬁcantly reduces the required
time for ﬁtting. Variational models are able to generate
Shannon entropy that is transferred to the regression stage,
leading to a more effective training that requires fewer
epochs to be ﬁtted. Therefore, the ﬁtting time needed to
reach acceptable results is substantially lower.
The results presented in this work can be considered as
generalizable, since they were analyzed in four represen-
tative and open CF datasets. Researchers can reproduce our
experiments and easily create their own models by using
the provided framework referenced in Sect. 3. The authors
of this work are committed to reproducible science, so the
code used in these experiments is publicly available.
Among the most promising future works, we propose
the following: (1) introducing the variational process in the
alternative inner layers of the relevant architectures in the
collaborative ﬁltering area, (2) screening the learning
evolution in the training process, since it is faster than the
classical models but it also requires early stopping in the
Fig. 10 Quality of the recommendations lists measured by NDCG. The higher the better
Table 3 Fitting time using a
Quadro RTX 8000
FilmTrust
MovieLens
MyAnimeList
Netﬂix
VDeepMF
61s (15 epochs)
601s (6 epochs)
7629s (9 epochs)
12655s (3 epochs).
DeepMF
75s (25 epochs)
677s (10 epochs)
13217s (20 epochs)
15697s (4 epochs)
VNCF
35s (7 epochs)
1030s (9 epochs)
9945s (9 epochs)
12650s (3 epochs)
NCF
56s (15 epochs)
876s (10 epochs)
12111s (15 epochs)
16896s (4 epochs)
Best ﬁtting times for each datased in bold
Neural Computing and Applications (2023) 35:7817–7831
7829
123

training stage, (3) providing further theoretical explana-
tions of the properties of the CF datasets, in terms of
Shannon entropy or other statistical features, that ensure a
good performance of the proposed models, (4) applying
probabilistic deep learning models in the CF ﬁeld to cap-
ture complex nonlinear stochastic relationships between
random variables, and (5) testing the impact of the pro-
posed concept when recommendations are made to groups
of users.
Acknowledgements A´ . G.-P. acknowledges the hospitality of the
Department of Mathematics at Universidad Auto´noma de Madrid
where part of this work was developed. This work was partially
supported by Ministerio de Ciencia e Innovacio´n of Spain under the
project PID2019-106493RB-I00 (DL-CEMG) and the Comunidad de
Madrid under Convenio Plurianual with the Universidad Polite´cnica
de Madrid in the actuation line of Programa de Excelencia para el
Profesorado Universitario. The forth author has been partially sup-
ported by the Madrid Government (Comunidad de Madrid – Spain)
under the Multiannual Agreement with the Universidad Complutense
de Madrid in the line Research Incentive for Young PhDs, in the
context of the V PRICIT (Regional Programme of Research and
Technological Innovation) through the project PR27/21-029.
Funding Open Access funding provided thanks to the CRUE-CSIC
agreement with Springer Nature.
Data availability The datasets analyzed during the current study are
available in the repositories referred in the references [41–44].
Declarations
Conflict of interest The authors declare that they have no conflict of
interest.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indicate
if changes were made. The images or other third party material in this
article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not
included in the article’s Creative Commons licence and your intended
use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright
holder. To view a copy of this licence, visit http://creativecommons.
org/licenses/by/4.0/.
References
1. Beel J, Langer S, Genzmehr M, Gipp B, Breitinger C, Nu¨rnberger
A (2013) Research paper recommender system evaluation: a
quantitative literature survey. In: Proceedings of the international
workshop on reproducibility and replication in recommender
systems evaluation, pp 15–22
2. Bobadilla J, Gonza´lez-Prieto A´ , Ortega F, Lara-Cabrera R (2021)
Deep learning feature selection to unhide demographic recom-
mender systems factors. Neural Comput Appl 33(12):7291–7308
3. Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender
systems leveraging multimedia content. ACM Comput Surveys
(CSUR) 53(5):1–38
4. Kulkarni S, Rodd SF (2020) Context aware recommendation
systems: a review of the state of the art techniques. Comput Sci
Rev 37:100255
5. Shokeen J, Rana C (2020) A study on features of social recom-
mender systems. Artif Intell Rev 53(2):965–988
6. Bobadilla J, Alonso S, Hernando A (2020) Deep learning archi-
tecture for collaborative ﬁltering recommender systems. Appl Sci
10(7):2441
7. Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of
a recommender system with ensemble learning and graph
embedding:
a
case
on
movielens.
Multimed
Tools
Appl
80(5):7805–7832
8. C¸ ano E, Morisio M (2017) Hybrid recommender systems: a
systematic literature review. Intell Data Anal 21(6):1487–1524
9. Mnih A, Salakhutdinov RR (2007) Probabilistic matrix factor-
ization. Adv Neural Inf Process Syst 20:1257–1264
10. Fe´votte C, Idier J (2011) Algorithms for nonnegative matrix
factorization
with
the
b-divergence.
Neural
Comput
23(9):2421–2456
11. Hernando A, Bobadilla J, Ortega F (2016) A non negative matrix
factorization for collaborative ﬁltering recommender systems
based on a bayesian probabilistic model. Knowl-Based Syst
97:188–202
12. Rendle S, Krichene W, Zhang L, Anderson J (2020) Neural
collaborative ﬁltering vs. matrix factorization revisited. In:
Fourteenth
ACM
conference
on
recommender
systems,
pp 240–248
13. He X, Liao L, Zhang H, Nie L, Hu X, Chua T-S (2017) Neural
collaborative ﬁltering. In: Proceedings of the 26th international
conference on world wide web, pp 173–182
14. Narang S, Taneja N (2018) Deep content-collaborative recom-
mender system (dccrs). In: 2018 international conference on
advances in computing, communication control and networking
(ICACCCN), pp 110–116. IEEE
15. Bobadilla J, Lara-Cabrera R, Gonza´lez-Prieto A´ , Ortega F (2021)
Deepfair: deep learning for improving fairness in recommender
systems. Int J Interact Multimed Artif Intell 6(6):86–94
16. Gao M, Zhang J, Yu J, Li J, Wen J, Xiong Q (2021) Recom-
mender systems based on generative adversarial networks: a
problem-driven perspective. Inf Sci 546:1166–1185
17. Xue H-J, Dai X, Zhang J, Huang S, Chen J (2017) Deep matrix
factorization models for recommender systems. In: IJCAI, Mel-
bourne, Australia, vol 17, pp 3203–3209
18. Wen J, She J, Li X, Mao H (2018) Visual background recom-
mendation for dance performances using deep matrix factoriza-
tion. ACM Trans Multimed Comput Commun Appl (TOMM)
14(1):1–19
19. Wan L, Xia F, Kong X, Hsu C-H, Huang R, Ma J (2020) Deep
matrix factorization for trust-aware recommendation in social
networks. IEEE Trans Network Sci Eng 8(1):511–528
20. Zou G, Chen J, He Q, Li K-C, Zhang B, Gan Y (2020) Ndmf:
Neighborhood-integrated deep matrix factorization for service
qos prediction. IEEE Trans Netw Serv Manage 17(4):2717–2730
21. Trigeorgis G, Bousmalis K, Zafeiriou S, Schuller BW (2016) A
deep matrix factorization method for learning attribute repre-
sentations. IEEE Trans Pattern Anal Mach Intell 39(3):417–429
22. Fan J, Cheng J (2018) Matrix completion by deep matrix fac-
torization. Neural Netw 98:34–41
23. Liu X, Gherbi A, Wei Z, Li W, Cheriet M (2020) Multispectral
image reconstruction from color images using enhanced varia-
tional autoencoder and generative adversarial network. IEEE
Access 9:1666–1679
7830
Neural Computing and Applications (2023) 35:7817–7831
123

24. Liu Z-S, Siu W-C, Wang L-W, Li C-T, Cani M-P (2020)
Unsupervised real image super-resolution via generative varia-
tional autoencoder. In: Proceedings of the IEEE/CVF conference
on
computer
vision
and
pattern
recognition
workshops,
pp 442–443
25. Liu Z-S, Siu W-C, Chan Y-L (2020) Photo-realistic image super-
resolution via variational autoencoders. IEEE Trans Circ Syst
Video Technol 31(4):1351–1365
26. Zhang S-s, Liu J-w, Zuo X, Lu R-k, Lian S-m (2021) Online deep
learning based on auto-encoder. Appl Intell 51(8):5420–5439
27. Liang D, Krishnan RG, Hoffman MD, Jebara T (2018) Varia-
tional autoencoders for collaborative ﬁltering. In: Proceedings of
the 2018 world wide web conference, pp 689–698
28. Nisha C, Mohan A (2019) A social recommender system using
deep
architecture
and
network
embedding.
Appl
Intell
49(5):1937–1953
29. Rama K, Kumar P, Bhasker B (2021) Deep autoencoders for
feature learning with embeddings for recommendations: a novel
recommender
system
solution.
Neural
Comput
Appl
33(21):14167–14177
30. Tahmasebi H, Ravanmehr R, Mohamadrezaei R (2021) Social
movie recommender system based on deep autoencoder network
using twitter data. Neural Comput Appl 33(5):1607–1623
31. Li X, She J (2017) Collaborative variational autoencoder for
recommender systems. In: Proceedings of the 23rd ACM
SIGKDD international conference on knowledge discovery and
data mining, pp 305–314
32. He M, Meng Q, Zhang S (2019) Collaborative additional varia-
tional autoencoder for top-n recommender systems. IEEE Access
7:5707–5713
33. Nahta R, Meena YK, Gopalani D, Chauhan GS (2021) Two-step
hybrid collaborative ﬁltering using deep variational bayesian
autoencoders. Inf Sci 562:136–154
34. Shenbin I, Alekseev A, Tutubalina E, Malykh V, Nikolenko SI
(2020) Recvae: a new variational autoencoder for top-n recom-
mendations with implicit feedback. In: Proceedings of the 13th
international conference on web search and data mining,
pp. 528–536
35. Wang K, Xu L, Huang L, Wang C-D, Lai J-H (2019) Sddrs:
stacked discriminative denoising auto-encoder based recom-
mender system. Cogn Syst Res 55:164–174
36. Liu Y, Wang S, Khan MS, He J (2018) A novel deep hybrid
recommender system based on auto-encoder with neural collab-
orative ﬁltering. Big Data Mining Anal 1(3):211–221
37. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013)
Distributed representations of words and phrases and their com-
positionality. Adv Neural Inform Process Syst 26
38. Mikolov T, Chen K, Corrado G, Dean J (2013) Efﬁcient esti-
mation of word representations in vector space. arXiv preprint
arXiv:1301.3781
39. Pennington J, Socher R, Manning CD (2014) Glove: Global
vectors for word representation. In: Proceedings of the 2014
conference on empirical methods in natural language processing
(EMNLP), pp 1532–1543
40. Peters M, Neumann M, Iyyer M, Gardner M, Clark C, Lee K,
Zettlemoyer L (1802) Deep contextualized word representations.
2018. arXiv preprint arXiv:1802.05365
41. Guo G, Zhang J, Yorke-Smith N (2013) A novel bayesian simi-
larity measure for recommender systems. In: Proceedings of the
23rd international joint conference on artiﬁcial intelligence
(IJCAI), pp 2619–2625
42. Harper FM, Konstan JA (2015) The movielens datasets: history
and context. Acm Trans Interact Intell Syst (tiis) 5(4):1–19
43. Azathoth: MyAnimeList Dataset. https://www.kaggle.com/aza
thoth42/myanimelist. [Online; accessed 06-July-2021] (2018)
44. Bennett J, Lanning S et al (2007) The netﬂix prize. In: Pro-
ceedings of KDD Cup and Workshop, New York, NY, USA, vol
2007, p 35.
45. Bobadilla J, Hernando A, Ortega F, Bernal J (2011) A framework
for collaborative ﬁltering recommender systems. Expert Syst
Appl 38(12):14609–14623
46. Herlocker J-L, Konstan J-A, Terveen L-G, Riedl J-T (2004)
Evaluating collaborative ﬁltering recommender systems. ACM
Trans Inf Syst 22(1):5–53
47. Gunawardana A, Shani G (2015) Evaluating recommender sys-
tems. Handbook, Boston, MA
48. Ortega F, Lara-Cabrera R, Gonza´lez-Prieto A´ , Bobadilla J (2021)
Providing reliability in recommender systems through bernoulli
matrix factorization. Inf Sci 553:110–128
49. Castells P, Vargas S, Wang J (2011) Novelty and diversity
metrics for recommender systems: choice, discovery and rele-
vance. In: Proceedings of the 33rd European conference on
information retrieval (ECIR’11)
50. Shannon CE, Weaver W (1949) The mathematical theory of
communication. University of Illinois Press, Urbana
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Neural Computing and Applications (2023) 35:7817–7831
7831
123
"
Neural group recommendation based on a probabilistic semantic aggregation.pdf,"ORIGINAL ARTICLE
Neural group recommendation based on a probabilistic semantic
aggregation
Jorge Duen˜as-Lerı´n2,3
• Rau´l Lara-Cabrera1,2 • Fernando Ortega1,2 • Jesu´s Bobadilla1,2
Received: 28 July 2022 / Accepted: 13 February 2023 / Published online: 22 March 2023
 The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2023
Abstract
Recommendation to groups of users is a challenging subﬁeld of recommendation systems. Its key concept is how and
where to make the aggregation of each set of user information into an individual entity, such as a ranked recommendation
list, a virtual user, or a multi-hot input vector encoding. This paper proposes an innovative strategy where aggregation is
made in the multi-hot vector that feeds the neural network model. The aggregation provides a probabilistic semantic, and
the resulting input vectors feed a model that is able to conveniently generalize the group recommendation from the
individual predictions. Furthermore, using the proposed architecture, group recommendations can be obtained by simply
feedforwarding the pre-trained model with individual ratings; that is, without the need to obtain datasets containing group
of user information, and without the need of running two separate trainings (individual and group). This approach also
avoids maintaining two different models to support both individual and group learning. Experiments have tested the
proposed architecture using three representative collaborative ﬁltering datasets and a series of baselines; results show
suitable accuracy improvements compared to the state of the art.
Keywords Group recommender system  Collaborative ﬁltering  Aggregation models  Deep learning
1 Introduction
Personalization is one of the ﬁelds of Artiﬁcial Intelligence
(AI) that has a greater impact on the lives of individuals.
We can ﬁnd a multitude of services that provide us with a
personalized choice of news, videos, songs, restaurants,
clothes, travels, etc. The most relevant tech companies
make extensive use of personalization services: Amazon,
Netﬂix, Spotify, TripAdvisor, Google, TikTok, etc. These
companies generate their personalized recommendations
using Recommender System (RS) [1] applications. Rec-
ommender System (RS) provides to their users personal-
ized products or services (items) by ﬁltering the most
relevant information regarding the logs of items consumed
by the users, the time and place that took place, as well as
the existing information about users, their social networks,
and the content of items (texts, pictures, videos, etc.). We
can classify Recommender System (RS) attending to their
ﬁltering strategy as demographic [2], content-based [3],
context-aware [4],
social [5],
Collaborative
Filtering
(CF) [6, 7] and ﬁltering ensembles [8, 9]. Currently, the
Matrix Factorization (MF) [10] machine learning model is
used to obtain accurate and fast recommendations between
input data (votes). Matrix Factorization (MF) translates the
very sparse and huge matrix of discrete votes (from users to
items) into two dense and relatively small matrices of real
values. One of the matrices contains the set of short and
dense vectors representing users, whereas the second
matrix vectors represent items. Each vector element (real
Jorge Duen˜as-Lerı´n, Rau´l Lara-Cabrera, Fernando Ortega
and Jesu´s Bobadilla have contributed equally to this work.
& Jorge Duen˜as-Lerı´n
jorgedl@alumnos.upm.es
Rau´l Lara-Cabrera
raul.lara@upm.es
Fernando Ortega
fernando.ortega@upm.es
Jesu´s Bobadilla
jesus.bobadilla@upm.es
1
Departamento de Sistemas Informa´ticos, Universidad
Polite´cnica de Madrid, Alan Turing s/n, 28031 Madrid, Spain
2
KNODIS Research Group, Universidad Polite´cnica de
Madrid, 28031 Madrid, Spain
3
Universidad Polite´cnica de Madrid, Madrid, Spain
123
Neural Computing and Applications (2023) 35:14081–14092
https://doi.org/10.1007/s00521-023-08410-6
(0123456789().,-volV)(0123456789().
,- volV)

value) is called the ‘hidden factor value’, since it represents
some complex and unknown relationship between the input
data (votes).
However, Matrix Factorization (MF) machine learning
models are fast and accurate, and they also present a
remarkable drawback: They cannot detect, in their hidden
factors, the complex nonlinear relationships between the
input data. Neural Network (NN) can solve this problem
through their nonlinear activation functions. Neural Net-
work (NN)-based Recommender System (RS) [2, 11]
makes a compression of information by coding the patterns
of the rating matrix in their embeddings and hidden lay-
ers [12]. These embeddings play the role of the Matrix
Factorization (MF) hidden factors, enriching the result by
incorporating non-linear relations. The most well-known
Neural Network (NN) base Recommender System (RS)
approaches are Generalized Matrix Factorization (GMF)
and Multi-Layer Perceptron (MLP) [13].
Group Recommendation (GR) [7, 14] is a subﬁeld of the
Recommender System (RS) area where recommendations
are made to sets of users instead of to individual users (e.g.,
to recommend a movie to a group of three friends). As in
the regular Recommender System (RS), the goal is to make
accurate recommendations to the group. In this case, sev-
eral policies can be followed; the most popular are (a) to
minimize the mean accuracy error: to recommend the items
that, on average, most like to all the group members, and
(b) to minimize the maximum accuracy error: to recom-
mend the items that does not excessively dislike to any of
the group members. It is important to state that there are
not open datasets containing group information to be used
by group recommendation models; for this reason, gener-
ally randomly generated groups are used for training and
testing research models.
Regardless of the machine learning approach used to
implement Group Recommendation (GR) Recommender
System (RS), the most notable design concept is to estab-
lish where to locate the aggregation stage to convert indi-
vidual information to group information. The general rule
is the sooner the aggregation stage, the better the perfor-
mance of Group Recommendation (GR) [14]. There are
three different locations where group information can be
aggregated into a uniﬁed group entity: (a) before the
model, (b) in the model, and (c) after the model. The most
intuitive approach is to combine individual recommenda-
tions into a uniﬁed group recommendation (option c) [15].
This approach is known as Individual Preference Aggre-
gation (IPA) and requires processing several individual
recommendations followed by rank aggregation. However,
the process is slow and not particularly accurate. On the
other hand, to consider the entire group for the recom-
mendation, we should work before or inside the model
(options a or b). These approaches are known as Group
Preference Aggregation (GPA). Aggregating group infor-
mation before the model requires working with the user-
item interaction matrix in a higher-dimensional space,
which can lead to misinformation problems. To aggregate
group information in the model, we need to work with the
user‘s hidden vector in the low-dimensional space.
Aggregating several hidden vectors from individual
users into a uniﬁed virtual user hidden vector [16] avoids
compute the model predictions several times and makes the
rank aggregation stage unnecessary. In addition, it takes
advantage of operating with condensed information com-
ing from the Matrix Factorization (MF) compression of
information: the virtual user can be obtained simply by
averaging the representative short and dense vectors of the
users group; this is efﬁcient and accurate. An interesting
question is can Neural Network (NN) operate the same way
that Matrix Factorization (MF) does to obtain virtual users
and generate recommendations? First, note that many
Neural Network (NN)-based Recommender System (RS)
models compress the user embedding in a different latent
space than the item embedding, and it can be a problem;
then, the Neural Network (NN) nonlinear ensemble repre-
sentations are more complex than the Matrix Factorization
(MF) hidden factor representations; consequently, simply
averaging the ensembles of the users in the group does not
automatically ensure a representative virtual user embed-
ding. Furthermore, model-based aggregations (option ‘b’ in
the previous paragraph) are model dependent, and then, it
is necessary to design and test different solutions for dif-
ferent Neural Network (NN)-based Recommender System
(RS) models, whereas the Neural Network (NN) latent
spaces are the state of the art to catch users and items
relations, some other machine learning approaches have
been designed, such as the use of the random walk with
restart method [17] providing a framework to relate users,
items, and groups, and to exploit the item content and the
proﬁles of the users. A three-stage method [18] is proposed
to increase the precision and fairness of Group Recom-
mendation (GR), where binary Matrix Factorization (MF),
graphs and the dynamic consensus model are processed
sequentially. Some relevant and current GR research aims
to make use of the concept of member preference (inﬂu-
ence or expertise) concept, based on similarity and trust.
The key idea is to detect the group leaders as group
members that are trusted more than others and have more
inﬂuence than others. In [19], fuzzy clustering and an
implicit trust metric are combined to ﬁnd neighborhoods.
Group Recommendation (GR) based on an average strategy
applied to user preference differences [20] has been com-
bined with trusted social networks to correct recommen-
dations. An aggregation approach for GR mimics crowd-
sourcing concepts to estimate the level of expertise of
group members [21]; it is implemented using parameters of
14082
Neural Computing and Applications (2023) 35:14081–14092
123

sensitivity and speciﬁcity. The impact of social factors on a
Group Recommendation (GR) computational model is
evaluated in [22], using the expertise factor, the inﬂuences
of personality, preference similarities, and interpersonal
relationships.
In this paper, we present how we can generate Group
Recommendation (GR) using Neural Network (NN)-based
Recommender System (RS) by training the model using
raw Collaborative Filtering (CF) data (i.e., the ratings of
individual users to the items without any additional infor-
mation). The Group Recommendation (GR) have been
tested using the two most popular implementations of
Neural Network (NN)-based Recommender System (RS):
Generalized Matrix Factorization (GMF) and Multi-Layer
Perceptron (MLP). As stated previously, to make recom-
mendations to groups using Neural Network (NN)-based
Recommender System (RS), information of the individual
users must be aggregated. The chosen information aggre-
gation design is to merge the users of the group in the input
vector that feeds the user embedding of the Neural Net-
work (NN). This aggregation design is not novel, since it
has been used by [23] applied to a Multi-Layer Perceptron
(MLP) architecture. However, our approach combines
several innovative aspects in comparison with the state of
the art. On the one hand, the aggregation of the users in the
group is a probabilistic function rather than a simple multi-
hot encoding [23]; this better captures the relative impor-
tance of users in the input vector that feeds the Neural
Network (NN), moreover: This aggregation approach
serves as front-end for any Neural Network (NN) Group
Recommendation (GR) model. On the other hand, we
propose the use of a simple Recommender System (RS)
Neural Network (NN) model (Generalized Matrix Factor-
ization (GMF)) instead of the deepest Multi-Layer Per-
ceptron (MLP) one [23]; the hypothesis is that complex
models overﬁt Group Recommendation (GR) scenarios,
since they are designed to accurately predict individual
predictions, whereas Group Recommendation (GR) must
satisfy an average of the tastes in the group of users, that is,
Group Recommendation (GR) should be designed to gen-
eralize the set of individual tastes in the group. Further-
more, the proposed architecture just needs a single training
to provide both individual recommendations and group
recommendations; particularly, the model is trained by
only using individual recommendations (as in regular
Recommender System (RS)). Once the model is trained to
return individual predictions, we can ﬁll the input vector by
aggregating all the users in the group, then feedforward the
trained model and ﬁnally obtain the recommendation for
the group of users. Anyway, the impact of these innovative
aspects can be evaluated in Sect. 3, where we empirically
compare the proposed aggregation designs with respect to
the main baseline [23]
In summary, the Group Recommendation (GR) state of
the art presents the following drawbacks: (a) Some
research relies on additional data to the Collaborative Fil-
tering (CF) ratings, such as trust or reputation information
that is not available on the majority of datasets, (b) differ-
ent proposals make the aggregation of individual users
before (Individual Preference Aggregation (IPA)) or after
(Ranking) the model, making it impossible to beneﬁt from
the machine learning model inner representations (Group
Preference Aggregation (GPA)), and (c) The proposed GR
neural model solutions tend to apply architectures designed
to make individual recommendations, rather than group
ones; this leads to the model overﬁtting and to a low
scalability referred to the number of users in a group. To
ﬁll the gap, our proposed model: (a) Acts exclusively on
Collaborative Filtering (CF) ratings, (b) Makes user
aggregation in the model, and (c) Its model depth and
design enable adequate learning generalizations. Addi-
tionally, the provided experiments test the proposed model
according to different aggregation strategies to set the
group labels used in the learning stage. In contrast, a
notable limitation of our architecture and the experiments
is the lack of testing on particularly demanding scenarios
such as cold start in groups users, extremely sparse data
sets, impact of popular item bias, and fear Group Recom-
mendation (GR).
The rest of the paper is structured as follows: Sect. 2
introduces the tested models and aggregation functions;
Sect. 3 describes the experiment design, the selected
quality measures, the chosen datasets and shows the results
obtained; Sect. 4 provides their explanations; and Sect. 5
highlights the main conclusions of the article and the
suggested future work.
2 Proposed model
In Collaborative Filtering (CF), interactions (purchase,
viewing, rating, etc.) between users and items are stored in
a sparse matrix since it is common for users to interact only
with a small proportion of the available items and, in the
same way, only a small percentage of existing users
interact with the items. The sparsity levels of this matrix
are around 95–98% as shown in Table 2. To handle this
sparsity, current Collaborative Filtering (CF) models based
on Neural Network (NN) [13] work with a projection of
users and items into a low-dimensional latent space using
embedding layers. Embedding layers are a very popular
type of layer used in Neural Network (NN) that receive as
input any entity and return a vector with a low-dimensional
representation of the entity in a latent space. These vectors
are commonly named latent factors. In order to transform
the entity into its low-dimensional representation, the
Neural Computing and Applications (2023) 35:14081–14092
14083
123

embedding layer ﬁrst transforms the entity into a one hot
encoding representation (typically using a hash function).
Figure 1 summarizes this process.
In the context of Collaborative Filtering (CF), two
embedding layers are required: one for the users and the
other for the items. Later, both embedding layers are
combined using a Neural Network (NN) architecture
(see Fig. 2). For example, the models Generalized Matrix
Factorization (GMF) and Multi-Layer Perceptron (MLP)
use a dot layer and a concatenate layer followed by some
fully connected dense layers as architectures, respectively.
Formally, we deﬁne a Neural Network (NN) model U
that predicts the rating that a user u will give to an item i
(^ru;i) combining the latent factors provided by the embed-
ding layer (EmbL) of the user u (lu~) and the item i (li~):
EmbLðuÞ ¼ lu~
EmbLðiÞ ¼ li~;
Uðl~u; l~iÞ ¼ ^ru;i
ð1Þ
As stated in Sect. 1, when working with Group Recom-
mendation (GR), a straightforward strategy is Individual
Preference Aggregation (IPA) [24]. This strategy makes a
prediction for each member of the group and then performs
an aggregation. This strategy does not treat the group as a
whole. If we have a group of users G ¼ fu1; u2; :::; ung, the
prediction of the rating of this group G to an item i (^rG;i) is
computed
as
the
average
value
of
the
individual
predictions:
^rG;i ¼
1
kGk
X
u2G
^ru;i ¼
1
kGk
X
u2G
Uðlu~; li~Þ
ð2Þ
On the other hand, the Group Preference Aggregation
(GPA) strategies take into account the group as a whole. It
should be noted that the order of users within the group and
the length of it should not affect the aggregation; thus, the
aggregations should meet the constraints of permutation
invariant and ﬁxed result length [23]. Our goal with the
Group Preference Aggregation (GPA) strategy is to be able
to obtain a prediction ^rG;i with a single forward propagation
and to treat the group as a whole entity. We can achieve
this by aggregating the latent factors of each user that
belongs to the group to obtain the latent factor of the group
l~G. Once the latent factors of the group are aggregated, the
model U can be used to compute the predictions:
EmbLðGÞ ¼ l~G
^rGi ¼ Uðl~G; l~iÞ
ð3Þ
The aggregation of group latent factors in embedding
layers can be achieved by modifying the input of the
Neural Network (NN). As mentioned previously, embed-
ding layers have as input a one hot representation of the
entities. This approach is adequate when performing indi-
vidual predictions, however, for group recommendations,
we need to apply a multi-hot representation to the users’
embedding layer, i.e., we encode the group by setting
multiple inputs of the user embedding layer (the inputs
related with the users that belong to the group) to a value
higher than 0. This encoding allows us to take into account
all group users at the same time for the extraction of latent
factors of the group l~G.
The
simplest
aggregation,
which
is
used
by
the
DeepGroup model [23], is to use as input for embedding a
constant value proportional to the size of the group. We
deﬁne the input of the user’s embedding layer for the user u
as
EmbeddingInputAverageðuÞ ¼
1
kGk
if
u 2 G
0
if
u 62 G
8
<
:
ð4Þ
We call this aggregation ‘Average’ since the embedding
layer will generate the group latent factor equal to the
average of the latent factors of all users in the group.
Recommender System (RS) can give better predictions
the more information they have about users, so to take
advantage of this fact, we have tested the ‘Expertise’
aggregation in which we give a weight to the users pro-
portional to the number of votes they have entered into the
system. Let kRuk the number of ratings of the user u, the
input of the users’ embedding layer for the user u is deﬁned
as
Fig. 1 Embedding layer schema
Fig. 2 Collaborative ﬁltering-based neural network model
14084
Neural Computing and Applications (2023) 35:14081–14092
123

EmbeddingInputExpertiseðuÞ ¼
kRuk
P
g2G kRgk
if
u 2 G
0
if
u 62 G
8
<
:
ð5Þ
In addition to the ‘Expertise’ aggregation, we also pro-
posed the ‘Softmax aggregation as a smooth version of the
‘Expertise’ aggregation. In this case, the input of the users’
embedding layer for the user u is deﬁned as
EmbeddingInputSoftmaxðuÞ ¼
ekRuk
P
g2G ekRgk
if
u 2 G
0
if
u 62 G
8
>
<
>
:
ð6Þ
In Fig. 3, we can see where the equations ﬁt in the group
recommendation process. The ﬁrst step is to generate the
multi-hot vector with some of the described aggregation
(Eqs. 4, 5, 6). This vector (multi-hot representation of the
group) is fed into the embedding layer to obtain a vector of
the latent factors of the groups l~G (Eq. 3). Once the latent
Fig. 3 Graphical representation of the proposed model
Table 1 Complete aggregation example
(a) Rating count.
User
.
u13
.
u24
.
u30
.
u42
.
#Rating
2
5
6
3
(b) Input values to the users’ embedding layer.
Strategy/
User
.
u13
.
u24
.
u30
.
u42
.
Average
0,25
0,25
0,25
0,25
Expertise
0,13
0,31
0,38
0,19
Softmax
0,01
0,26
0,70
0,03
Users’ latent factors assuming a latent space of size 3.
User/factor
l1
l2
l3
u13
0,1
0,6
0,3
u24
0,7
0,2
0,9
u30
0,8
0,4
0,1
u42
0,5
0,7
0,8
Group latent factors using different aggregations
Agg factor
lG1
lG2
lG3
Average
0,525
0,475
0,525
Expertise
0,629
0,425
0,508
Softmax
0,758
0,359
0,331
Neural Computing and Applications (2023) 35:14081–14092
14085
123

factors of the group and the item are obtained, they are
used to feed the model U (Generalized Matrix Factoriza-
tion (GMF) or Multi-Layer Perceptron (MLP)) and produce
the rating prediction for the group G on the item i (Eq. 2).
In Table 1, we can ﬁnd an example with some users (13,
24, 30 and 42) with different rating counts (Table 1a), their
input values to the users’ embedding layer in a multi-hot
fashion
(Table
1b),
their
individual
latent
factors
(Table 1c), and the ﬁnal group latent factors with different
aggregations (Table 1d).
3 Experimental evaluation
In this section, we show the experiments carried out to
validate the aggregation proposed in this manuscript. As
previously stated, the experiments have been performed
using the most popular Neural Network (NN)-based Rec-
ommender System (RS) architectures: Generalized Matrix
Factorization (GMF) and Multi-Layer Perceptron (MLP).
We have chosen these two architectures because they are
the best known and offer the best results for individual
predictions. However, the aggregation strategies proposed
can be applied to any Neural Network (NN) architecture
based on embedding layers.
The choice of datasets has been made considering that
(a) there are no open datasets containing information on
group voting; and (b) Generalized Matrix Factorization
(GMF) and Multi-Layer Perceptron (MLP) models should
be trained using individual voting, since the proposed
aggregations allow computing predictions for groups on
already trained models. For these reasons, we have chosen
the following gold standard datasets in the ﬁeld of Rec-
ommender System (RS): MovieLens1M [25], the most
popular dataset in the research of Recommender System
(RS);
FilmTrust [26],
a
dataset
smaller
than
MovieLens1M to measure the performance of the
aggregation in datasets with a lower number of users,
items, and ratings; and MyAnimeList, a dataset with a
range of votes higher than the MovieLens1M. Other
popular
datasets
such
as
Netﬂix
Prize
or
MovieLens10M have not been selected due to the high
computational time required to train and test the models.
The main parameters of the selected datasets can be found
in Table 2.
The generation of synthetic groups has been carried out
in such a way that all groups have voted at least ﬁve items
in test. In this way, it is possible to evaluate both the
quality of the predictions and the quality of the recom-
mendations to the groups as detailed below. Groups of
different sizes (from 2 to 10 users) have been generated.
For each group size, 10000 synthetic groups have been
generated. The generation of a group has been carried out
following the following algorithm:
1.
Deﬁne the size of the group S.
2.
Random select 5 items rated in test by at least S users.
3.
Find all users who have rated the 5 items selected in 2.
4.
If we found fewer than S users, go back to 2.
Otherwise, random select S users and create a group.
To measure the quality of the predictions for the group, we
have calculated the mean absolute error
MAE ¼
1
#groups
X
G
1
kGk  kIGk
X
u2G
X
i2IG
j ^rG;i  ru;i j;
ð7Þ
the mean squared error
MSE ¼
1
#groups
X
G
1
kGk
1
kGk  kIGk
X
u2G
X
i2IG
^rG;i  ru;i

2;
ð8Þ
and mean maximum group error
MAX ¼
1
#groups
X
G
max
u2G max
i2IG j ^rG;i  ru;i j;
ð9Þ
where IG denotes the items rated by the group G
To measure the quality of the recommendations for the
group, we have calculated the Normalized Discounted
Cumulative Gain (NDCG) score
NDCG@N ¼
1
#groups
X
G
DCGG@N
IDCGG@N ;
ð10Þ
DCGG@N ¼
X
i2XN
G
rG;i
log2ðposGðiÞ þ 1Þ ;
ð11Þ
IDCGG@N ¼
X
i2TN
G
rG;i
log2ðiposGðiÞ þ 1Þ ;
ð12Þ
where N is the number of items recommended to the group
(in our experiments N ¼ 5 according to the generation of
synthetic groups), XN
G is the set of N items recommended to
the group G, posGðiÞ is the position of the item i in the
group’s G recommendation list, TN
G is the set of the top N
items for the group G, iposGðiÞ is the ideal rank of the item
Table 2 Main parameters of the datasets used in the experiments
Dataset
#users
#items
#ratings
Scores
Sparsity
Movie lens1M
6040
3706
911,031
1–5
95.94
Filmtrust
1508
2071
35,497
0–5
87.98
My anime list
19,179
2692
548,967
1–10
98.94
14086
Neural Computing and Applications (2023) 35:14081–14092
123

Table 3 Mean absolute error
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.74205
(0.409)
0.76075
(0.341)
0.76893
(0.299)
0.77009
(0.271)
0.77745
(0.249)
0.77659
(0.234)
0.77681
(0.221)
0.77599
(0.212)
0.77558
(0.201)
GMF Expertise
0.74393
(0.41)
0.76207
(0.341)
0.77018
(0.299)
0.77155
(0.27)
0.779
(0.249)
0.77782
(0.234)
0.77834
(0.221)
0.77729
(0.211)
0.77685
(0.201)
GMF Softmax
0.74246
(0.409)
0.7608
(0.341)
0.76891
(0.299)
0.77012
(0.27)
0.77751
(0.249)
0.7766
(0.234)
0.77687
(0.221)
0.77602
(0.212)
0.77562
(0.201)
MLP IPA
0.74956
(0.444)
0.77342
(0.361)
0.78055
(0.313)
0.78211
(0.28)
0.78853
(0.258)
0.78633
(0.241)
0.78678
(0.228)
0.78591
(0.219)
0.78509
(0.207)
MLP Avg
DeepGroup
0.7236
(0.486)
0.74289
(0.404)
0.74916
(0.355)
0.75018
(0.32)
0.75656
(0.295)
0.75537
(0.275)
0.75551
(0.261)
0.75388
(0.25)
0.75355
(0.238)
MLP Expertise
0.72596
(0.486)
0.74432
(0.405)
0.75031
(0.355)
0.75132
(0.32)
0.75787
(0.295)
0.75607
(0.276)
0.75709
(0.261)
0.75481
(0.25)
0.755
(0.238)
MLP Softmax
0.72407
(0.485)
0.74297
(0.404)
0.74925
(0.355)
0.75019
(0.32)
0.75669
(0.295)
0.75531
(0.275)
0.7556
(0.261)
0.75389
(0.25)
0.75361
(0.238)
(a) MovieLens1M
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.61552
(0.451)
0.71033
(0.32)
0.73011
(0.28)
0.73419
(0.252)
0.73606
(0.232)
0.73832
(0.217)
0.74045
(0.203)
0.74298
(0.193)
0.74212
(0.185)
GMF Expertise
0.6149
(0.444)
0.71239
(0.319)
0.73144
(0.281)
0.73583
(0.252)
0.73742
(0.232)
0.7396
(0.217)
0.74166
(0.203)
0.74418
(0.193)
0.74336
(0.184)
GMF Softmax
0.61512
(0.448)
0.71088
(0.319)
0.73035
(0.28)
0.73445
(0.252)
0.73624
(0.232)
0.73847
(0.217)
0.74057
(0.203)
0.74309
(0.193)
0.74222
(0.185)
MLP IPA
0.58165
(0.442)
0.70368
(0.325)
0.72062
(0.281)
0.7252
(0.252)
0.72788
(0.232)
0.73073
(0.217)
0.73353
(0.203)
0.73623
(0.193)
0.73525
(0.185)
MLP Avg
DeepGroup
0.58199
(0.447)
0.70129
(0.319)
0.71693
(0.275)
0.7212
(0.247)
0.72421
(0.228)
0.727
(0.214)
0.72976
(0.2)
0.7328
(0.191)
0.73188
(0.183)
MLP Expertise
0.57903
(0.453)
0.70449
(0.321)
0.71891
(0.276)
0.72315
(0.247)
0.72581
(0.228)
0.72865
(0.213)
0.73127
(0.2)
0.73429
(0.191)
0.73336
(0.183)
MLP Softmax
0.58063
(0.45)
0.70226
(0.319)
0.71734
(0.275)
0.72153
(0.247)
0.72443
(0.228)
0.7272
(0.214)
0.72993
(0.2)
0.73295
(0.191)
0.73202
(0.183)
(b) FilmTrust
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.93068
(0.567)
0.95819
(0.477)
0.97595
(0.417)
0.99149
(0.38)
1.0017
(0.346)
1.01404
(0.329)
1.01942
(0.312)
1.022
(0.297)
1.02445
(0.282)
GMF Expertise
0.93675
(0.572)
0.96424
(0.48)
0.98092
(0.419)
0.99603
(0.382)
1.00649
(0.349)
1.01805
(0.331)
1.02352
(0.314)
1.0258
(0.3)
1.0284
(0.284)
GMF Softmax
0.93219
(0.568)
0.95848
(0.477)
0.97559
(0.417)
0.99104
(0.38)
1.00133
(0.346)
1.01363
(0.329)
1.0191
(0.312)
1.02173
(0.297)
1.02422
(0.282)
MLP IPA
0.95479
(0.585)
0.98155
(0.484)
0.99709
(0.42)
1.00977
(0.381)
1.01745
(0.347)
1.02794
(0.329)
1.03188
(0.311)
1.03335
(0.298)
1.03451
(0.282)
MLP Avg
DeepGroup
0.93161
(0.618)
0.95609
(0.515)
0.96961
(0.45)
0.98456
(0.408)
0.99331
(0.371)
1.0052
(0.351)
1.00857
(0.332)
1.01039
(0.317)
1.01233
(0.3)
MLP Expertise
0.93803
(0.622)
0.96132
(0.517)
0.97587
(0.453)
0.98923
(0.409)
0.99851
(0.373)
1.00879
(0.353)
1.01258
(0.334)
1.01493
(0.319)
1.01599
(0.302)
MLP Softmax
0.93351
(0.619)
0.95594
(0.515)
0.97007
(0.451)
0.9843
(0.408)
0.99329
(0.371)
1.00486
(0.351)
1.00844
(0.332)
1.0101
(0.317)
1.01211
(0.3)
(c) MyAnimeList
Neural Computing and Applications (2023) 35:14081–14092
14087
123

Table 4 Mean squared error
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.87504
(0.914)
0.90947
(0.767)
0.92437
(0.676)
0.92648
(0.609)
0.94191
(0.567)
0.93896
(0.53)
0.9376
(0.5)
0.93832
(0.48)
0.93571
(0.454)
GMF Expertise
0.88044
(0.918)
0.91329
(0.77)
0.92754
(0.678)
0.92979
(0.61)
0.94504
(0.568)
0.94135
(0.531)
0.93994
(0.501)
0.94037
(0.481)
0.93737
(0.454)
GMF Softmax
0.87614
(0.914)
0.90952
(0.767)
0.92424
(0.676)
0.92645
(0.609)
0.94188
(0.567)
0.93885
(0.53)
0.93752
(0.5)
0.93823
(0.48)
0.93561
(0.454)
MLP IPA
0.94301
(0.982)
0.96557
(0.814)
0.96809
(0.712)
0.96499
(0.635)
0.97727
(0.591)
0.96889
(0.55)
0.96634
(0.518)
0.96648
(0.498)
0.96187
(0.47)
MLP Avg
DeepGroup
0.99415
(1.037)
1.03182
(0.875)
1.04278
(0.779)
1.04242
(0.695)
1.05597
(0.651)
1.05056
(0.605)
1.04927
(0.574)
1.04836
(0.552)
1.04669
(0.524)
MLP Expertise
0.9986
(1.038)
1.03522
(0.88)
1.04511
(0.78)
1.04435
(0.696)
1.05806
(0.651)
1.05121
(0.604)
1.05084
(0.575)
1.0491
(0.553)
1.04787
(0.523)
MLP Softmax
0.99487
(1.035)
1.03145
(0.874)
1.04258
(0.779)
1.04235
(0.694)
1.05605
(0.651)
1.05034
(0.604)
1.04925
(0.574)
1.04822
(0.552)
1.04659
(0.524)
(a) MovieLens1M
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.67941
(1.077)
0.7889
(0.688)
0.82014
(0.608)
0.82302
(0.544)
0.82574
(0.502)
0.83038
(0.469)
0.8324
(0.44)
0.8375
(0.42)
0.83544
(0.401)
GMF Expertise
0.67314
(1.05)
0.79105
(0.686)
0.82229
(0.608)
0.82546
(0.543)
0.82758
(0.501)
0.83205
(0.468)
0.83382
(0.439)
0.83875
(0.418)
0.83673
(0.399)
GMF Softmax
0.67596
(1.063)
0.7892
(0.687)
0.82043
(0.608)
0.82333
(0.544)
0.82592
(0.502)
0.83053
(0.469)
0.83251
(0.44)
0.83758
(0.42)
0.83552
(0.401)
MLP IPA
0.61169
(1.002)
0.77197
(0.683)
0.79514
(0.592)
0.7988
(0.528)
0.80315
(0.487)
0.80807
(0.454)
0.81084
(0.427)
0.81611
(0.406)
0.81408
(0.388)
MLP Avg
DeepGroup
0.61859
(1.009)
0.7636
(0.674)
0.78665
(0.585)
0.79085
(0.523)
0.79606
(0.484)
0.80126
(0.452)
0.80444
(0.425)
0.81023
(0.405)
0.80841
(0.387)
MLP Expertise
0.62484
(0.979)
0.7691
(0.68)
0.78956
(0.586)
0.79332
(0.522)
0.79799
(0.483)
0.80305
(0.45)
0.80595
(0.423)
0.81166
(0.403)
0.80977
(0.385)
MLP Softmax
0.62106
(0.992)
0.7649
(0.675)
0.78709
(0.585)
0.79116
(0.523)
0.79625
(0.484)
0.80142
(0.452)
0.80456
(0.424)
0.81033
(0.404)
0.8085
(0.387)
(b) FilmTrust
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
1.47037
(1.922)
1.53981
(1.648)
1.58373
(1.43)
1.63441
(1.324)
1.66233
(1.213)
1.70615
(1.178)
1.72188
(1.111)
1.73177
(1.065)
1.74048
(1.015)
GMF Expertise
1.49874
(1.963)
1.57277
(1.689)
1.61625
(1.463)
1.66444
(1.351)
1.69199
(1.239)
1.73293
(1.202)
1.74812
(1.133)
1.75649
(1.087)
1.76487
(1.035)
GMF Softmax
1.47707
(1.932)
1.54383
(1.654)
1.58617
(1.433)
1.63575
(1.326)
1.66342
(1.214)
1.70677
(1.179)
1.72239
(1.112)
1.73223
(1.066)
1.74092
(1.016)
MLP IPA
1.56623
(1.959)
1.61737
(1.655)
1.64909
(1.431)
1.69132
(1.32)
1.71029
(1.209)
1.74683
(1.169)
1.75711
(1.105)
1.76368
(1.058)
1.76877
(1.008)
MLP Avg
DeepGroup
1.61669
(2.032)
1.68103
(1.718)
1.71103
(1.492)
1.75681
(1.368)
1.77828
(1.254)
1.81759
(1.217)
1.82764
(1.148)
1.83403
(1.098)
1.84177
(1.049)
MLP Expertise
1.64412
(2.059)
1.70965
(1.747)
1.74467
(1.517)
1.78446
(1.386)
1.80671
(1.276)
1.84221
(1.232)
1.85095
(1.163)
1.85803
(1.116)
1.86351
(1.063)
MLP Softmax
1.62458
(2.038)
1.68445
(1.723)
1.7153
(1.496)
1.75838
(1.368)
1.78024
(1.256)
1.81848
(1.218)
1.82855
(1.149)
1.83446
(1.099)
1.84249
(1.049)
(c) MyAnimeList
14088
Neural Computing and Applications (2023) 35:14081–14092
123

Table 5 Mean max error
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
1.02112
(0.6)
1.2213
(0.598)
1.35658
(0.588)
1.45195
(0.583)
1.54115
(0.578)
1.59908
(0.577)
1.64979
(0.573)
1.69807
(0.576)
1.73598
(0.571)
GMF Expertise
1.02474
(0.602)
1.22441
(0.599)
1.35928
(0.59)
1.4551
(0.584)
1.54414
(0.579)
1.60112
(0.577)
1.65118
(0.574)
1.69926
(0.576)
1.7366
(0.571)
GMF Softmax
1.02191
(0.6)
1.22143
(0.598)
1.35653
(0.588)
1.45202
(0.583)
1.54118
(0.578)
1.59897
(0.577)
1.64965
(0.573)
1.69792
(0.576)
1.7358
(0.571)
MLP IPA
1.04098
(0.642)
1.25715
(0.614)
1.38214
(0.602)
1.47972
(0.591)
1.56516
(0.586)
1.62067
(0.581)
1.66905
(0.576)
1.71687
(0.58)
1.7528
(0.573)
MLP Avg
DeepGroup
1.07144
(0.666)
1.28743
(0.643)
1.41937
(0.635)
1.51234
(0.633)
1.59812
(0.638)
1.65421
(0.642)
1.70673
(0.641)
1.75552
(0.647)
1.79703
(0.645)
MLP Expertise
1.07476
(0.667)
1.28932
(0.644)
1.42078
(0.635)
1.51325
(0.634)
1.59883
(0.638)
1.65371
(0.64)
1.70637
(0.64)
1.75428
(0.645)
1.79566
(0.643)
MLP Softmax
1.07226
(0.666)
1.28716
(0.643)
1.41906
(0.635)
1.51227
(0.633)
1.59788
(0.638)
1.65391
(0.641)
1.7066
(0.641)
1.75515
(0.646)
1.79669
(0.645)
(a) MovieLens1M
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.85192
(0.555)
1.13757
(0.573)
1.27696
(0.573)
1.36783
(0.57)
1.44244
(0.571)
1.51262
(0.575)
1.56818
(0.573)
1.62204
(0.571)
1.6636
(0.567)
GMF Expertise
0.85171
(0.549)
1.13845
(0.571)
1.27815
(0.571)
1.36895
(0.567)
1.4425
(0.569)
1.51199
(0.573)
1.56717
(0.57)
1.62058
(0.569)
1.66146
(0.565)
GMF Softmax
0.85147
(0.552)
1.13761
(0.572)
1.27708
(0.572)
1.3679
(0.569)
1.44234
(0.571)
1.51245
(0.575)
1.56799
(0.572)
1.62184
(0.571)
1.66335
(0.567)
MLP IPA
0.78405
(0.555)
1.11519
(0.564)
1.24935
(0.56)
1.33899
(0.555)
1.41348
(0.556)
1.48001
(0.557)
1.5341
(0.554)
1.58622
(0.552)
1.62791
(0.549)
MLP Avg
DeepGroup
0.79205
(0.553)
1.11222
(0.557)
1.24695
(0.557)
1.33662
(0.555)
1.4118
(0.557)
1.47902
(0.559)
1.53438
(0.556)
1.58697
(0.555)
1.62916
(0.55)
MLP Expertise
0.79191
(0.56)
1.11441
(0.556)
1.24814
(0.555)
1.33715
(0.551)
1.41142
(0.554)
1.47765
(0.556)
1.53235
(0.553)
1.58432
(0.551)
1.62581
(0.547)
MLP Softmax
0.79235
(0.555)
1.11261
(0.556)
1.24698
(0.556)
1.33654
(0.554)
1.41161
(0.556)
1.47872
(0.558)
1.53406
(0.555)
1.58662
(0.554)
1.62879
(0.55)
(b) FilmTrust
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
1.30264
(0.845)
1.58715
(0.866)
1.79755
(0.864)
1.9702
(0.877)
2.10648
(0.876)
2.22836
(0.896)
2.31758
(0.895)
2.39541
(0.91)
2.47078
(0.919)
GMF Expertise
1.31784
(0.858)
1.60964
(0.883)
1.8266
(0.882)
1.99902
(0.894)
2.13631
(0.891)
2.25748
(0.91)
2.34706
(0.908)
2.42357
(0.922)
2.49875
(0.932)
GMF Softmax
1.30626
(0.847)
1.59061
(0.869)
1.80139
(0.867)
1.97309
(0.879)
2.10896
(0.878)
2.23046
(0.898)
2.31946
(0.896)
2.39701
(0.911)
2.47222
(0.92)
MLP IPA
1.34151
(0.874)
1.63805
(0.873)
1.84317
(0.862)
2.01188
(0.868)
2.1406
(0.863)
2.25649
(0.876)
2.33763
(0.875)
2.4112
(0.889)
2.4836
(0.896)
MLP Avg
DeepGroup
1.36617
(0.893)
1.66385
(0.902)
1.8675
(0.896)
2.03818
(0.907)
2.17095
(0.906)
2.28936
(0.924)
2.37588
(0.921)
2.4495
(0.934)
2.52558
(0.943)
MLP Expertise
1.38038
(0.9)
1.68165
(0.913)
1.89237
(0.908)
2.0611
(0.918)
2.1951
(0.914)
2.31311
(0.929)
2.39796
(0.923)
2.47081
(0.935)
2.54585
(0.943)
MLP Softmax
1.37076
(0.894)
1.66676
(0.904)
1.87147
(0.898)
2.04076
(0.908)
2.17377
(0.907)
2.29111
(0.924)
2.37734
(0.921)
2.45073
(0.934)
2.52696
(0.943)
(c) MyAnimeList
Neural Computing and Applications (2023) 35:14081–14092
14089
123

i for the group G, and rG;i is the average rating of the users
belonging to the group G for the item i.
We can see the results of the experiment executed with
these scores in Table 3 (MAE), Table 4 (MSE), Table 5
(Max), and Table 6 (NDCG). The cells with the best results
Table 6 Discounted cumulative gain
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.98021
(0.024)
0.98543
(0.018)
0.9886
(0.015)
0.99067
(0.012)
0.99178
(0.011)
0.99286
(0.01)
0.99358
(0.009)
0.99415
(0.008)
0.99457
(0.008)
GMF Expertise
0.97995
(0.024)
0.98528
(0.019)
0.98853
(0.015)
0.99058
(0.012)
0.99171
(0.011)
0.99282
(0.01)
0.99351
(0.009)
0.99411
(0.008)
0.99454
(0.008)
GMF Softmax
0.98006
(0.024)
0.98536
(0.019)
0.9886
(0.015)
0.99064
(0.012)
0.99178
(0.011)
0.99288
(0.01)
0.9936
(0.009)
0.99415
(0.008)
0.99456
(0.008)
MLP IPA
0.97854
(0.026)
0.98342
(0.02)
0.98689
(0.016)
0.98906
(0.014)
0.99046
(0.012)
0.99178
(0.011)
0.99251
(0.01)
0.99303
(0.009)
0.99358
(0.009)
MLP Avg
DeepGroup
0.97778
(0.026)
0.98247
(0.021)
0.98556
(0.017)
0.98762
(0.015)
0.98894
(0.014)
0.98999
(0.013)
0.99064
(0.012)
0.99109
(0.011)
0.99152
(0.011)
MLP Expertise
0.97777
(0.026)
0.98251
(0.021)
0.9855
(0.017)
0.98763
(0.015)
0.98894
(0.014)
0.99004
(0.013)
0.99071
(0.012)
0.99116
(0.011)
0.9916
(0.011)
MLP Softmax
0.97784
(0.026)
0.98248
(0.021)
0.98554
(0.017)
0.98762
(0.015)
0.98895
(0.014)
0.98998
(0.013)
0.99069
(0.012)
0.9911
(0.011)
0.99153
(0.011)
(a) MovieLens1M
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.96788
(0.028)
0.97795
(0.022)
0.9815
(0.019)
0.98393
(0.016)
0.98596
(0.014)
0.9876
(0.013)
0.98869
(0.011)
0.98967
(0.011)
0.99038
(0.01)
GMF Expertise
0.96795
(0.028)
0.97794
(0.022)
0.9815
(0.019)
0.98392
(0.016)
0.98593
(0.014)
0.9876
(0.013)
0.98868
(0.012)
0.98965
(0.011)
0.99038
(0.01)
GMF Softmax
0.96795
(0.028)
0.97796
(0.022)
0.9815
(0.019)
0.98394
(0.016)
0.98596
(0.014)
0.9876
(0.013)
0.98868
(0.012)
0.98967
(0.011)
0.99038
(0.01)
MLP IPA
0.97057
(0.027)
0.9788
(0.023)
0.98249
(0.019)
0.98523
(0.016)
0.98704
(0.015)
0.98896
(0.012)
0.98994
(0.011)
0.9911
(0.01)
0.99165
(0.01)
MLP Avg
DeepGroup
0.96897
(0.027)
0.9789
(0.022)
0.98275
(0.018)
0.98527
(0.016)
0.98729
(0.014)
0.98895
(0.012)
0.98998
(0.011)
0.99104
(0.01)
0.99166
(0.009)
MLP Expertise
0.9694
(0.027)
0.97897
(0.022)
0.98276
(0.018)
0.98532
(0.016)
0.98724
(0.014)
0.98893
(0.012)
0.98993
(0.011)
0.99106
(0.01)
0.99166
(0.009)
MLP Softmax
0.9693
(0.027)
0.97891
(0.022)
0.98278
(0.018)
0.98527
(0.016)
0.98728
(0.014)
0.98895
(0.012)
0.98998
(0.011)
0.99106
(0.01)
0.99167
(0.009)
(b) FilmTrust
Model \Group Size
2
3
4
5
6
7
8
9
10
GMF IPA
GMF Avg
0.98898
(0.014)
0.99218
(0.01)
0.99401
(0.008)
0.9949
(0.007)
0.99571
(0.006)
0.99615
(0.005)
0.99662
(0.005)
0.99679
(0.005)
0.99714
(0.004)
GMF Expertise
0.98893
(0.014)
0.99209
(0.01)
0.99383
(0.008)
0.99479
(0.007)
0.99566
(0.006)
0.99609
(0.005)
0.99658
(0.005)
0.99674
(0.005)
0.99708
(0.004)
GMF Softmax
0.98898
(0.014)
0.99217
(0.01)
0.99399
(0.008)
0.99492
(0.007)
0.99572
(0.006)
0.99615
(0.005)
0.99664
(0.005)
0.99679
(0.005)
0.99715
(0.004)
MLP IPA
0.98723
(0.015)
0.99062
(0.011)
0.99255
(0.009)
0.9936
(0.008)
0.99458
(0.007)
0.99507
(0.006)
0.99573
(0.006)
0.99591
(0.005)
0.99632
(0.005)
MLP Avg
DeepGroup
0.9868
(0.016)
0.99023
(0.012)
0.99212
(0.01)
0.99307
(0.008)
0.99415
(0.007)
0.99459
(0.007)
0.99519
(0.006)
0.99537
(0.006)
0.99565
(0.005)
MLP Expertise
0.9867
(0.016)
0.99021
(0.012)
0.9921
(0.01)
0.99312
(0.008)
0.99414
(0.007)
0.99458
(0.007)
0.99521
(0.006)
0.99542
(0.006)
0.9957
(0.005)
MLP Softmax
0.98684
(0.016)
0.99028
(0.012)
0.99211
(0.01)
0.99308
(0.008)
0.99416
(0.007)
0.9946
(0.007)
0.99521
(0.006)
0.99539
(0.006)
0.99566
(0.005)
(c) MyAnimeList
14090
Neural Computing and Applications (2023) 35:14081–14092
123

have been highlighted (gray background and bold), while
the standard deviation of each metric is in parentheses. All
results are analyzed in Sect. 4.
All experiments have been run using an NVIDIA
Quadro RTX 8000 GPU with 48 GB GDDR6 of memory,
4,608 NVIDIA Tensor Cores and a performance of 16.3
TFLOPS. We are committed to reproducible science, so the
source code of all experiments with the values of the
parameters used and their random seeds have been shared
on GitHub.1
4 Discussion
The main goal of this research is to evaluate different
aggregation techniques to make recommendations to
groups. As shown in Sect. 3, we can see different trends
according to (a) the models used; (b) the way group
information is aggregated; (c) the datasets on which they
act; and (d) the size of the groups.
Focusing on the models, we can see how Multi-Layer
Perceptron (MLP), which has several hidden layers, obtains
a lower MAE; however, Generalized Matrix Factorization
(GMF), a simpler model, obtains a lower MSE. Although
the Multi-Layer Perceptron (MLP) model has great power
in these types of problem, it seems to overﬁt, generating
very good recommendations for some users in the group
but bad ones for the rest, hence achieving higher MSE
values. On the other hand, the Generalized Matrix Fac-
torization (GMF) model can obtain smaller maximum
errors in each group, which means that no user in the group
is badly affected by the recommendation. In the results, we
can also observe how the models with higher maximum
errors lead to a poorer order of items according to user
preferences and obtain worse performance in the Normal-
izaed Discounted Cumulative Gain (NDCG) metric.
Looking at the aggregation of users, we can see that the
best performing user aggregation is the average, followed
by a very similar performance by the Softmax. However,
the use of expert user weighting without softmax produces
worse results. Based on the results, we can observe that in
models that do not use a deep architecture, with several
hidden layers, the Individual Preference Aggregation (IPA)
and Group Preference Aggregation (GPA) strategies pro-
duce similar results when the aggregation function is a
linear transformation of latent factors (Generalized Matrix
Factorization (GMF)). However, we can see how the
nonlinearity of Multi-Layer Perceptron (MLP) produces
different results between both two strategies.
Regarding the different datasets, we can see that there is
a clear trend in the models that achieve the best results in
complex datasets with a large number of users, items, and
votes, such as Movilens or MyAnimeList, while in the
FilmTrust dataset, with a smaller number of votes, there is
no clear trend.
In terms of group size, as more users have the group, the
probability of ﬁnding discrepancies between user prefer-
ences increases. Therefore, we can see how a larger group
size leads to higher values in all error metrics.
5 Conclusions and future work
With the irruption of Neural Network (NN) in the world of
Collaborative Filtering (CF), the possibilities of their
ability to ﬁnd nonlinear patterns within user preferences to
generate better predictions are opening up. To use these
systems to generate a recommendation for a group of users,
we need to aggregate their preferences. As we have seen in
this research, there are several key points at which aggre-
gation can be performed. Group Preference Aggregation
(GPA) strategies do the aggregation before or inside the
model, so they have the advantage of taking into account
the preferences of the entire group and that a single feed-
forward step generates the prediction. In contrast, the
Individual Preference Aggregation (IPA) strategy, requires
multiple predictions for each user and performs the
aggregations after the model. In this study, we have tested
how different approaches to perform Group Preference
Aggregation (GPA) work in different datasets comparing
different metrics.
As future work, there are two key factors to consider.
First, in this research, the researchers have designed user
aggregation techniques presented to the models; in future
work, these functions will be explored by different machine
learning models. The second key point is that in this work,
models perform a knowledge transfer from the model
trained for individuals to make group predictions; it is
shown that although the models have high performance
(MAE improvement), they tend to overﬁt when working in
groups (larger errors in group prediction leading to worse
MSE). To solve this problem, future work will try to per-
form a specialization training stage for groups after indi-
vidual training.
Funding This work has been co-funded by the Ministerio de Ciencia
e Innovacio´n of Spain and the European Regional Development Fund
(FEDER) under grants PID2019-106493RB-I00 (DL-CEMG) and the
Comunidad de Madrid under Convenio Plurianual with the Uni-
versidad Polite´cnica de Madrid in the actuation line of Programa de
Excelencia para el Profesorado Universitario.
Data availability The MovieLens1M, FilmTrust and MyAni-
meList dataset along with the source code of the experiments that
support the ﬁndings of this study is available in neural-cf-for-
1 https://github.com/KNODIS-Research-Group/neural-cf-for-groups
Neural Computing and Applications (2023) 35:14081–14092
14091
123

groups
GitHub’s
repository
[https://github.com/KNODIS-
Research-Group/neural-cf-for-groups].
Declarations
Conflict of interest The authors of this paper declare that they have no
conflict of interest.
References
1. Batmaz Z, Yurekli A, Bilge A, Kaleli C (2019) A review on deep
learning for recommender systems: challenges and remedies.
Artif Intell Rev 52(1):1–37. https://doi.org/10.1007/s10462-018-
9654-y
2. Bobadilla J, Gonza´lez-Prieto A´ , Ortega F, Lara-Cabrera R (2021)
Deep learning feature selection to unhide demographic recom-
mender systems factors. Neural Comput Appl 33(12):7291–7308.
https://doi.org/10.1007/s00521-020-05494-2
3. Deldjoo Y, Schedl M, Cremonesi P, Pasi G (2020) Recommender
systems leveraging multimedia content. ACM Comput Surv
53(5):1–38. https://doi.org/10.1145/3407190
4. Kulkarni S, Rodd SF (2020) Context aware recommendation
systems: a review of the state of the art techniques. Comput Sci
Rev 37:100255. https://doi.org/10.1016/j.cosrev.2020.100255
5. Shokeen J, Rana C (2020) A study on features of social recom-
mender systems. Artif Intell Rev 53(2):965–988. https://doi.org/
10.1007/s10462-019-09684-w
6. Bobadilla J, Alonso S, Hernando A (2020) Deep learning archi-
tecture for collaborative ﬁltering recommender systems. Appl Sci
10(7):2441. https://doi.org/10.3390/app10072441
7. Dara S, Chowdary CR, Kumar C (2020) A survey on group
recommender systems. J Intell Inf Syst 54(2):271–295. https://
doi.org/10.1007/s10844-018-0542-3
8. Forouzandeh S, Berahmand K, Rostami M (2021) Presentation of
a recommender system with ensemble learning and graph
embedding:
a
case
on
movielens.
Multimed
Tools
Appl
80(5):7805–7832. https://doi.org/10.1007/s11042-020-09949-5
9. C¸ ano E, Morisio M (2017) Hybrid recommender systems: a
systematic literature review. Intell Data Anal 21(6):1487–1524.
https://doi.org/10.3233/IDA-163209
10. Salakhutdinov R, Mnih A (2007) Probabilistic matrix factoriza-
tion. In: Proceedings of the 20th international conference on
neural information processing systems. NIPS’07, pp. 1257–1264.
Curran Associates Inc., Red Hook, NY, USA. https://doi.org/10.
5555/2981562.2981720
11. Bobadilla J, Gonza´lez-Prieto A´ , Ortega F, Lara-Cabrera R (2022)
Deep learning approach to obtain collaborative ﬁltering neigh-
borhoods. Neural Comput Appl 34(4):2939–2951. https://doi.org/
10.1007/s00521-021-06493-7
12. Huang T, Zhang D-f, Bi L (2020) Neural embedding collabora-
tive ﬁltering for recommender systems. Neural Comput Appl
32:1–15. https://doi.org/10.1007/s00521-020-04920-9
13. He X, Liao L, Zhang H, Nie L, Hu X, Chua T-S (2017) Neural
collaborative ﬁltering. In: Proceedings of the 26th international
conference on world wide web. WWW ’17, pp. 173–182. Inter-
national World Wide Web Conferences Steering Committee,
Republic and Canton of Geneva, CHE. https://doi.org/10.1145/
3038912.3052569
14. Ortega F, Bobadilla J, Hernando A, Gutie´Rrez A (2013) Incor-
porating group recommendations to recommender systems:
alternatives
and
performance.
Inf
Process
Manage
49(4):895–901. https://doi.org/10.1016/j.ipm.2013.02.003
15. Baltrunas L, Makcinskas T, Ricci F (2010) Group recommen-
dations with rank aggregation and collaborative ﬁltering. In:
Proceedings of the fourth acm conference on recommender sys-
tems. RecSys ’10, pp. 119–126. Association for Computing
Machinery,
New
York,
NY,
USA.
https://doi.org/10.1145/
1864708.1864733
16. Ortega F, Hernando A, Bobadilla J, Kang JH (2016) Recom-
mending items to group of users using matrix factorization based
collaborative ﬁltering. Inf Sci 345:313–324. https://doi.org/10.
1016/j.ins.2016.01.083
17. Feng S, Zhang H, Wang L, Liu L, Xu Y (2019) Detecting the
latent associations hidden in multi-source information for better
group recommendation. Know-Based Syst 171:56–68. https://doi.
org/10.1016/j.knosys.2019.02.002
18. Abolghasemi R, Engelstad P, Herrera-Viedma E, Yazidi A (2022)
A personality-aware group recommendation system based on
pairwise preferences. Inf Sci 595:1–17. https://doi.org/10.1016/j.
ins.2022.02.033
19. Barzegar Nozari R, Koohi H (2020) A novel group recommender
system based on members’ inﬂuence and leader impact. Know-
Based Syst 205:106296. https://doi.org/10.1016/j.knosys.2020.
106296
20. Wang X, Su L, Zhou Q, Wu L, Zhang Y (2020) Group recom-
mender systems based on members’ preference for trusted social
networks. Sec Commun Netw 2020:1–11. https://doi.org/10.
1155/2020/1924140
21. Ismailoglu F (2022) Aggregating user preferences in group rec-
ommender systems: a crowdsourcing approach. Decis Support
Syst 152:113663. https://doi.org/10.1016/j.dss.2021.113663
22. Guo J, Zhu Y, Li A, Wang Q, Han W (2016) A social inﬂuence
approach for group user modeling in group recommendation
systems. IEEE Intell Syst 31(5):40–48. https://doi.org/10.1109/
MIS.2016.28
23. Sajjadi Ghaemmaghami S, Salehi-Abari A (2021) DeepGroup:
group recommendation with implicit feedback. Association for
Computing Machinery, New York, pp 3408–3412
24. Hu L, Cao J, Xu G, Cao L, Gu Z, Cao W (2014) Deep modeling
of group preferences for group-based recommendation. In: Pro-
ceedings of the twenty-eighth AAAI conference on artiﬁcial
intelligence. AAAI’14, pp. 1861–1867. AAAI Press, Palo Alto,
California. https://doi.org/10.1609/aaai.v28i1.9007
25. Harper FM, Konstan JA (2015) The movielens datasets: history
and context. ACM Trans Interact Intell Syst 5(4):1–19. https://
doi.org/10.1145/2827872
26. Guo G, Zhang J, Yorke-Smith N (2013) A novel bayesian simi-
larity measure for recommender systems. In: Proceedings of the
twenty-third international joint conference on artiﬁcial intelli-
gence. IJCAI ’13, pp. 2619–2625. AAAI Press, Menlo Park,
California. https://doi.org/10.5555/2540128.2540506
Publisher’s Note Springer Nature remains neutral with regard to
jurisdictional claims in published maps and institutional afﬁliations.
Springer Nature or its licensor (e.g. a society or other partner) holds
exclusive rights to this article under a publishing agreement with the
author(s) or other rightsholder(s); author self-archiving of the
accepted manuscript version of this article is solely governed by the
terms of such publishing agreement and applicable law.
14092
Neural Computing and Applications (2023) 35:14081–14092
123
"
